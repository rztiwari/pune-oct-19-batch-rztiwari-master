{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Computer_Vision_with_CNN_R7_Project1_Plant_seedling1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvpdw7ASVrEN",
        "colab_type": "text"
      },
      "source": [
        "##Try downloading the data from the kaggle site"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx8bGRBUeNzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "b3526dc2-2de5-459b-f5ef-d635d94b3a63"
      },
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.15.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fkXZaXqfUsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKiHL1Bvfo0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c971385-c7b8-4f92-b7ac-53714227bb86"
      },
      "source": [
        "!kaggle competitions download -c plant-seedlings-classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.6 / client 1.5.4)\n",
            "Downloading 084d21b80.png.zip to /content\n",
            "  0% 0.00/1.67M [00:00<?, ?B/s]\n",
            "100% 1.67M/1.67M [00:00<00:00, 56.0MB/s]\n",
            "Downloading 107bd7230.png.zip to /content\n",
            "  0% 0.00/1.10M [00:00<?, ?B/s]\n",
            "100% 1.10M/1.10M [00:00<00:00, 36.4MB/s]\n",
            "Downloading 01642cae8.png to /content\n",
            "  0% 0.00/5.70k [00:00<?, ?B/s]\n",
            "100% 5.70k/5.70k [00:00<00:00, 5.17MB/s]\n",
            "Downloading 0bdaf1d8f.png to /content\n",
            "  0% 0.00/482k [00:00<?, ?B/s]\n",
            "100% 482k/482k [00:00<00:00, 65.6MB/s]\n",
            "Downloading 0086c28b2.png to /content\n",
            "  0% 0.00/44.3k [00:00<?, ?B/s]\n",
            "100% 44.3k/44.3k [00:00<00:00, 54.2MB/s]\n",
            "Downloading 030e7f9ef.png.zip to /content\n",
            "  0% 0.00/3.15M [00:00<?, ?B/s]\n",
            "100% 3.15M/3.15M [00:00<00:00, 104MB/s]\n",
            "Downloading 11323514a.png to /content\n",
            "  0% 0.00/517k [00:00<?, ?B/s]\n",
            "100% 517k/517k [00:00<00:00, 69.9MB/s]\n",
            "Downloading 0f16cf10a.png to /content\n",
            "  0% 0.00/46.6k [00:00<?, ?B/s]\n",
            "100% 46.6k/46.6k [00:00<00:00, 45.0MB/s]\n",
            "Downloading 16511dd22.png to /content\n",
            "  0% 0.00/9.68k [00:00<?, ?B/s]\n",
            "100% 9.68k/9.68k [00:00<00:00, 10.6MB/s]\n",
            "Downloading 11dc03905.png to /content\n",
            "  0% 0.00/638k [00:00<?, ?B/s]\n",
            "100% 638k/638k [00:00<00:00, 42.2MB/s]\n",
            "Downloading 0419f5bbc.png to /content\n",
            "  0% 0.00/6.67k [00:00<?, ?B/s]\n",
            "100% 6.67k/6.67k [00:00<00:00, 6.86MB/s]\n",
            "Downloading 0cd0d9b8c.png.zip to /content\n",
            "  0% 0.00/1.11M [00:00<?, ?B/s]\n",
            "100% 1.11M/1.11M [00:00<00:00, 75.3MB/s]\n",
            "Downloading 06e1ee6e1.png to /content\n",
            "  0% 0.00/9.55k [00:00<?, ?B/s]\n",
            "100% 9.55k/9.55k [00:00<00:00, 9.81MB/s]\n",
            "Downloading 0150b5b7e.png to /content\n",
            "  0% 0.00/8.48k [00:00<?, ?B/s]\n",
            "100% 8.48k/8.48k [00:00<00:00, 8.89MB/s]\n",
            "Downloading 00a18f05e.png.zip to /content\n",
            "  0% 0.00/2.04M [00:00<?, ?B/s]\n",
            "100% 2.04M/2.04M [00:00<00:00, 138MB/s]\n",
            "Downloading 006196e1c.png to /content\n",
            "  0% 0.00/0.98M [00:00<?, ?B/s]\n",
            "100% 0.98M/0.98M [00:00<00:00, 68.0MB/s]\n",
            "Downloading 051ea51d0.png to /content\n",
            "  0% 0.00/17.2k [00:00<?, ?B/s]\n",
            "100% 17.2k/17.2k [00:00<00:00, 15.3MB/s]\n",
            "Downloading 12025fcc1.png to /content\n",
            "  0% 0.00/786k [00:00<?, ?B/s]\n",
            "100% 786k/786k [00:00<00:00, 51.7MB/s]\n",
            "Downloading 115808876.png to /content\n",
            "  0% 0.00/411k [00:00<?, ?B/s]\n",
            "100% 411k/411k [00:00<00:00, 56.4MB/s]\n",
            "Downloading 0184ec53f.png to /content\n",
            "  0% 0.00/10.3k [00:00<?, ?B/s]\n",
            "100% 10.3k/10.3k [00:00<00:00, 10.6MB/s]\n",
            "Downloading 14db20a90.png to /content\n",
            "  0% 0.00/212k [00:00<?, ?B/s]\n",
            "100% 212k/212k [00:00<00:00, 66.7MB/s]\n",
            "Downloading 0b1df6f5a.png to /content\n",
            "  0% 0.00/29.2k [00:00<?, ?B/s]\n",
            "100% 29.2k/29.2k [00:00<00:00, 28.3MB/s]\n",
            "Downloading 150ab985f.png to /content\n",
            "  0% 0.00/35.2k [00:00<?, ?B/s]\n",
            "100% 35.2k/35.2k [00:00<00:00, 34.6MB/s]\n",
            "Downloading 02afc3d7a.png to /content\n",
            "  0% 0.00/17.0k [00:00<?, ?B/s]\n",
            "100% 17.0k/17.0k [00:00<00:00, 16.9MB/s]\n",
            "Downloading 0e93f4d05.png to /content\n",
            "  0% 0.00/12.8k [00:00<?, ?B/s]\n",
            "100% 12.8k/12.8k [00:00<00:00, 12.4MB/s]\n",
            "Downloading 0bef4ae08.png to /content\n",
            "  0% 0.00/469k [00:00<?, ?B/s]\n",
            "100% 469k/469k [00:00<00:00, 66.4MB/s]\n",
            "Downloading 0ca928305.png to /content\n",
            "  0% 0.00/30.4k [00:00<?, ?B/s]\n",
            "100% 30.4k/30.4k [00:00<00:00, 31.7MB/s]\n",
            "Downloading 05ea7f987.png to /content\n",
            "  0% 0.00/5.98k [00:00<?, ?B/s]\n",
            "100% 5.98k/5.98k [00:00<00:00, 6.41MB/s]\n",
            "Downloading 0ad0ef03e.png to /content\n",
            "  0% 0.00/15.8k [00:00<?, ?B/s]\n",
            "100% 15.8k/15.8k [00:00<00:00, 17.1MB/s]\n",
            "Downloading 15ca828c8.png to /content\n",
            "  0% 0.00/210k [00:00<?, ?B/s]\n",
            "100% 210k/210k [00:00<00:00, 68.3MB/s]\n",
            "Downloading 04e064c46.png to /content\n",
            "  0% 0.00/701k [00:00<?, ?B/s]\n",
            "100% 701k/701k [00:00<00:00, 44.8MB/s]\n",
            "Downloading 006a4d00d.png to /content\n",
            "  0% 0.00/23.7k [00:00<?, ?B/s]\n",
            "100% 23.7k/23.7k [00:00<00:00, 23.1MB/s]\n",
            "Downloading 0ddeaa6e7.png to /content\n",
            "  0% 0.00/8.80k [00:00<?, ?B/s]\n",
            "100% 8.80k/8.80k [00:00<00:00, 9.17MB/s]\n",
            "Downloading 132d3d6e7.png to /content\n",
            "  0% 0.00/259k [00:00<?, ?B/s]\n",
            "100% 259k/259k [00:00<00:00, 81.6MB/s]\n",
            "Downloading 00dd0d16a.png to /content\n",
            "  0% 0.00/144k [00:00<?, ?B/s]\n",
            "100% 144k/144k [00:00<00:00, 47.8MB/s]\n",
            "Downloading 143203030.png to /content\n",
            "  0% 0.00/81.9k [00:00<?, ?B/s]\n",
            "100% 81.9k/81.9k [00:00<00:00, 73.1MB/s]\n",
            "Downloading 01aef64d2.png to /content\n",
            "  0% 0.00/12.9k [00:00<?, ?B/s]\n",
            "100% 12.9k/12.9k [00:00<00:00, 12.9MB/s]\n",
            "Downloading 07f867aa5.png to /content\n",
            "  0% 0.00/15.6k [00:00<?, ?B/s]\n",
            "100% 15.6k/15.6k [00:00<00:00, 15.9MB/s]\n",
            "Downloading 04fafa0d3.png to /content\n",
            "  0% 0.00/194k [00:00<?, ?B/s]\n",
            "100% 194k/194k [00:00<00:00, 64.1MB/s]\n",
            "Downloading 1022cc155.png to /content\n",
            "  0% 0.00/200k [00:00<?, ?B/s]\n",
            "100% 200k/200k [00:00<00:00, 66.6MB/s]\n",
            "Downloading 01e90ef62.png to /content\n",
            "  0% 0.00/6.33k [00:00<?, ?B/s]\n",
            "100% 6.33k/6.33k [00:00<00:00, 6.57MB/s]\n",
            "Downloading 05f76f20c.png to /content\n",
            "  0% 0.00/400k [00:00<?, ?B/s]\n",
            "100% 400k/400k [00:00<00:00, 56.4MB/s]\n",
            "Downloading 0258481da.png to /content\n",
            "  0% 0.00/26.2k [00:00<?, ?B/s]\n",
            "100% 26.2k/26.2k [00:00<00:00, 23.5MB/s]\n",
            "Downloading 060f69cd9.png to /content\n",
            "  0% 0.00/24.6k [00:00<?, ?B/s]\n",
            "100% 24.6k/24.6k [00:00<00:00, 25.5MB/s]\n",
            "Downloading 01358344b.png to /content\n",
            "  0% 0.00/48.4k [00:00<?, ?B/s]\n",
            "100% 48.4k/48.4k [00:00<00:00, 51.1MB/s]\n",
            "Downloading 04f89976a.png to /content\n",
            "  0% 0.00/12.6k [00:00<?, ?B/s]\n",
            "100% 12.6k/12.6k [00:00<00:00, 15.3MB/s]\n",
            "Downloading 04dff1857.png to /content\n",
            "  0% 0.00/10.6k [00:00<?, ?B/s]\n",
            "100% 10.6k/10.6k [00:00<00:00, 11.1MB/s]\n",
            "Downloading 03906fdd7.png to /content\n",
            "  0% 0.00/5.68k [00:00<?, ?B/s]\n",
            "100% 5.68k/5.68k [00:00<00:00, 5.95MB/s]\n",
            "Downloading 025155400.png to /content\n",
            "  0% 0.00/18.7k [00:00<?, ?B/s]\n",
            "100% 18.7k/18.7k [00:00<00:00, 18.3MB/s]\n",
            "Downloading 07bed57ea.png to /content\n",
            "  0% 0.00/404k [00:00<?, ?B/s]\n",
            "100% 404k/404k [00:00<00:00, 56.3MB/s]\n",
            "Downloading 015215883.png to /content\n",
            "  0% 0.00/20.9k [00:00<?, ?B/s]\n",
            "100% 20.9k/20.9k [00:00<00:00, 21.2MB/s]\n",
            "Downloading 03a92ad22.png to /content\n",
            "  0% 0.00/7.98k [00:00<?, ?B/s]\n",
            "100% 7.98k/7.98k [00:00<00:00, 8.29MB/s]\n",
            "Downloading 061592f02.png to /content\n",
            "  0% 0.00/343k [00:00<?, ?B/s]\n",
            "100% 343k/343k [00:00<00:00, 114MB/s]\n",
            "Downloading 0372b48e1.png to /content\n",
            "  0% 0.00/50.6k [00:00<?, ?B/s]\n",
            "100% 50.6k/50.6k [00:00<00:00, 45.9MB/s]\n",
            "Downloading 0438cc647.png to /content\n",
            "  0% 0.00/16.0k [00:00<?, ?B/s]\n",
            "100% 16.0k/16.0k [00:00<00:00, 16.7MB/s]\n",
            "Downloading 078d42d74.png.zip to /content\n",
            "  0% 0.00/1.05M [00:00<?, ?B/s]\n",
            "100% 1.05M/1.05M [00:00<00:00, 35.3MB/s]\n",
            "Downloading 05ceff7d1.png to /content\n",
            "  0% 0.00/350k [00:00<?, ?B/s]\n",
            "100% 350k/350k [00:00<00:00, 167MB/s]\n",
            "Downloading 03ee6340f.png to /content\n",
            "  0% 0.00/8.65k [00:00<?, ?B/s]\n",
            "100% 8.65k/8.65k [00:00<00:00, 7.82MB/s]\n",
            "Downloading 02ae4e8a3.png to /content\n",
            "  0% 0.00/11.3k [00:00<?, ?B/s]\n",
            "100% 11.3k/11.3k [00:00<00:00, 11.0MB/s]\n",
            "Downloading 06efbd2bf.png to /content\n",
            "  0% 0.00/21.1k [00:00<?, ?B/s]\n",
            "100% 21.1k/21.1k [00:00<00:00, 23.4MB/s]\n",
            "Downloading 0975602f4.png to /content\n",
            "  0% 0.00/18.5k [00:00<?, ?B/s]\n",
            "100% 18.5k/18.5k [00:00<00:00, 21.7MB/s]\n",
            "Downloading 126c8b947.png to /content\n",
            "  0% 0.00/368k [00:00<?, ?B/s]\n",
            "100% 368k/368k [00:00<00:00, 120MB/s]\n",
            "Downloading 012db0f43.png to /content\n",
            "  0% 0.00/637k [00:00<?, ?B/s]\n",
            "100% 637k/637k [00:00<00:00, 89.2MB/s]\n",
            "Downloading 096ec46ec.png to /content\n",
            "  0% 0.00/316k [00:00<?, ?B/s]\n",
            "100% 316k/316k [00:00<00:00, 101MB/s]\n",
            "Downloading 106bfb13a.png to /content\n",
            "  0% 0.00/575k [00:00<?, ?B/s]\n",
            "100% 575k/575k [00:00<00:00, 78.3MB/s]\n",
            "Downloading 12e0ffb23.png to /content\n",
            "  0% 0.00/641k [00:00<?, ?B/s]\n",
            "100% 641k/641k [00:00<00:00, 41.5MB/s]\n",
            "Downloading 0a4a26651.png to /content\n",
            "  0% 0.00/347k [00:00<?, ?B/s]\n",
            "100% 347k/347k [00:00<00:00, 114MB/s]\n",
            "Downloading 143774101.png to /content\n",
            "  0% 0.00/55.8k [00:00<?, ?B/s]\n",
            "100% 55.8k/55.8k [00:00<00:00, 56.9MB/s]\n",
            "Downloading 0df5ee8a2.png to /content\n",
            "  0% 0.00/306k [00:00<?, ?B/s]\n",
            "100% 306k/306k [00:00<00:00, 99.5MB/s]\n",
            "Downloading 136134853.png to /content\n",
            "  0% 0.00/733k [00:00<?, ?B/s]\n",
            "100% 733k/733k [00:00<00:00, 47.7MB/s]\n",
            "Downloading 0ec23ca76.png to /content\n",
            "  0% 0.00/14.4k [00:00<?, ?B/s]\n",
            "100% 14.4k/14.4k [00:00<00:00, 15.0MB/s]\n",
            "Downloading 0ff464e3e.png to /content\n",
            "  0% 0.00/683k [00:00<?, ?B/s]\n",
            "100% 683k/683k [00:00<00:00, 44.5MB/s]\n",
            "Downloading 01a2ae45e.png to /content\n",
            "  0% 0.00/10.3k [00:00<?, ?B/s]\n",
            "100% 10.3k/10.3k [00:00<00:00, 9.76MB/s]\n",
            "Downloading 108bf8703.png to /content\n",
            "  0% 0.00/438k [00:00<?, ?B/s]\n",
            "100% 438k/438k [00:00<00:00, 60.6MB/s]\n",
            "Downloading 04468fad4.png to /content\n",
            "  0% 0.00/469k [00:00<?, ?B/s]\n",
            "100% 469k/469k [00:00<00:00, 64.2MB/s]\n",
            "Downloading 138581771.png to /content\n",
            "  0% 0.00/337k [00:00<?, ?B/s]\n",
            "100% 337k/337k [00:00<00:00, 109MB/s]\n",
            "Downloading 060e8f499.png to /content\n",
            "  0% 0.00/11.3k [00:00<?, ?B/s]\n",
            "100% 11.3k/11.3k [00:00<00:00, 11.7MB/s]\n",
            "Downloading 1478bcfcd.png to /content\n",
            "  0% 0.00/10.5k [00:00<?, ?B/s]\n",
            "100% 10.5k/10.5k [00:00<00:00, 16.6MB/s]\n",
            "Downloading 11b88da7c.png to /content\n",
            "  0% 0.00/22.7k [00:00<?, ?B/s]\n",
            "100% 22.7k/22.7k [00:00<00:00, 24.6MB/s]\n",
            "Downloading 0382d0faf.png to /content\n",
            "  0% 0.00/807k [00:00<?, ?B/s]\n",
            "100% 807k/807k [00:00<00:00, 52.9MB/s]\n",
            "Downloading 04559c69c.png to /content\n",
            "  0% 0.00/6.25k [00:00<?, ?B/s]\n",
            "100% 6.25k/6.25k [00:00<00:00, 6.86MB/s]\n",
            "Downloading 0958bdb64.png to /content\n",
            "  0% 0.00/573k [00:00<?, ?B/s]\n",
            "100% 573k/573k [00:00<00:00, 38.0MB/s]\n",
            "Downloading 071907a13.png to /content\n",
            "  0% 0.00/280k [00:00<?, ?B/s]\n",
            "100% 280k/280k [00:00<00:00, 75.3MB/s]\n",
            "Downloading 00e049fe8.png to /content\n",
            "  0% 0.00/29.6k [00:00<?, ?B/s]\n",
            "100% 29.6k/29.6k [00:00<00:00, 30.9MB/s]\n",
            "Downloading 09d34fe5b.png to /content\n",
            "  0% 0.00/152k [00:00<?, ?B/s]\n",
            "100% 152k/152k [00:00<00:00, 47.4MB/s]\n",
            "Downloading 06a64ac47.png to /content\n",
            "  0% 0.00/557k [00:00<?, ?B/s]\n",
            "100% 557k/557k [00:00<00:00, 77.1MB/s]\n",
            "Downloading 003402ea0.png to /content\n",
            "  0% 0.00/185k [00:00<?, ?B/s]\n",
            "100% 185k/185k [00:00<00:00, 59.7MB/s]\n",
            "Downloading 055922489.png to /content\n",
            "  0% 0.00/187k [00:00<?, ?B/s]\n",
            "100% 187k/187k [00:00<00:00, 62.0MB/s]\n",
            "Downloading 09be59300.png to /content\n",
            "  0% 0.00/41.2k [00:00<?, ?B/s]\n",
            "100% 41.2k/41.2k [00:00<00:00, 42.9MB/s]\n",
            "Downloading 00cc58829.png to /content\n",
            "  0% 0.00/109k [00:00<?, ?B/s]\n",
            "100% 109k/109k [00:00<00:00, 89.0MB/s]\n",
            "Downloading 05598e057.png to /content\n",
            "  0% 0.00/32.8k [00:00<?, ?B/s]\n",
            "100% 32.8k/32.8k [00:00<00:00, 34.1MB/s]\n",
            "Downloading 01e18fd1a.png to /content\n",
            "  0% 0.00/225k [00:00<?, ?B/s]\n",
            "100% 225k/225k [00:00<00:00, 73.7MB/s]\n",
            "Downloading 076b550b6.png to /content\n",
            "  0% 0.00/265k [00:00<?, ?B/s]\n",
            "100% 265k/265k [00:00<00:00, 86.3MB/s]\n",
            "Downloading 018c28574.png to /content\n",
            "  0% 0.00/46.2k [00:00<?, ?B/s]\n",
            "100% 46.2k/46.2k [00:00<00:00, 47.5MB/s]\n",
            "Downloading 027f505f5.png to /content\n",
            "  0% 0.00/124k [00:00<?, ?B/s]\n",
            "100% 124k/124k [00:00<00:00, 112MB/s]\n",
            "Downloading 028329f45.png to /content\n",
            "  0% 0.00/70.1k [00:00<?, ?B/s]\n",
            "100% 70.1k/70.1k [00:00<00:00, 36.5MB/s]\n",
            "Downloading 0268ecbe2.png to /content\n",
            "  0% 0.00/28.9k [00:00<?, ?B/s]\n",
            "100% 28.9k/28.9k [00:00<00:00, 30.1MB/s]\n",
            "Downloading 00d030ea0.png to /content\n",
            "  0% 0.00/862k [00:00<?, ?B/s]\n",
            "100% 862k/862k [00:00<00:00, 57.1MB/s]\n",
            "Downloading 085debc4e.png to /content\n",
            "  0% 0.00/21.2k [00:00<?, ?B/s]\n",
            "100% 21.2k/21.2k [00:00<00:00, 22.5MB/s]\n",
            "Downloading 05e5b5cb6.png to /content\n",
            "  0% 0.00/55.4k [00:00<?, ?B/s]\n",
            "100% 55.4k/55.4k [00:00<00:00, 57.7MB/s]\n",
            "Downloading 08fe5538d.png to /content\n",
            "  0% 0.00/18.1k [00:00<?, ?B/s]\n",
            "100% 18.1k/18.1k [00:00<00:00, 18.3MB/s]\n",
            "Downloading 07d939b1e.png to /content\n",
            "  0% 0.00/17.7k [00:00<?, ?B/s]\n",
            "100% 17.7k/17.7k [00:00<00:00, 19.5MB/s]\n",
            "Downloading 077190c7a.png to /content\n",
            "  0% 0.00/201k [00:00<?, ?B/s]\n",
            "100% 201k/201k [00:00<00:00, 66.6MB/s]\n",
            "Downloading 00268e97d.png to /content\n",
            "  0% 0.00/26.7k [00:00<?, ?B/s]\n",
            "100% 26.7k/26.7k [00:00<00:00, 27.6MB/s]\n",
            "429 - Too Many Requests\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPDK6d5wcguT",
        "colab_type": "text"
      },
      "source": [
        "The kaggle was responding with 429 - Too many requests even after multiple attempts. The data was not fully downloaded as well.\n",
        "\n",
        "So downloaded the data on to the google drive and analyzing the same from the downloaded data on drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nviOjlNdR1z",
        "colab_type": "text"
      },
      "source": [
        "## Download the data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vx4YCXKhF8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "d153deb4-c2dd-4e31-eb0d-67fee0cb888e"
      },
      "source": [
        "# run this cell to to mount the google drive if you are using google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "project_path = '/content/drive/My Drive/assignments/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0yRjPozZg5_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5131da7e-0d2d-413e-b140-dfd89920b982"
      },
      "source": [
        "#Unzip the data from the gdive\n",
        "!unzip '/content/drive/My Drive/assignments/plant-seedlings-classification.zip'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: test/adb7a032c.png      \n",
            "  inflating: test/ae90f2827.png      \n",
            "  inflating: test/aecfaed64.png      \n",
            "  inflating: test/aee6fa3df.png      \n",
            "  inflating: test/af45e222a.png      \n",
            "  inflating: test/afa446484.png      \n",
            "  inflating: test/afcf6abd5.png      \n",
            "  inflating: test/b026bf8ca.png      \n",
            "  inflating: test/b03397525.png      \n",
            "  inflating: test/b0acaff4a.png      \n",
            "  inflating: test/b130a0632.png      \n",
            "  inflating: test/b145ba9d4.png      \n",
            "  inflating: test/b1cd2a91e.png      \n",
            "  inflating: test/b215531dd.png      \n",
            "  inflating: test/b2706e2b3.png      \n",
            "  inflating: test/b29339405.png      \n",
            "  inflating: test/b30ab4659.png      \n",
            "  inflating: test/b31292706.png      \n",
            "  inflating: test/b341d0aab.png      \n",
            "  inflating: test/b39c71707.png      \n",
            "  inflating: test/b3d6fdb80.png      \n",
            "  inflating: test/b3e08b037.png      \n",
            "  inflating: test/b47691c08.png      \n",
            "  inflating: test/b4c3df835.png      \n",
            "  inflating: test/b4f7c9214.png      \n",
            "  inflating: test/b573b7a56.png      \n",
            "  inflating: test/b5c7fd009.png      \n",
            "  inflating: test/b62dca166.png      \n",
            "  inflating: test/b687160f5.png      \n",
            "  inflating: test/b6a3f7876.png      \n",
            "  inflating: test/b6f3d8b5d.png      \n",
            "  inflating: test/b7192c70f.png      \n",
            "  inflating: test/b7a7f6390.png      \n",
            "  inflating: test/b7ad92859.png      \n",
            "  inflating: test/b828443ff.png      \n",
            "  inflating: test/b9062c1c8.png      \n",
            "  inflating: test/b944a49ca.png      \n",
            "  inflating: test/b98327bf4.png      \n",
            "  inflating: test/ba3ce6b3e.png      \n",
            "  inflating: test/bb1c84bbc.png      \n",
            "  inflating: test/bb1d1bfd3.png      \n",
            "  inflating: test/bb20fce02.png      \n",
            "  inflating: test/bb64660b7.png      \n",
            "  inflating: test/bb7621cb3.png      \n",
            "  inflating: test/bd72d4d8a.png      \n",
            "  inflating: test/bd789d151.png      \n",
            "  inflating: test/bdde957ec.png      \n",
            "  inflating: test/be2499cf4.png      \n",
            "  inflating: test/be341dbdc.png      \n",
            "  inflating: test/bea23d9f8.png      \n",
            "  inflating: test/bebcaab66.png      \n",
            "  inflating: test/beebe5f4e.png      \n",
            "  inflating: test/bf3924a57.png      \n",
            "  inflating: test/bf66b9cd2.png      \n",
            "  inflating: test/bfab3e3d0.png      \n",
            "  inflating: test/bffc08672.png      \n",
            "  inflating: test/c0461776c.png      \n",
            "  inflating: test/c069fc3fa.png      \n",
            "  inflating: test/c06e7c748.png      \n",
            "  inflating: test/c0bc3997b.png      \n",
            "  inflating: test/c0d9e170b.png      \n",
            "  inflating: test/c0f5d9ac8.png      \n",
            "  inflating: test/c10ccbd82.png      \n",
            "  inflating: test/c10db7ae2.png      \n",
            "  inflating: test/c1ecff98b.png      \n",
            "  inflating: test/c26ccf73c.png      \n",
            "  inflating: test/c2de6020a.png      \n",
            "  inflating: test/c35efa095.png      \n",
            "  inflating: test/c4ed8ed38.png      \n",
            "  inflating: test/c50335991.png      \n",
            "  inflating: test/c5078bac5.png      \n",
            "  inflating: test/c5e419015.png      \n",
            "  inflating: test/c5e88cd42.png      \n",
            "  inflating: test/c60e91e07.png      \n",
            "  inflating: test/c63da993b.png      \n",
            "  inflating: test/c64370a72.png      \n",
            "  inflating: test/c6b76307d.png      \n",
            "  inflating: test/c6c8d4ba0.png      \n",
            "  inflating: test/c7051c902.png      \n",
            "  inflating: test/c74c5b7fc.png      \n",
            "  inflating: test/c75a82234.png      \n",
            "  inflating: test/c7ae30f3a.png      \n",
            "  inflating: test/c7b07431e.png      \n",
            "  inflating: test/c7eb96871.png      \n",
            "  inflating: test/c832e4302.png      \n",
            "  inflating: test/c85ef220d.png      \n",
            "  inflating: test/c88ebfb47.png      \n",
            "  inflating: test/c8f50f0c3.png      \n",
            "  inflating: test/caa2fbd79.png      \n",
            "  inflating: test/cadab6616.png      \n",
            "  inflating: test/cae684f8f.png      \n",
            "  inflating: test/cb496f36e.png      \n",
            "  inflating: test/cb76a7766.png      \n",
            "  inflating: test/cbba27d89.png      \n",
            "  inflating: test/cbe761896.png      \n",
            "  inflating: test/cc3d2a59a.png      \n",
            "  inflating: test/cc74feadc.png      \n",
            "  inflating: test/cd5f0db1c.png      \n",
            "  inflating: test/cd6adba97.png      \n",
            "  inflating: test/ce15eee52.png      \n",
            "  inflating: test/ce3d280eb.png      \n",
            "  inflating: test/ce42adffb.png      \n",
            "  inflating: test/cec5bf198.png      \n",
            "  inflating: test/cf3a8b2fd.png      \n",
            "  inflating: test/cf46d09c5.png      \n",
            "  inflating: test/cf90fc52d.png      \n",
            "  inflating: test/cfb18d262.png      \n",
            "  inflating: test/cfd8165e9.png      \n",
            "  inflating: test/d0152bd7c.png      \n",
            "  inflating: test/d01873fdd.png      \n",
            "  inflating: test/d09275360.png      \n",
            "  inflating: test/d09d24c58.png      \n",
            "  inflating: test/d0cdc768f.png      \n",
            "  inflating: test/d102e1a15.png      \n",
            "  inflating: test/d14aa43f3.png      \n",
            "  inflating: test/d17f48d3b.png      \n",
            "  inflating: test/d2f0f326e.png      \n",
            "  inflating: test/d2f422ccb.png      \n",
            "  inflating: test/d2fd9df40.png      \n",
            "  inflating: test/d3331e071.png      \n",
            "  inflating: test/d350a25fa.png      \n",
            "  inflating: test/d41d87796.png      \n",
            "  inflating: test/d488a4fe1.png      \n",
            "  inflating: test/d515398fd.png      \n",
            "  inflating: test/d563be369.png      \n",
            "  inflating: test/d5f7dd60a.png      \n",
            "  inflating: test/d668409ff.png      \n",
            "  inflating: test/d689256be.png      \n",
            "  inflating: test/d6c8c3c48.png      \n",
            "  inflating: test/d6d31dcbe.png      \n",
            "  inflating: test/d6d80a321.png      \n",
            "  inflating: test/d7017f701.png      \n",
            "  inflating: test/d84d37a61.png      \n",
            "  inflating: test/d89db156f.png      \n",
            "  inflating: test/d8f4923f8.png      \n",
            "  inflating: test/d93c7ab6d.png      \n",
            "  inflating: test/d9c50616e.png      \n",
            "  inflating: test/da231c97f.png      \n",
            "  inflating: test/da4ed3a28.png      \n",
            "  inflating: test/da5255450.png      \n",
            "  inflating: test/da713c465.png      \n",
            "  inflating: test/da9ef7858.png      \n",
            "  inflating: test/dabe3e5be.png      \n",
            "  inflating: test/dabea05f4.png      \n",
            "  inflating: test/dc4cd56a3.png      \n",
            "  inflating: test/dc55449b2.png      \n",
            "  inflating: test/dcd7ff249.png      \n",
            "  inflating: test/dce2f6612.png      \n",
            "  inflating: test/dd5ec63d9.png      \n",
            "  inflating: test/dd9f36df7.png      \n",
            "  inflating: test/de0b79659.png      \n",
            "  inflating: test/df11d56a7.png      \n",
            "  inflating: test/df521c0c0.png      \n",
            "  inflating: test/df7cb5f87.png      \n",
            "  inflating: test/dfb1d9012.png      \n",
            "  inflating: test/e0ec5b6a1.png      \n",
            "  inflating: test/e14afa235.png      \n",
            "  inflating: test/e15472085.png      \n",
            "  inflating: test/e15fce4f2.png      \n",
            "  inflating: test/e1809cef2.png      \n",
            "  inflating: test/e19673dc9.png      \n",
            "  inflating: test/e19ad6ac9.png      \n",
            "  inflating: test/e1a0e3202.png      \n",
            "  inflating: test/e1abb4ff9.png      \n",
            "  inflating: test/e3f50adfc.png      \n",
            "  inflating: test/e471f1d3a.png      \n",
            "  inflating: test/e478c452c.png      \n",
            "  inflating: test/e4a76885b.png      \n",
            "  inflating: test/e4d5ec761.png      \n",
            "  inflating: test/e5064f6be.png      \n",
            "  inflating: test/e52493d0b.png      \n",
            "  inflating: test/e5297b675.png      \n",
            "  inflating: test/e5368474f.png      \n",
            "  inflating: test/e5881dd33.png      \n",
            "  inflating: test/e5e3dccff.png      \n",
            "  inflating: test/e6f1211a2.png      \n",
            "  inflating: test/e7077322d.png      \n",
            "  inflating: test/e721c6ac8.png      \n",
            "  inflating: test/e73e308be.png      \n",
            "  inflating: test/e783f5a4f.png      \n",
            "  inflating: test/e80a259c5.png      \n",
            "  inflating: test/e82017baa.png      \n",
            "  inflating: test/e84464f5a.png      \n",
            "  inflating: test/e88bf0db9.png      \n",
            "  inflating: test/e901b0f28.png      \n",
            "  inflating: test/e921021a8.png      \n",
            "  inflating: test/e96e57a90.png      \n",
            "  inflating: test/e98e5d1d5.png      \n",
            "  inflating: test/e9cd91682.png      \n",
            "  inflating: test/e9d48d664.png      \n",
            "  inflating: test/eaf0815e2.png      \n",
            "  inflating: test/ec08a5d56.png      \n",
            "  inflating: test/ede6b84b4.png      \n",
            "  inflating: test/edfdb4aeb.png      \n",
            "  inflating: test/eec1079a1.png      \n",
            "  inflating: test/eef131644.png      \n",
            "  inflating: test/ef02b4ee7.png      \n",
            "  inflating: test/ef3e232ad.png      \n",
            "  inflating: test/ef65533d5.png      \n",
            "  inflating: test/ef74dbcad.png      \n",
            "  inflating: test/ef7a5651d.png      \n",
            "  inflating: test/ef9676433.png      \n",
            "  inflating: test/efe19dc32.png      \n",
            "  inflating: test/f0ffa00bd.png      \n",
            "  inflating: test/f1e87cba7.png      \n",
            "  inflating: test/f1f7c833f.png      \n",
            "  inflating: test/f23faf9c1.png      \n",
            "  inflating: test/f25996db8.png      \n",
            "  inflating: test/f2dc546ca.png      \n",
            "  inflating: test/f33e9d918.png      \n",
            "  inflating: test/f351ce097.png      \n",
            "  inflating: test/f3fcfff1b.png      \n",
            "  inflating: test/f4021df6c.png      \n",
            "  inflating: test/f4234cf4f.png      \n",
            "  inflating: test/f445fe6fb.png      \n",
            "  inflating: test/f48916a8c.png      \n",
            "  inflating: test/f4ad9d950.png      \n",
            "  inflating: test/f4caf74f9.png      \n",
            "  inflating: test/f4e7733d4.png      \n",
            "  inflating: test/f593c9cf0.png      \n",
            "  inflating: test/f66ae4070.png      \n",
            "  inflating: test/f6d250856.png      \n",
            "  inflating: test/f8318faf1.png      \n",
            "  inflating: test/f85ed9b6d.png      \n",
            "  inflating: test/f9b6bfb00.png      \n",
            "  inflating: test/f9ea23fb5.png      \n",
            "  inflating: test/f9f35cbd4.png      \n",
            "  inflating: test/fa5fd1384.png      \n",
            "  inflating: test/fa9f3a8f9.png      \n",
            "  inflating: test/fadc6adbc.png      \n",
            "  inflating: test/fb022edf9.png      \n",
            "  inflating: test/fba8fc78a.png      \n",
            "  inflating: test/fbf88b6be.png      \n",
            "  inflating: test/fc3e58836.png      \n",
            "  inflating: test/fc6f686fb.png      \n",
            "  inflating: test/fd253a74e.png      \n",
            "  inflating: test/fd87b36ae.png      \n",
            "  inflating: test/fd925f542.png      \n",
            "  inflating: test/fda0b5c38.png      \n",
            "  inflating: test/fda39e16f.png      \n",
            "  inflating: test/fdea6b119.png      \n",
            "  inflating: test/fe29629fb.png      \n",
            "  inflating: test/fe9e87b78.png      \n",
            "  inflating: test/fea1d13d6.png      \n",
            "  inflating: test/fea355851.png      \n",
            "  inflating: test/fea3da57c.png      \n",
            "  inflating: test/fef2ade8c.png      \n",
            "  inflating: test/ff65bc002.png      \n",
            "  inflating: test/ffc6f8527.png      \n",
            "  inflating: train/Black-grass/0050f38b3.png  \n",
            "  inflating: train/Black-grass/0183fdf68.png  \n",
            "  inflating: train/Black-grass/0260cffa8.png  \n",
            "  inflating: train/Black-grass/05eedce4d.png  \n",
            "  inflating: train/Black-grass/075d004bc.png  \n",
            "  inflating: train/Black-grass/078eae073.png  \n",
            "  inflating: train/Black-grass/082314602.png  \n",
            "  inflating: train/Black-grass/0ace21089.png  \n",
            "  inflating: train/Black-grass/0b228a6b8.png  \n",
            "  inflating: train/Black-grass/0b3e7a7a9.png  \n",
            "  inflating: train/Black-grass/0bb75ded8.png  \n",
            "  inflating: train/Black-grass/0be707615.png  \n",
            "  inflating: train/Black-grass/0c67c3fc3.png  \n",
            "  inflating: train/Black-grass/0d1a9985f.png  \n",
            "  inflating: train/Black-grass/0d28c429b.png  \n",
            "  inflating: train/Black-grass/0d4f74f4a.png  \n",
            "  inflating: train/Black-grass/0dad57e7f.png  \n",
            "  inflating: train/Black-grass/0e91f92a1.png  \n",
            "  inflating: train/Black-grass/0fe440ed5.png  \n",
            "  inflating: train/Black-grass/1276dffba.png  \n",
            "  inflating: train/Black-grass/129c51855.png  \n",
            "  inflating: train/Black-grass/13a7f553a.png  \n",
            "  inflating: train/Black-grass/14719a83e.png  \n",
            "  inflating: train/Black-grass/1576ce9fd.png  \n",
            "  inflating: train/Black-grass/163c13912.png  \n",
            "  inflating: train/Black-grass/163e571a6.png  \n",
            "  inflating: train/Black-grass/16c69a6d8.png  \n",
            "  inflating: train/Black-grass/173cec485.png  \n",
            "  inflating: train/Black-grass/1a125880e.png  \n",
            "  inflating: train/Black-grass/1af1eddd3.png  \n",
            "  inflating: train/Black-grass/1d39b8f30.png  \n",
            "  inflating: train/Black-grass/1e49633e0.png  \n",
            "  inflating: train/Black-grass/20b2cbaed.png  \n",
            "  inflating: train/Black-grass/20d4fab57.png  \n",
            "  inflating: train/Black-grass/2269e0a1e.png  \n",
            "  inflating: train/Black-grass/228d8ad5c.png  \n",
            "  inflating: train/Black-grass/22be204a3.png  \n",
            "  inflating: train/Black-grass/25daae389.png  \n",
            "  inflating: train/Black-grass/260c4eed3.png  \n",
            "  inflating: train/Black-grass/26103af9c.png  \n",
            "  inflating: train/Black-grass/279ac215b.png  \n",
            "  inflating: train/Black-grass/28a707630.png  \n",
            "  inflating: train/Black-grass/2aa60045d.png  \n",
            "  inflating: train/Black-grass/2ed589264.png  \n",
            "  inflating: train/Black-grass/2ee4dad8c.png  \n",
            "  inflating: train/Black-grass/2f0ae1b34.png  \n",
            "  inflating: train/Black-grass/2f6bc240a.png  \n",
            "  inflating: train/Black-grass/3002e5d9d.png  \n",
            "  inflating: train/Black-grass/31958c132.png  \n",
            "  inflating: train/Black-grass/31f2766cb.png  \n",
            "  inflating: train/Black-grass/32d97b170.png  \n",
            "  inflating: train/Black-grass/332f68a21.png  \n",
            "  inflating: train/Black-grass/34a672a63.png  \n",
            "  inflating: train/Black-grass/355cad34c.png  \n",
            "  inflating: train/Black-grass/37d85d833.png  \n",
            "  inflating: train/Black-grass/39e9bf4c7.png  \n",
            "  inflating: train/Black-grass/3a8c485bc.png  \n",
            "  inflating: train/Black-grass/3b7266ac3.png  \n",
            "  inflating: train/Black-grass/3b7d1fe82.png  \n",
            "  inflating: train/Black-grass/3dc08a0f7.png  \n",
            "  inflating: train/Black-grass/3de7650a2.png  \n",
            "  inflating: train/Black-grass/3e9ef1999.png  \n",
            "  inflating: train/Black-grass/3f268bcf8.png  \n",
            "  inflating: train/Black-grass/3fb361e79.png  \n",
            "  inflating: train/Black-grass/3ff68fa8a.png  \n",
            "  inflating: train/Black-grass/40bf7be90.png  \n",
            "  inflating: train/Black-grass/418808d19.png  \n",
            "  inflating: train/Black-grass/42336b187.png  \n",
            "  inflating: train/Black-grass/429a48df1.png  \n",
            "  inflating: train/Black-grass/42af989bc.png  \n",
            "  inflating: train/Black-grass/448a59eac.png  \n",
            "  inflating: train/Black-grass/455546801.png  \n",
            "  inflating: train/Black-grass/461feacba.png  \n",
            "  inflating: train/Black-grass/470608aba.png  \n",
            "  inflating: train/Black-grass/48141d6a7.png  \n",
            "  inflating: train/Black-grass/495cebacf.png  \n",
            "  inflating: train/Black-grass/498269666.png  \n",
            "  inflating: train/Black-grass/4a3b96198.png  \n",
            "  inflating: train/Black-grass/4a7e7eba8.png  \n",
            "  inflating: train/Black-grass/4cd2a07dd.png  \n",
            "  inflating: train/Black-grass/4cf922aea.png  \n",
            "  inflating: train/Black-grass/4e1cb1e27.png  \n",
            "  inflating: train/Black-grass/4f0dcbcc3.png  \n",
            "  inflating: train/Black-grass/4f48eb987.png  \n",
            "  inflating: train/Black-grass/5212d8564.png  \n",
            "  inflating: train/Black-grass/5296a06e6.png  \n",
            "  inflating: train/Black-grass/53ab7a3da.png  \n",
            "  inflating: train/Black-grass/54b2dac6e.png  \n",
            "  inflating: train/Black-grass/54c6dbde4.png  \n",
            "  inflating: train/Black-grass/550dfcb36.png  \n",
            "  inflating: train/Black-grass/594485a0c.png  \n",
            "  inflating: train/Black-grass/595e77ddf.png  \n",
            "  inflating: train/Black-grass/5a1295fb4.png  \n",
            "  inflating: train/Black-grass/5a8b75712.png  \n",
            "  inflating: train/Black-grass/5c405ae2d.png  \n",
            "  inflating: train/Black-grass/5d358beb9.png  \n",
            "  inflating: train/Black-grass/5db29d0b5.png  \n",
            "  inflating: train/Black-grass/5e21fa6f1.png  \n",
            "  inflating: train/Black-grass/5e4d1ee0d.png  \n",
            "  inflating: train/Black-grass/5e65dbdd7.png  \n",
            "  inflating: train/Black-grass/6104de96e.png  \n",
            "  inflating: train/Black-grass/6172f64fd.png  \n",
            "  inflating: train/Black-grass/6182bd48c.png  \n",
            "  inflating: train/Black-grass/675a6956e.png  \n",
            "  inflating: train/Black-grass/686132594.png  \n",
            "  inflating: train/Black-grass/69ad6773e.png  \n",
            "  inflating: train/Black-grass/6a19547c5.png  \n",
            "  inflating: train/Black-grass/6aabdeb45.png  \n",
            "  inflating: train/Black-grass/6afa7c717.png  \n",
            "  inflating: train/Black-grass/6b9ebf8cc.png  \n",
            "  inflating: train/Black-grass/6e027ec7d.png  \n",
            "  inflating: train/Black-grass/6e193f1bb.png  \n",
            "  inflating: train/Black-grass/7050b0b8a.png  \n",
            "  inflating: train/Black-grass/70bfa70ff.png  \n",
            "  inflating: train/Black-grass/71f6e3227.png  \n",
            "  inflating: train/Black-grass/72fd52505.png  \n",
            "  inflating: train/Black-grass/75ef53b3b.png  \n",
            "  inflating: train/Black-grass/765a69082.png  \n",
            "  inflating: train/Black-grass/775735fb9.png  \n",
            "  inflating: train/Black-grass/77629b9e3.png  \n",
            "  inflating: train/Black-grass/7b71d3e65.png  \n",
            "  inflating: train/Black-grass/7b72b398d.png  \n",
            "  inflating: train/Black-grass/7e1bf9449.png  \n",
            "  inflating: train/Black-grass/7f37cd4e4.png  \n",
            "  inflating: train/Black-grass/7f84c8699.png  \n",
            "  inflating: train/Black-grass/7fa6dbe11.png  \n",
            "  inflating: train/Black-grass/8029e3396.png  \n",
            "  inflating: train/Black-grass/807f9b257.png  \n",
            "  inflating: train/Black-grass/82e0d98d2.png  \n",
            "  inflating: train/Black-grass/840a7ed59.png  \n",
            "  inflating: train/Black-grass/84e43f2ff.png  \n",
            "  inflating: train/Black-grass/850a09a6b.png  \n",
            "  inflating: train/Black-grass/86dfe670c.png  \n",
            "  inflating: train/Black-grass/87dd8ebac.png  \n",
            "  inflating: train/Black-grass/88fceea2f.png  \n",
            "  inflating: train/Black-grass/891c720f8.png  \n",
            "  inflating: train/Black-grass/898ecfa78.png  \n",
            "  inflating: train/Black-grass/89f06ca64.png  \n",
            "  inflating: train/Black-grass/8dd397cd9.png  \n",
            "  inflating: train/Black-grass/9052e3e7f.png  \n",
            "  inflating: train/Black-grass/90ea1e327.png  \n",
            "  inflating: train/Black-grass/91a175741.png  \n",
            "  inflating: train/Black-grass/92e5fcce4.png  \n",
            "  inflating: train/Black-grass/93f68d1a9.png  \n",
            "  inflating: train/Black-grass/9443199bb.png  \n",
            "  inflating: train/Black-grass/957b8523c.png  \n",
            "  inflating: train/Black-grass/963dbc831.png  \n",
            "  inflating: train/Black-grass/97cbef805.png  \n",
            "  inflating: train/Black-grass/983663c56.png  \n",
            "  inflating: train/Black-grass/9959fb099.png  \n",
            "  inflating: train/Black-grass/9b1c42272.png  \n",
            "  inflating: train/Black-grass/9e2bfa93d.png  \n",
            "  inflating: train/Black-grass/9e99f6a34.png  \n",
            "  inflating: train/Black-grass/a03bc7b24.png  \n",
            "  inflating: train/Black-grass/a0405de4d.png  \n",
            "  inflating: train/Black-grass/a08892355.png  \n",
            "  inflating: train/Black-grass/a0baf5f7b.png  \n",
            "  inflating: train/Black-grass/a1cb5a321.png  \n",
            "  inflating: train/Black-grass/a20b64ac6.png  \n",
            "  inflating: train/Black-grass/a26cb8017.png  \n",
            "  inflating: train/Black-grass/a37d61200.png  \n",
            "  inflating: train/Black-grass/a47cfeec4.png  \n",
            "  inflating: train/Black-grass/a53088ca0.png  \n",
            "  inflating: train/Black-grass/a5f23b59f.png  \n",
            "  inflating: train/Black-grass/a6f939a8b.png  \n",
            "  inflating: train/Black-grass/a7d2b005e.png  \n",
            "  inflating: train/Black-grass/a87fd277c.png  \n",
            "  inflating: train/Black-grass/a8ab1ff26.png  \n",
            "  inflating: train/Black-grass/a8cdae28a.png  \n",
            "  inflating: train/Black-grass/a8de8a80a.png  \n",
            "  inflating: train/Black-grass/aa5bb06a1.png  \n",
            "  inflating: train/Black-grass/ab479d343.png  \n",
            "  inflating: train/Black-grass/ab787fb46.png  \n",
            "  inflating: train/Black-grass/abe0f4751.png  \n",
            "  inflating: train/Black-grass/ac47ebc4d.png  \n",
            "  inflating: train/Black-grass/ac56bd408.png  \n",
            "  inflating: train/Black-grass/adc5443dc.png  \n",
            "  inflating: train/Black-grass/ade525bad.png  \n",
            "  inflating: train/Black-grass/ae8f69724.png  \n",
            "  inflating: train/Black-grass/af1b91028.png  \n",
            "  inflating: train/Black-grass/af3e2c6da.png  \n",
            "  inflating: train/Black-grass/afaade548.png  \n",
            "  inflating: train/Black-grass/b024eeb75.png  \n",
            "  inflating: train/Black-grass/b26a7a6ed.png  \n",
            "  inflating: train/Black-grass/b4b8b1507.png  \n",
            "  inflating: train/Black-grass/b504c071f.png  \n",
            "  inflating: train/Black-grass/b561b3bc2.png  \n",
            "  inflating: train/Black-grass/b790f7be5.png  \n",
            "  inflating: train/Black-grass/b937353c0.png  \n",
            "  inflating: train/Black-grass/b9dfffe2a.png  \n",
            "  inflating: train/Black-grass/b9e36fa79.png  \n",
            "  inflating: train/Black-grass/bab8eb04a.png  \n",
            "  inflating: train/Black-grass/bac2710a2.png  \n",
            "  inflating: train/Black-grass/bc68a27f9.png  \n",
            "  inflating: train/Black-grass/befaed3e4.png  \n",
            "  inflating: train/Black-grass/bf5662989.png  \n",
            "  inflating: train/Black-grass/c025e2886.png  \n",
            "  inflating: train/Black-grass/c0cbaa32c.png  \n",
            "  inflating: train/Black-grass/c11422bb2.png  \n",
            "  inflating: train/Black-grass/c1a625098.png  \n",
            "  inflating: train/Black-grass/c1ab59648.png  \n",
            "  inflating: train/Black-grass/c39541d9a.png  \n",
            "  inflating: train/Black-grass/c3b38d028.png  \n",
            "  inflating: train/Black-grass/c66bab8b6.png  \n",
            "  inflating: train/Black-grass/c8884407d.png  \n",
            "  inflating: train/Black-grass/c999c3095.png  \n",
            "  inflating: train/Black-grass/c9f6ffa0c.png  \n",
            "  inflating: train/Black-grass/cbeb36bc6.png  \n",
            "  inflating: train/Black-grass/cc90c662f.png  \n",
            "  inflating: train/Black-grass/cd8031a0c.png  \n",
            "  inflating: train/Black-grass/cfbe062b3.png  \n",
            "  inflating: train/Black-grass/d090d6b25.png  \n",
            "  inflating: train/Black-grass/d0ad9c78b.png  \n",
            "  inflating: train/Black-grass/d112c9c28.png  \n",
            "  inflating: train/Black-grass/d3c72d4c3.png  \n",
            "  inflating: train/Black-grass/d3e69adad.png  \n",
            "  inflating: train/Black-grass/d3ff1a639.png  \n",
            "  inflating: train/Black-grass/d441eeda3.png  \n",
            "  inflating: train/Black-grass/d6036a0f4.png  \n",
            "  inflating: train/Black-grass/d622ca3d2.png  \n",
            "  inflating: train/Black-grass/d8afd58f3.png  \n",
            "  inflating: train/Black-grass/da4f48653.png  \n",
            "  inflating: train/Black-grass/da5082ce2.png  \n",
            "  inflating: train/Black-grass/daa7d4620.png  \n",
            "  inflating: train/Black-grass/db337c4e7.png  \n",
            "  inflating: train/Black-grass/dca86daba.png  \n",
            "  inflating: train/Black-grass/dcbd3fa08.png  \n",
            "  inflating: train/Black-grass/dd091a2a9.png  \n",
            "  inflating: train/Black-grass/dde665ea5.png  \n",
            "  inflating: train/Black-grass/df2e6e002.png  \n",
            "  inflating: train/Black-grass/e0380dff9.png  \n",
            "  inflating: train/Black-grass/e2b2a20b2.png  \n",
            "  inflating: train/Black-grass/e47987eab.png  \n",
            "  inflating: train/Black-grass/e4af651a3.png  \n",
            "  inflating: train/Black-grass/e5a6e8ebc.png  \n",
            "  inflating: train/Black-grass/e5f50d22a.png  \n",
            "  inflating: train/Black-grass/e62aa6d6e.png  \n",
            "  inflating: train/Black-grass/e67dbce63.png  \n",
            "  inflating: train/Black-grass/e7d7e6351.png  \n",
            "  inflating: train/Black-grass/ea498dd9c.png  \n",
            "  inflating: train/Black-grass/ea85eb4a1.png  \n",
            "  inflating: train/Black-grass/eac39cfa8.png  \n",
            "  inflating: train/Black-grass/ebd2350df.png  \n",
            "  inflating: train/Black-grass/ecd14321d.png  \n",
            "  inflating: train/Black-grass/ed0bc2794.png  \n",
            "  inflating: train/Black-grass/ed17d766b.png  \n",
            "  inflating: train/Black-grass/ed4b42936.png  \n",
            "  inflating: train/Black-grass/ed540beb6.png  \n",
            "  inflating: train/Black-grass/f007dfa26.png  \n",
            "  inflating: train/Black-grass/f0a7c51a2.png  \n",
            "  inflating: train/Black-grass/f20bf670a.png  \n",
            "  inflating: train/Black-grass/f39ddbe0a.png  \n",
            "  inflating: train/Black-grass/f423f84da.png  \n",
            "  inflating: train/Black-grass/f47390401.png  \n",
            "  inflating: train/Black-grass/f4b7ddbce.png  \n",
            "  inflating: train/Black-grass/f5ca3d442.png  \n",
            "  inflating: train/Black-grass/f7f671785.png  \n",
            "  inflating: train/Black-grass/f82d13d23.png  \n",
            "  inflating: train/Black-grass/f84089a55.png  \n",
            "  inflating: train/Black-grass/fab809601.png  \n",
            "  inflating: train/Black-grass/fb487c5a4.png  \n",
            "  inflating: train/Black-grass/fc1001932.png  \n",
            "  inflating: train/Black-grass/fef14b865.png  \n",
            "  inflating: train/Charlock/022179d65.png  \n",
            "  inflating: train/Charlock/02c95e601.png  \n",
            "  inflating: train/Charlock/04098447d.png  \n",
            "  inflating: train/Charlock/04142acb9.png  \n",
            "  inflating: train/Charlock/0537577cd.png  \n",
            "  inflating: train/Charlock/084a9cb18.png  \n",
            "  inflating: train/Charlock/08fcc43a7.png  \n",
            "  inflating: train/Charlock/09f038ce4.png  \n",
            "  inflating: train/Charlock/0a7e1ca41.png  \n",
            "  inflating: train/Charlock/0b2fba287.png  \n",
            "  inflating: train/Charlock/0b3167af8.png  \n",
            "  inflating: train/Charlock/0c4a435c7.png  \n",
            "  inflating: train/Charlock/0cf0581b1.png  \n",
            "  inflating: train/Charlock/0d5f555a3.png  \n",
            "  inflating: train/Charlock/0dba71eba.png  \n",
            "  inflating: train/Charlock/0e1627696.png  \n",
            "  inflating: train/Charlock/0e51b1876.png  \n",
            "  inflating: train/Charlock/0edcd02cd.png  \n",
            "  inflating: train/Charlock/0f951d51e.png  \n",
            "  inflating: train/Charlock/0fa930fa9.png  \n",
            "  inflating: train/Charlock/100f53f2a.png  \n",
            "  inflating: train/Charlock/10b7d4de4.png  \n",
            "  inflating: train/Charlock/10d884885.png  \n",
            "  inflating: train/Charlock/11185184a.png  \n",
            "  inflating: train/Charlock/12c18399b.png  \n",
            "  inflating: train/Charlock/137c4e78e.png  \n",
            "  inflating: train/Charlock/137d1ed87.png  \n",
            "  inflating: train/Charlock/13d67633c.png  \n",
            "  inflating: train/Charlock/143f07e0d.png  \n",
            "  inflating: train/Charlock/145635854.png  \n",
            "  inflating: train/Charlock/157ade042.png  \n",
            "  inflating: train/Charlock/1639ed813.png  \n",
            "  inflating: train/Charlock/167202290.png  \n",
            "  inflating: train/Charlock/168982d9c.png  \n",
            "  inflating: train/Charlock/16d4e0dd3.png  \n",
            "  inflating: train/Charlock/1716acd53.png  \n",
            "  inflating: train/Charlock/17199b5a7.png  \n",
            "  inflating: train/Charlock/17fa6cd9c.png  \n",
            "  inflating: train/Charlock/1876d091f.png  \n",
            "  inflating: train/Charlock/1a92d7c2a.png  \n",
            "  inflating: train/Charlock/1aae936b7.png  \n",
            "  inflating: train/Charlock/1b31ec656.png  \n",
            "  inflating: train/Charlock/1b534df5b.png  \n",
            "  inflating: train/Charlock/1b8281227.png  \n",
            "  inflating: train/Charlock/1d9676402.png  \n",
            "  inflating: train/Charlock/1da9ac0ba.png  \n",
            "  inflating: train/Charlock/1dfeec485.png  \n",
            "  inflating: train/Charlock/1e808df13.png  \n",
            "  inflating: train/Charlock/1f6df012f.png  \n",
            "  inflating: train/Charlock/1fb63feb5.png  \n",
            "  inflating: train/Charlock/1fe41c75c.png  \n",
            "  inflating: train/Charlock/200589bf2.png  \n",
            "  inflating: train/Charlock/207c8f749.png  \n",
            "  inflating: train/Charlock/2097eebd0.png  \n",
            "  inflating: train/Charlock/20b955bc3.png  \n",
            "  inflating: train/Charlock/23bf27e1f.png  \n",
            "  inflating: train/Charlock/25cab77ab.png  \n",
            "  inflating: train/Charlock/270209308.png  \n",
            "  inflating: train/Charlock/299c79409.png  \n",
            "  inflating: train/Charlock/2d9af457e.png  \n",
            "  inflating: train/Charlock/2f75f171e.png  \n",
            "  inflating: train/Charlock/2fd604008.png  \n",
            "  inflating: train/Charlock/303cfad91.png  \n",
            "  inflating: train/Charlock/307dcba5f.png  \n",
            "  inflating: train/Charlock/30a669574.png  \n",
            "  inflating: train/Charlock/3217ef372.png  \n",
            "  inflating: train/Charlock/32fc83278.png  \n",
            "  inflating: train/Charlock/34842c4a1.png  \n",
            "  inflating: train/Charlock/34ada5879.png  \n",
            "  inflating: train/Charlock/34dab243e.png  \n",
            "  inflating: train/Charlock/350a133fe.png  \n",
            "  inflating: train/Charlock/35e06a98d.png  \n",
            "  inflating: train/Charlock/363fbba0d.png  \n",
            "  inflating: train/Charlock/36fad6f5d.png  \n",
            "  inflating: train/Charlock/385a498a1.png  \n",
            "  inflating: train/Charlock/3981b625f.png  \n",
            "  inflating: train/Charlock/3ac96f88f.png  \n",
            "  inflating: train/Charlock/3b2ae0151.png  \n",
            "  inflating: train/Charlock/3b8b3e7be.png  \n",
            "  inflating: train/Charlock/3ba14a843.png  \n",
            "  inflating: train/Charlock/3c75e7d79.png  \n",
            "  inflating: train/Charlock/3d32fbf63.png  \n",
            "  inflating: train/Charlock/3d957e19d.png  \n",
            "  inflating: train/Charlock/3e2cf6dc3.png  \n",
            "  inflating: train/Charlock/3e3cb2246.png  \n",
            "  inflating: train/Charlock/40ae56209.png  \n",
            "  inflating: train/Charlock/410598462.png  \n",
            "  inflating: train/Charlock/412966c74.png  \n",
            "  inflating: train/Charlock/420c6475c.png  \n",
            "  inflating: train/Charlock/42203bff1.png  \n",
            "  inflating: train/Charlock/42868a82d.png  \n",
            "  inflating: train/Charlock/43b607981.png  \n",
            "  inflating: train/Charlock/447c52f72.png  \n",
            "  inflating: train/Charlock/4537042f3.png  \n",
            "  inflating: train/Charlock/476068dcb.png  \n",
            "  inflating: train/Charlock/47c2398c9.png  \n",
            "  inflating: train/Charlock/48ca70ed5.png  \n",
            "  inflating: train/Charlock/49a8c3409.png  \n",
            "  inflating: train/Charlock/4bbc8083f.png  \n",
            "  inflating: train/Charlock/4c19db861.png  \n",
            "  inflating: train/Charlock/4cf7fd2d0.png  \n",
            "  inflating: train/Charlock/4e0cef11d.png  \n",
            "  inflating: train/Charlock/4e4234576.png  \n",
            "  inflating: train/Charlock/503bc583a.png  \n",
            "  inflating: train/Charlock/512f3e717.png  \n",
            "  inflating: train/Charlock/51b461a5e.png  \n",
            "  inflating: train/Charlock/523c4c9dd.png  \n",
            "  inflating: train/Charlock/526a7944e.png  \n",
            "  inflating: train/Charlock/5283b8c96.png  \n",
            "  inflating: train/Charlock/55d410aca.png  \n",
            "  inflating: train/Charlock/562ff38ac.png  \n",
            "  inflating: train/Charlock/566cd15f1.png  \n",
            "  inflating: train/Charlock/57b972eb5.png  \n",
            "  inflating: train/Charlock/5841a3656.png  \n",
            "  inflating: train/Charlock/5883bf7ce.png  \n",
            "  inflating: train/Charlock/5a6794cb7.png  \n",
            "  inflating: train/Charlock/5b8301d7f.png  \n",
            "  inflating: train/Charlock/5c75eae98.png  \n",
            "  inflating: train/Charlock/5ca18ec83.png  \n",
            "  inflating: train/Charlock/5d820b1cb.png  \n",
            "  inflating: train/Charlock/5db43df54.png  \n",
            "  inflating: train/Charlock/5e2a94fb3.png  \n",
            "  inflating: train/Charlock/5e5644439.png  \n",
            "  inflating: train/Charlock/5ecc28145.png  \n",
            "  inflating: train/Charlock/5f331f09c.png  \n",
            "  inflating: train/Charlock/60d99c56c.png  \n",
            "  inflating: train/Charlock/6266112a8.png  \n",
            "  inflating: train/Charlock/638267ce4.png  \n",
            "  inflating: train/Charlock/63a15c42c.png  \n",
            "  inflating: train/Charlock/63a3d8fa0.png  \n",
            "  inflating: train/Charlock/6535bc073.png  \n",
            "  inflating: train/Charlock/654bb174e.png  \n",
            "  inflating: train/Charlock/65ea4e47c.png  \n",
            "  inflating: train/Charlock/66815e7fd.png  \n",
            "  inflating: train/Charlock/66e9a00b7.png  \n",
            "  inflating: train/Charlock/671b2d4eb.png  \n",
            "  inflating: train/Charlock/67af4b8ca.png  \n",
            "  inflating: train/Charlock/67e37de9b.png  \n",
            "  inflating: train/Charlock/692e2c6f0.png  \n",
            "  inflating: train/Charlock/69734d263.png  \n",
            "  inflating: train/Charlock/69ee68f42.png  \n",
            "  inflating: train/Charlock/6a0d339e2.png  \n",
            "  inflating: train/Charlock/6a23010e3.png  \n",
            "  inflating: train/Charlock/6a3339ef1.png  \n",
            "  inflating: train/Charlock/6a773e6f5.png  \n",
            "  inflating: train/Charlock/6a84bdf5f.png  \n",
            "  inflating: train/Charlock/6aae02cb3.png  \n",
            "  inflating: train/Charlock/6b0964b5e.png  \n",
            "  inflating: train/Charlock/6c64e78b5.png  \n",
            "  inflating: train/Charlock/6ccccc87f.png  \n",
            "  inflating: train/Charlock/6d8b4041a.png  \n",
            "  inflating: train/Charlock/6dd8febb5.png  \n",
            "  inflating: train/Charlock/6e31a4a1b.png  \n",
            "  inflating: train/Charlock/6e891673a.png  \n",
            "  inflating: train/Charlock/6ea9a8d71.png  \n",
            "  inflating: train/Charlock/6ed6398a3.png  \n",
            "  inflating: train/Charlock/6fa73fdf6.png  \n",
            "  inflating: train/Charlock/6fb06e7ea.png  \n",
            "  inflating: train/Charlock/704e5bf0a.png  \n",
            "  inflating: train/Charlock/7066f950c.png  \n",
            "  inflating: train/Charlock/71472bf20.png  \n",
            "  inflating: train/Charlock/7249f8ad5.png  \n",
            "  inflating: train/Charlock/72f587775.png  \n",
            "  inflating: train/Charlock/730875fd4.png  \n",
            "  inflating: train/Charlock/73ddcfa90.png  \n",
            "  inflating: train/Charlock/7427b0007.png  \n",
            "  inflating: train/Charlock/74dc2a5ae.png  \n",
            "  inflating: train/Charlock/75ae3b231.png  \n",
            "  inflating: train/Charlock/75dcfc459.png  \n",
            "  inflating: train/Charlock/75e838f3e.png  \n",
            "  inflating: train/Charlock/7603e9dc8.png  \n",
            "  inflating: train/Charlock/781470a54.png  \n",
            "  inflating: train/Charlock/78a4d13c5.png  \n",
            "  inflating: train/Charlock/78c15e337.png  \n",
            "  inflating: train/Charlock/78fc2ef58.png  \n",
            "  inflating: train/Charlock/79e819019.png  \n",
            "  inflating: train/Charlock/7a766307c.png  \n",
            "  inflating: train/Charlock/7b28fc70e.png  \n",
            "  inflating: train/Charlock/7b4d1832b.png  \n",
            "  inflating: train/Charlock/7be37a159.png  \n",
            "  inflating: train/Charlock/7cfc5943e.png  \n",
            "  inflating: train/Charlock/7d16f8f00.png  \n",
            "  inflating: train/Charlock/7ed1aa0b2.png  \n",
            "  inflating: train/Charlock/7f251fb9d.png  \n",
            "  inflating: train/Charlock/7f6d9ef11.png  \n",
            "  inflating: train/Charlock/8194b5c6a.png  \n",
            "  inflating: train/Charlock/8194f9c87.png  \n",
            "  inflating: train/Charlock/819cf3a32.png  \n",
            "  inflating: train/Charlock/81be6d02f.png  \n",
            "  inflating: train/Charlock/82b303659.png  \n",
            "  inflating: train/Charlock/83ac9fba2.png  \n",
            "  inflating: train/Charlock/84281dd7c.png  \n",
            "  inflating: train/Charlock/846db9b8a.png  \n",
            "  inflating: train/Charlock/84e4249ae.png  \n",
            "  inflating: train/Charlock/85637dbdc.png  \n",
            "  inflating: train/Charlock/85f558e39.png  \n",
            "  inflating: train/Charlock/8705afe70.png  \n",
            "  inflating: train/Charlock/87168f1c7.png  \n",
            "  inflating: train/Charlock/88c2c6f8a.png  \n",
            "  inflating: train/Charlock/88c5a0b11.png  \n",
            "  inflating: train/Charlock/88c8cbb35.png  \n",
            "  inflating: train/Charlock/89b586174.png  \n",
            "  inflating: train/Charlock/89b84d6e9.png  \n",
            "  inflating: train/Charlock/8b35222d0.png  \n",
            "  inflating: train/Charlock/8b3f0fba7.png  \n",
            "  inflating: train/Charlock/8c9c6b343.png  \n",
            "  inflating: train/Charlock/8cc7a03c6.png  \n",
            "  inflating: train/Charlock/8d16914a3.png  \n",
            "  inflating: train/Charlock/8dd1dbca2.png  \n",
            "  inflating: train/Charlock/8de73f114.png  \n",
            "  inflating: train/Charlock/8df0f80e9.png  \n",
            "  inflating: train/Charlock/8f4d248bd.png  \n",
            "  inflating: train/Charlock/903fff84f.png  \n",
            "  inflating: train/Charlock/90981bf00.png  \n",
            "  inflating: train/Charlock/90faa7ccd.png  \n",
            "  inflating: train/Charlock/92184e2d6.png  \n",
            "  inflating: train/Charlock/921ef57b0.png  \n",
            "  inflating: train/Charlock/92209cbd5.png  \n",
            "  inflating: train/Charlock/946b7baea.png  \n",
            "  inflating: train/Charlock/94af3e45a.png  \n",
            "  inflating: train/Charlock/95d959662.png  \n",
            "  inflating: train/Charlock/961517fac.png  \n",
            "  inflating: train/Charlock/97907da1a.png  \n",
            "  inflating: train/Charlock/982f40990.png  \n",
            "  inflating: train/Charlock/99a661104.png  \n",
            "  inflating: train/Charlock/99f80eb00.png  \n",
            "  inflating: train/Charlock/9a3903864.png  \n",
            "  inflating: train/Charlock/9ae330eda.png  \n",
            "  inflating: train/Charlock/9b419387b.png  \n",
            "  inflating: train/Charlock/9c55fa55a.png  \n",
            "  inflating: train/Charlock/9cdf5eb74.png  \n",
            "  inflating: train/Charlock/9d7f4d92c.png  \n",
            "  inflating: train/Charlock/9df7399d3.png  \n",
            "  inflating: train/Charlock/9f24b0c83.png  \n",
            "  inflating: train/Charlock/9ffcb543c.png  \n",
            "  inflating: train/Charlock/a04a7dda2.png  \n",
            "  inflating: train/Charlock/a1f862ba9.png  \n",
            "  inflating: train/Charlock/a30113dfc.png  \n",
            "  inflating: train/Charlock/a48a9b71d.png  \n",
            "  inflating: train/Charlock/a501dcf4b.png  \n",
            "  inflating: train/Charlock/a51f29e7d.png  \n",
            "  inflating: train/Charlock/a5470a35a.png  \n",
            "  inflating: train/Charlock/a631f152b.png  \n",
            "  inflating: train/Charlock/a6e31acae.png  \n",
            "  inflating: train/Charlock/a7a3c2a2f.png  \n",
            "  inflating: train/Charlock/a7f1b995f.png  \n",
            "  inflating: train/Charlock/a880d32f3.png  \n",
            "  inflating: train/Charlock/a89ee4eb7.png  \n",
            "  inflating: train/Charlock/a8e7520de.png  \n",
            "  inflating: train/Charlock/a96c1356b.png  \n",
            "  inflating: train/Charlock/abb06eb1e.png  \n",
            "  inflating: train/Charlock/abe5f09d9.png  \n",
            "  inflating: train/Charlock/ad5acca39.png  \n",
            "  inflating: train/Charlock/adc0ca647.png  \n",
            "  inflating: train/Charlock/ae00ed5e0.png  \n",
            "  inflating: train/Charlock/ae5116f62.png  \n",
            "  inflating: train/Charlock/ae66022e7.png  \n",
            "  inflating: train/Charlock/ae813adcd.png  \n",
            "  inflating: train/Charlock/afba2f2ad.png  \n",
            "  inflating: train/Charlock/b10562f56.png  \n",
            "  inflating: train/Charlock/b10fcd298.png  \n",
            "  inflating: train/Charlock/b182916bb.png  \n",
            "  inflating: train/Charlock/b1b089347.png  \n",
            "  inflating: train/Charlock/b207ff8b2.png  \n",
            "  inflating: train/Charlock/b3b28cf8e.png  \n",
            "  inflating: train/Charlock/b50f12de9.png  \n",
            "  inflating: train/Charlock/b525a32bc.png  \n",
            "  inflating: train/Charlock/b53281833.png  \n",
            "  inflating: train/Charlock/b5b065c02.png  \n",
            "  inflating: train/Charlock/b7b28f840.png  \n",
            "  inflating: train/Charlock/b8421d027.png  \n",
            "  inflating: train/Charlock/b87ad78b7.png  \n",
            "  inflating: train/Charlock/ba2cfd858.png  \n",
            "  inflating: train/Charlock/ba2e2326d.png  \n",
            "  inflating: train/Charlock/bac8b25d8.png  \n",
            "  inflating: train/Charlock/bacbdf153.png  \n",
            "  inflating: train/Charlock/bdc8e6025.png  \n",
            "  inflating: train/Charlock/bde26ebbb.png  \n",
            "  inflating: train/Charlock/be27e52a5.png  \n",
            "  inflating: train/Charlock/bec896ebd.png  \n",
            "  inflating: train/Charlock/bef36ff4f.png  \n",
            "  inflating: train/Charlock/bfab71be4.png  \n",
            "  inflating: train/Charlock/c07c67078.png  \n",
            "  inflating: train/Charlock/c0aac7fc2.png  \n",
            "  inflating: train/Charlock/c0ae1e074.png  \n",
            "  inflating: train/Charlock/c0e365810.png  \n",
            "  inflating: train/Charlock/c19bc586d.png  \n",
            "  inflating: train/Charlock/c1b1e4f9d.png  \n",
            "  inflating: train/Charlock/c291766ce.png  \n",
            "  inflating: train/Charlock/c2a1c8163.png  \n",
            "  inflating: train/Charlock/c397b8d1a.png  \n",
            "  inflating: train/Charlock/c39fea9e3.png  \n",
            "  inflating: train/Charlock/c45aa7507.png  \n",
            "  inflating: train/Charlock/c5cca5955.png  \n",
            "  inflating: train/Charlock/c61d3ee3c.png  \n",
            "  inflating: train/Charlock/c6368b901.png  \n",
            "  inflating: train/Charlock/c7ac56c83.png  \n",
            "  inflating: train/Charlock/c836af21e.png  \n",
            "  inflating: train/Charlock/c842c829b.png  \n",
            "  inflating: train/Charlock/c8b44af53.png  \n",
            "  inflating: train/Charlock/c911db696.png  \n",
            "  inflating: train/Charlock/c9601d3ab.png  \n",
            "  inflating: train/Charlock/c97f86135.png  \n",
            "  inflating: train/Charlock/c9a504525.png  \n",
            "  inflating: train/Charlock/ca1aaeb27.png  \n",
            "  inflating: train/Charlock/ca8f0f871.png  \n",
            "  inflating: train/Charlock/caa43063c.png  \n",
            "  inflating: train/Charlock/cb142c018.png  \n",
            "  inflating: train/Charlock/cdbf55495.png  \n",
            "  inflating: train/Charlock/ce5473300.png  \n",
            "  inflating: train/Charlock/cf04964e6.png  \n",
            "  inflating: train/Charlock/d04eff450.png  \n",
            "  inflating: train/Charlock/d0641b545.png  \n",
            "  inflating: train/Charlock/d1b362c43.png  \n",
            "  inflating: train/Charlock/d2f07f3af.png  \n",
            "  inflating: train/Charlock/d31803377.png  \n",
            "  inflating: train/Charlock/d3228543a.png  \n",
            "  inflating: train/Charlock/d37702684.png  \n",
            "  inflating: train/Charlock/d5798e4cd.png  \n",
            "  inflating: train/Charlock/d5b30d4d2.png  \n",
            "  inflating: train/Charlock/d5f629eb0.png  \n",
            "  inflating: train/Charlock/d633facf4.png  \n",
            "  inflating: train/Charlock/d733b32d8.png  \n",
            "  inflating: train/Charlock/d8690d738.png  \n",
            "  inflating: train/Charlock/d8bd75282.png  \n",
            "  inflating: train/Charlock/d8da05d11.png  \n",
            "  inflating: train/Charlock/d903a2a6d.png  \n",
            "  inflating: train/Charlock/d9d4e0305.png  \n",
            "  inflating: train/Charlock/d9de67550.png  \n",
            "  inflating: train/Charlock/da2aa8585.png  \n",
            "  inflating: train/Charlock/db96c0bab.png  \n",
            "  inflating: train/Charlock/dc20df305.png  \n",
            "  inflating: train/Charlock/dccc5c0a7.png  \n",
            "  inflating: train/Charlock/dd021836c.png  \n",
            "  inflating: train/Charlock/ddb90e66a.png  \n",
            "  inflating: train/Charlock/ddcc7d9bf.png  \n",
            "  inflating: train/Charlock/ddf0958d2.png  \n",
            "  inflating: train/Charlock/de0e95661.png  \n",
            "  inflating: train/Charlock/de1dc1b00.png  \n",
            "  inflating: train/Charlock/de2cf9517.png  \n",
            "  inflating: train/Charlock/df586ac63.png  \n",
            "  inflating: train/Charlock/df7d711a9.png  \n",
            "  inflating: train/Charlock/dfb3d9a79.png  \n",
            "  inflating: train/Charlock/e23998a9b.png  \n",
            "  inflating: train/Charlock/e260cb2db.png  \n",
            "  inflating: train/Charlock/e262e89e9.png  \n",
            "  inflating: train/Charlock/e28fd1de4.png  \n",
            "  inflating: train/Charlock/e5868b452.png  \n",
            "  inflating: train/Charlock/e5d2875b4.png  \n",
            "  inflating: train/Charlock/e6918f541.png  \n",
            "  inflating: train/Charlock/e71a5f489.png  \n",
            "  inflating: train/Charlock/e795c53c9.png  \n",
            "  inflating: train/Charlock/e80aaab6b.png  \n",
            "  inflating: train/Charlock/e84b5ba79.png  \n",
            "  inflating: train/Charlock/ea0751e51.png  \n",
            "  inflating: train/Charlock/ea31723d5.png  \n",
            "  inflating: train/Charlock/ead0df392.png  \n",
            "  inflating: train/Charlock/eb9d116cf.png  \n",
            "  inflating: train/Charlock/ec205ced8.png  \n",
            "  inflating: train/Charlock/ec48facfa.png  \n",
            "  inflating: train/Charlock/ec615729c.png  \n",
            "  inflating: train/Charlock/ec84cacfe.png  \n",
            "  inflating: train/Charlock/ed260e354.png  \n",
            "  inflating: train/Charlock/eda5cd97f.png  \n",
            "  inflating: train/Charlock/ee111c2ae.png  \n",
            "  inflating: train/Charlock/ee4a02bf9.png  \n",
            "  inflating: train/Charlock/eeee1a97b.png  \n",
            "  inflating: train/Charlock/eeee9e96c.png  \n",
            "  inflating: train/Charlock/efdd9e2ea.png  \n",
            "  inflating: train/Charlock/f0a9c3b66.png  \n",
            "  inflating: train/Charlock/f0ced8a62.png  \n",
            "  inflating: train/Charlock/f11e74453.png  \n",
            "  inflating: train/Charlock/f193af8d4.png  \n",
            "  inflating: train/Charlock/f1f9ab9b0.png  \n",
            "  inflating: train/Charlock/f340a3378.png  \n",
            "  inflating: train/Charlock/f39a873f8.png  \n",
            "  inflating: train/Charlock/f5672ef15.png  \n",
            "  inflating: train/Charlock/f5eec02cf.png  \n",
            "  inflating: train/Charlock/f67425c37.png  \n",
            "  inflating: train/Charlock/f7edd4688.png  \n",
            "  inflating: train/Charlock/f81c60e7b.png  \n",
            "  inflating: train/Charlock/f9a6d5127.png  \n",
            "  inflating: train/Charlock/fc0a2a9b8.png  \n",
            "  inflating: train/Charlock/fc3e15a2e.png  \n",
            "  inflating: train/Charlock/fc584691c.png  \n",
            "  inflating: train/Charlock/fca3c13ca.png  \n",
            "  inflating: train/Charlock/fd3e62689.png  \n",
            "  inflating: train/Charlock/fd5021432.png  \n",
            "  inflating: train/Charlock/fe98d5e24.png  \n",
            "  inflating: train/Charlock/ffe349114.png  \n",
            "  inflating: train/Cleavers/005b4a3e3.png  \n",
            "  inflating: train/Cleavers/00aa8d5a4.png  \n",
            "  inflating: train/Cleavers/00df90ed6.png  \n",
            "  inflating: train/Cleavers/01605ed73.png  \n",
            "  inflating: train/Cleavers/03bee43c0.png  \n",
            "  inflating: train/Cleavers/0515bc601.png  \n",
            "  inflating: train/Cleavers/0522ec53b.png  \n",
            "  inflating: train/Cleavers/06f447848.png  \n",
            "  inflating: train/Cleavers/0756fd41e.png  \n",
            "  inflating: train/Cleavers/07ac7bc07.png  \n",
            "  inflating: train/Cleavers/0920492fd.png  \n",
            "  inflating: train/Cleavers/09da4f213.png  \n",
            "  inflating: train/Cleavers/0a1e622bc.png  \n",
            "  inflating: train/Cleavers/0a33283c7.png  \n",
            "  inflating: train/Cleavers/0ac0f0a66.png  \n",
            "  inflating: train/Cleavers/0ac327873.png  \n",
            "  inflating: train/Cleavers/0b44c3a6c.png  \n",
            "  inflating: train/Cleavers/0bcf29af7.png  \n",
            "  inflating: train/Cleavers/0bdee2052.png  \n",
            "  inflating: train/Cleavers/0bec204c7.png  \n",
            "  inflating: train/Cleavers/0dbfac958.png  \n",
            "  inflating: train/Cleavers/0f557bb1a.png  \n",
            "  inflating: train/Cleavers/10a18e8d1.png  \n",
            "  inflating: train/Cleavers/11760cf9f.png  \n",
            "  inflating: train/Cleavers/11c5eb5b6.png  \n",
            "  inflating: train/Cleavers/124f732d8.png  \n",
            "  inflating: train/Cleavers/136d5af0b.png  \n",
            "  inflating: train/Cleavers/1526846f7.png  \n",
            "  inflating: train/Cleavers/153f94019.png  \n",
            "  inflating: train/Cleavers/15917308c.png  \n",
            "  inflating: train/Cleavers/15f41649c.png  \n",
            "  inflating: train/Cleavers/16e0482dd.png  \n",
            "  inflating: train/Cleavers/184e9eb16.png  \n",
            "  inflating: train/Cleavers/1896061bc.png  \n",
            "  inflating: train/Cleavers/198653e67.png  \n",
            "  inflating: train/Cleavers/1a4fe0d36.png  \n",
            "  inflating: train/Cleavers/1af15c939.png  \n",
            "  inflating: train/Cleavers/1b28a0668.png  \n",
            "  inflating: train/Cleavers/1bc5bea6c.png  \n",
            "  inflating: train/Cleavers/1c43f8251.png  \n",
            "  inflating: train/Cleavers/1c801fa69.png  \n",
            "  inflating: train/Cleavers/1d0b4527d.png  \n",
            "  inflating: train/Cleavers/1e0caafd3.png  \n",
            "  inflating: train/Cleavers/1e33a3dce.png  \n",
            "  inflating: train/Cleavers/1e5041183.png  \n",
            "  inflating: train/Cleavers/1e510607e.png  \n",
            "  inflating: train/Cleavers/1ef8ef4af.png  \n",
            "  inflating: train/Cleavers/1ffcc6d2c.png  \n",
            "  inflating: train/Cleavers/20964b9b7.png  \n",
            "  inflating: train/Cleavers/2261906cf.png  \n",
            "  inflating: train/Cleavers/2297085a0.png  \n",
            "  inflating: train/Cleavers/252585ad8.png  \n",
            "  inflating: train/Cleavers/2559137f2.png  \n",
            "  inflating: train/Cleavers/265a7f826.png  \n",
            "  inflating: train/Cleavers/294d28c60.png  \n",
            "  inflating: train/Cleavers/299190b08.png  \n",
            "  inflating: train/Cleavers/2a1d10e06.png  \n",
            "  inflating: train/Cleavers/2baa68b6e.png  \n",
            "  inflating: train/Cleavers/2c5c4d127.png  \n",
            "  inflating: train/Cleavers/2e47c7233.png  \n",
            "  inflating: train/Cleavers/2e547d792.png  \n",
            "  inflating: train/Cleavers/2e97487c4.png  \n",
            "  inflating: train/Cleavers/2f38351b7.png  \n",
            "  inflating: train/Cleavers/30418535b.png  \n",
            "  inflating: train/Cleavers/3232000bf.png  \n",
            "  inflating: train/Cleavers/323f03521.png  \n",
            "  inflating: train/Cleavers/33c1f167f.png  \n",
            "  inflating: train/Cleavers/33d0d3358.png  \n",
            "  inflating: train/Cleavers/34648ed0b.png  \n",
            "  inflating: train/Cleavers/34f210a03.png  \n",
            "  inflating: train/Cleavers/3516267fc.png  \n",
            "  inflating: train/Cleavers/360f883f6.png  \n",
            "  inflating: train/Cleavers/364a60044.png  \n",
            "  inflating: train/Cleavers/368707311.png  \n",
            "  inflating: train/Cleavers/36e0d8ca6.png  \n",
            "  inflating: train/Cleavers/37a3c8675.png  \n",
            "  inflating: train/Cleavers/398cc8af9.png  \n",
            "  inflating: train/Cleavers/3a4e50c40.png  \n",
            "  inflating: train/Cleavers/3c2e76718.png  \n",
            "  inflating: train/Cleavers/3ccb22924.png  \n",
            "  inflating: train/Cleavers/3d9ea1649.png  \n",
            "  inflating: train/Cleavers/3f5b465c6.png  \n",
            "  inflating: train/Cleavers/3fc47de35.png  \n",
            "  inflating: train/Cleavers/413a96d3d.png  \n",
            "  inflating: train/Cleavers/41da62977.png  \n",
            "  inflating: train/Cleavers/420f3654f.png  \n",
            "  inflating: train/Cleavers/42645d809.png  \n",
            "  inflating: train/Cleavers/4301f9c21.png  \n",
            "  inflating: train/Cleavers/44047de36.png  \n",
            "  inflating: train/Cleavers/4444af894.png  \n",
            "  inflating: train/Cleavers/4449f17c5.png  \n",
            "  inflating: train/Cleavers/44ef950c3.png  \n",
            "  inflating: train/Cleavers/4613b263e.png  \n",
            "  inflating: train/Cleavers/4620b59f7.png  \n",
            "  inflating: train/Cleavers/491286e9c.png  \n",
            "  inflating: train/Cleavers/491b8502a.png  \n",
            "  inflating: train/Cleavers/495602403.png  \n",
            "  inflating: train/Cleavers/4b9125e90.png  \n",
            "  inflating: train/Cleavers/4c7552a5c.png  \n",
            "  inflating: train/Cleavers/4e01a3eab.png  \n",
            "  inflating: train/Cleavers/500a7b5b3.png  \n",
            "  inflating: train/Cleavers/5010487f2.png  \n",
            "  inflating: train/Cleavers/502eee444.png  \n",
            "  inflating: train/Cleavers/5439f9f36.png  \n",
            "  inflating: train/Cleavers/5543b3415.png  \n",
            "  inflating: train/Cleavers/565a81e32.png  \n",
            "  inflating: train/Cleavers/5778852ed.png  \n",
            "  inflating: train/Cleavers/581c0ecb3.png  \n",
            "  inflating: train/Cleavers/58b68b1ea.png  \n",
            "  inflating: train/Cleavers/58be08e22.png  \n",
            "  inflating: train/Cleavers/591daf862.png  \n",
            "  inflating: train/Cleavers/595d40614.png  \n",
            "  inflating: train/Cleavers/5aaaf0ea0.png  \n",
            "  inflating: train/Cleavers/5abac7fbf.png  \n",
            "  inflating: train/Cleavers/5b5a7214a.png  \n",
            "  inflating: train/Cleavers/5d2b820a9.png  \n",
            "  inflating: train/Cleavers/609c74407.png  \n",
            "  inflating: train/Cleavers/61287f47a.png  \n",
            "  inflating: train/Cleavers/655d1ffdd.png  \n",
            "  inflating: train/Cleavers/666fa4f1a.png  \n",
            "  inflating: train/Cleavers/67afc84df.png  \n",
            "  inflating: train/Cleavers/6820df17f.png  \n",
            "  inflating: train/Cleavers/6895ae7c9.png  \n",
            "  inflating: train/Cleavers/68a24bc47.png  \n",
            "  inflating: train/Cleavers/68a3d6b27.png  \n",
            "  inflating: train/Cleavers/6a4ef17c2.png  \n",
            "  inflating: train/Cleavers/6acdb8e68.png  \n",
            "  inflating: train/Cleavers/6bcc0c252.png  \n",
            "  inflating: train/Cleavers/6c10be2a7.png  \n",
            "  inflating: train/Cleavers/6cd9902c0.png  \n",
            "  inflating: train/Cleavers/6cead585f.png  \n",
            "  inflating: train/Cleavers/6e0fa9bcc.png  \n",
            "  inflating: train/Cleavers/750c447e0.png  \n",
            "  inflating: train/Cleavers/75371625f.png  \n",
            "  inflating: train/Cleavers/77e4bee70.png  \n",
            "  inflating: train/Cleavers/78dde1704.png  \n",
            "  inflating: train/Cleavers/794b914bf.png  \n",
            "  inflating: train/Cleavers/796b61337.png  \n",
            "  inflating: train/Cleavers/7a9efaf6c.png  \n",
            "  inflating: train/Cleavers/7b257e388.png  \n",
            "  inflating: train/Cleavers/7b49d39c2.png  \n",
            "  inflating: train/Cleavers/7b9729321.png  \n",
            "  inflating: train/Cleavers/7cef265e7.png  \n",
            "  inflating: train/Cleavers/7da015f0a.png  \n",
            "  inflating: train/Cleavers/7da3bda82.png  \n",
            "  inflating: train/Cleavers/7e501d74c.png  \n",
            "  inflating: train/Cleavers/7e8212b65.png  \n",
            "  inflating: train/Cleavers/7e9b71110.png  \n",
            "  inflating: train/Cleavers/7f3369186.png  \n",
            "  inflating: train/Cleavers/7fa1a92ca.png  \n",
            "  inflating: train/Cleavers/8035a175e.png  \n",
            "  inflating: train/Cleavers/82a9322ea.png  \n",
            "  inflating: train/Cleavers/83abc465e.png  \n",
            "  inflating: train/Cleavers/853b1a32e.png  \n",
            "  inflating: train/Cleavers/857304342.png  \n",
            "  inflating: train/Cleavers/85b23f3e6.png  \n",
            "  inflating: train/Cleavers/863b1dbd4.png  \n",
            "  inflating: train/Cleavers/868da1d55.png  \n",
            "  inflating: train/Cleavers/871ec43cc.png  \n",
            "  inflating: train/Cleavers/88218400c.png  \n",
            "  inflating: train/Cleavers/899877fe1.png  \n",
            "  inflating: train/Cleavers/8a112bfb4.png  \n",
            "  inflating: train/Cleavers/8aa470d03.png  \n",
            "  inflating: train/Cleavers/8baf67453.png  \n",
            "  inflating: train/Cleavers/8c0e41a93.png  \n",
            "  inflating: train/Cleavers/8c26d810d.png  \n",
            "  inflating: train/Cleavers/8c73a6f46.png  \n",
            "  inflating: train/Cleavers/8cc66b39e.png  \n",
            "  inflating: train/Cleavers/8cf59e291.png  \n",
            "  inflating: train/Cleavers/8df9991b8.png  \n",
            "  inflating: train/Cleavers/8ee3f0b6f.png  \n",
            "  inflating: train/Cleavers/9214c2997.png  \n",
            "  inflating: train/Cleavers/944e50e37.png  \n",
            "  inflating: train/Cleavers/94f82e916.png  \n",
            "  inflating: train/Cleavers/94fa687c1.png  \n",
            "  inflating: train/Cleavers/952540220.png  \n",
            "  inflating: train/Cleavers/95a10dd51.png  \n",
            "  inflating: train/Cleavers/98937154f.png  \n",
            "  inflating: train/Cleavers/98ad03ef1.png  \n",
            "  inflating: train/Cleavers/99c858d54.png  \n",
            "  inflating: train/Cleavers/9b35827fa.png  \n",
            "  inflating: train/Cleavers/9b8ed8471.png  \n",
            "  inflating: train/Cleavers/9b941ac1b.png  \n",
            "  inflating: train/Cleavers/9c337ab7a.png  \n",
            "  inflating: train/Cleavers/9d763f383.png  \n",
            "  inflating: train/Cleavers/9e250b25f.png  \n",
            "  inflating: train/Cleavers/9e899d1ee.png  \n",
            "  inflating: train/Cleavers/9e966d9db.png  \n",
            "  inflating: train/Cleavers/a06109e80.png  \n",
            "  inflating: train/Cleavers/a07efb1e0.png  \n",
            "  inflating: train/Cleavers/a20cd3ec2.png  \n",
            "  inflating: train/Cleavers/a26d6a898.png  \n",
            "  inflating: train/Cleavers/a3085c4c8.png  \n",
            "  inflating: train/Cleavers/a3b9a33dd.png  \n",
            "  inflating: train/Cleavers/a4cfc9036.png  \n",
            "  inflating: train/Cleavers/a51e6f301.png  \n",
            "  inflating: train/Cleavers/a562c2b14.png  \n",
            "  inflating: train/Cleavers/a5b126385.png  \n",
            "  inflating: train/Cleavers/a714fbc63.png  \n",
            "  inflating: train/Cleavers/a96e704e7.png  \n",
            "  inflating: train/Cleavers/a9b4417a7.png  \n",
            "  inflating: train/Cleavers/aa2c378e7.png  \n",
            "  inflating: train/Cleavers/abb763143.png  \n",
            "  inflating: train/Cleavers/b022e53c3.png  \n",
            "  inflating: train/Cleavers/b07aeb162.png  \n",
            "  inflating: train/Cleavers/b0a1ac210.png  \n",
            "  inflating: train/Cleavers/b15ca8e4a.png  \n",
            "  inflating: train/Cleavers/b6220c08e.png  \n",
            "  inflating: train/Cleavers/ba79cdbcb.png  \n",
            "  inflating: train/Cleavers/bc66f9116.png  \n",
            "  inflating: train/Cleavers/bd2fa36aa.png  \n",
            "  inflating: train/Cleavers/bd4f2a692.png  \n",
            "  inflating: train/Cleavers/bd6681c02.png  \n",
            "  inflating: train/Cleavers/be41914d8.png  \n",
            "  inflating: train/Cleavers/becdd9442.png  \n",
            "  inflating: train/Cleavers/bf07fe75d.png  \n",
            "  inflating: train/Cleavers/c0784c573.png  \n",
            "  inflating: train/Cleavers/c0c1567ab.png  \n",
            "  inflating: train/Cleavers/c30be2849.png  \n",
            "  inflating: train/Cleavers/c336c8da6.png  \n",
            "  inflating: train/Cleavers/c33c988c1.png  \n",
            "  inflating: train/Cleavers/c3b79748e.png  \n",
            "  inflating: train/Cleavers/c3f4b326a.png  \n",
            "  inflating: train/Cleavers/c4a974463.png  \n",
            "  inflating: train/Cleavers/c4dfaf0ee.png  \n",
            "  inflating: train/Cleavers/c5f01dc34.png  \n",
            "  inflating: train/Cleavers/c5f57467a.png  \n",
            "  inflating: train/Cleavers/c6c56d45c.png  \n",
            "  inflating: train/Cleavers/c7b4ce2e3.png  \n",
            "  inflating: train/Cleavers/c806aa45d.png  \n",
            "  inflating: train/Cleavers/c9576e39e.png  \n",
            "  inflating: train/Cleavers/c977c4903.png  \n",
            "  inflating: train/Cleavers/c9a021a1a.png  \n",
            "  inflating: train/Cleavers/c9e078b54.png  \n",
            "  inflating: train/Cleavers/cb3f058b9.png  \n",
            "  inflating: train/Cleavers/cc2ffb750.png  \n",
            "  inflating: train/Cleavers/cc8f45811.png  \n",
            "  inflating: train/Cleavers/cd17d9cd6.png  \n",
            "  inflating: train/Cleavers/cf285e97d.png  \n",
            "  inflating: train/Cleavers/d17e2cfa1.png  \n",
            "  inflating: train/Cleavers/d20d68489.png  \n",
            "  inflating: train/Cleavers/d3a7492ff.png  \n",
            "  inflating: train/Cleavers/d4552e3f6.png  \n",
            "  inflating: train/Cleavers/d4803dca4.png  \n",
            "  inflating: train/Cleavers/d55caa949.png  \n",
            "  inflating: train/Cleavers/d81f07b0b.png  \n",
            "  inflating: train/Cleavers/d8597aa6a.png  \n",
            "  inflating: train/Cleavers/d8f23e930.png  \n",
            "  inflating: train/Cleavers/d90adada5.png  \n",
            "  inflating: train/Cleavers/d96c1e585.png  \n",
            "  inflating: train/Cleavers/d9739baa6.png  \n",
            "  inflating: train/Cleavers/d99ddf94b.png  \n",
            "  inflating: train/Cleavers/da38229c7.png  \n",
            "  inflating: train/Cleavers/db4d4f5a7.png  \n",
            "  inflating: train/Cleavers/db735ff97.png  \n",
            "  inflating: train/Cleavers/dbb42229b.png  \n",
            "  inflating: train/Cleavers/df341f2d4.png  \n",
            "  inflating: train/Cleavers/e2d1c8a71.png  \n",
            "  inflating: train/Cleavers/e39dd6305.png  \n",
            "  inflating: train/Cleavers/e63ef7169.png  \n",
            "  inflating: train/Cleavers/e8398d543.png  \n",
            "  inflating: train/Cleavers/e923f523a.png  \n",
            "  inflating: train/Cleavers/e96ebe7f6.png  \n",
            "  inflating: train/Cleavers/ea343a58f.png  \n",
            "  inflating: train/Cleavers/ead01d4a3.png  \n",
            "  inflating: train/Cleavers/eb2b0d2f6.png  \n",
            "  inflating: train/Cleavers/ebe7c4208.png  \n",
            "  inflating: train/Cleavers/ed3e7e5a7.png  \n",
            "  inflating: train/Cleavers/ee1cd35fa.png  \n",
            "  inflating: train/Cleavers/ee5cc2c19.png  \n",
            "  inflating: train/Cleavers/eed8f31b8.png  \n",
            "  inflating: train/Cleavers/ef4a36963.png  \n",
            "  inflating: train/Cleavers/f1810d3fa.png  \n",
            "  inflating: train/Cleavers/f185a1d75.png  \n",
            "  inflating: train/Cleavers/f2d50dae3.png  \n",
            "  inflating: train/Cleavers/f4248cc7f.png  \n",
            "  inflating: train/Cleavers/f5608b615.png  \n",
            "  inflating: train/Cleavers/f5a58ab42.png  \n",
            "  inflating: train/Cleavers/f607ea0bf.png  \n",
            "  inflating: train/Cleavers/f60e82a03.png  \n",
            "  inflating: train/Cleavers/f9f3c4595.png  \n",
            "  inflating: train/Cleavers/fab297bc9.png  \n",
            "  inflating: train/Cleavers/fc26df0cf.png  \n",
            "  inflating: train/Cleavers/fd2400d5f.png  \n",
            "  inflating: train/Cleavers/fd2683ed1.png  \n",
            "  inflating: train/Cleavers/fd5074d77.png  \n",
            "  inflating: train/Cleavers/feecf1be6.png  \n",
            "  inflating: train/Common Chickweed/00b6eee9f.png  \n",
            "  inflating: train/Common Chickweed/00ba5f88a.png  \n",
            "  inflating: train/Common Chickweed/00d33935c.png  \n",
            "  inflating: train/Common Chickweed/0118f1f70.png  \n",
            "  inflating: train/Common Chickweed/017a3000a.png  \n",
            "  inflating: train/Common Chickweed/019c3bbea.png  \n",
            "  inflating: train/Common Chickweed/01eef6041.png  \n",
            "  inflating: train/Common Chickweed/024b144e3.png  \n",
            "  inflating: train/Common Chickweed/02ad544a4.png  \n",
            "  inflating: train/Common Chickweed/02beb950e.png  \n",
            "  inflating: train/Common Chickweed/0331a0d41.png  \n",
            "  inflating: train/Common Chickweed/0366e36eb.png  \n",
            "  inflating: train/Common Chickweed/036eca712.png  \n",
            "  inflating: train/Common Chickweed/037295da4.png  \n",
            "  inflating: train/Common Chickweed/04526c399.png  \n",
            "  inflating: train/Common Chickweed/04b597a3f.png  \n",
            "  inflating: train/Common Chickweed/04baa9ae6.png  \n",
            "  inflating: train/Common Chickweed/054a3e47e.png  \n",
            "  inflating: train/Common Chickweed/05675900a.png  \n",
            "  inflating: train/Common Chickweed/0573b0ff7.png  \n",
            "  inflating: train/Common Chickweed/0593874ae.png  \n",
            "  inflating: train/Common Chickweed/05a56fcc4.png  \n",
            "  inflating: train/Common Chickweed/062f0fec6.png  \n",
            "  inflating: train/Common Chickweed/06c42cf3f.png  \n",
            "  inflating: train/Common Chickweed/06e9bbeba.png  \n",
            "  inflating: train/Common Chickweed/0704bc02b.png  \n",
            "  inflating: train/Common Chickweed/072fc34f1.png  \n",
            "  inflating: train/Common Chickweed/07e651912.png  \n",
            "  inflating: train/Common Chickweed/085df38fb.png  \n",
            "  inflating: train/Common Chickweed/08cdaa9ed.png  \n",
            "  inflating: train/Common Chickweed/0907487ed.png  \n",
            "  inflating: train/Common Chickweed/09202a9a6.png  \n",
            "  inflating: train/Common Chickweed/0965e0741.png  \n",
            "  inflating: train/Common Chickweed/096eb593d.png  \n",
            "  inflating: train/Common Chickweed/09a6108ae.png  \n",
            "  inflating: train/Common Chickweed/09d0908b0.png  \n",
            "  inflating: train/Common Chickweed/09f58b32a.png  \n",
            "  inflating: train/Common Chickweed/0a1c68ef9.png  \n",
            "  inflating: train/Common Chickweed/0a26afdf8.png  \n",
            "  inflating: train/Common Chickweed/0a2d20538.png  \n",
            "  inflating: train/Common Chickweed/0a8227413.png  \n",
            "  inflating: train/Common Chickweed/0b91b1f50.png  \n",
            "  inflating: train/Common Chickweed/0bc00be9f.png  \n",
            "  inflating: train/Common Chickweed/0c07d7246.png  \n",
            "  inflating: train/Common Chickweed/0c25871d9.png  \n",
            "  inflating: train/Common Chickweed/0c7fc717a.png  \n",
            "  inflating: train/Common Chickweed/0cc84e7b2.png  \n",
            "  inflating: train/Common Chickweed/0e261908b.png  \n",
            "  inflating: train/Common Chickweed/0f390ed98.png  \n",
            "  inflating: train/Common Chickweed/0f44cf2af.png  \n",
            "  inflating: train/Common Chickweed/0f872f09e.png  \n",
            "  inflating: train/Common Chickweed/0ff3c628c.png  \n",
            "  inflating: train/Common Chickweed/0ffbcb790.png  \n",
            "  inflating: train/Common Chickweed/1007fd84f.png  \n",
            "  inflating: train/Common Chickweed/108dfd224.png  \n",
            "  inflating: train/Common Chickweed/10a42d01f.png  \n",
            "  inflating: train/Common Chickweed/10c956c81.png  \n",
            "  inflating: train/Common Chickweed/1120761ef.png  \n",
            "  inflating: train/Common Chickweed/125c2316a.png  \n",
            "  inflating: train/Common Chickweed/1265c4a42.png  \n",
            "  inflating: train/Common Chickweed/128d62bc2.png  \n",
            "  inflating: train/Common Chickweed/137dad5ef.png  \n",
            "  inflating: train/Common Chickweed/142c503e1.png  \n",
            "  inflating: train/Common Chickweed/14486549c.png  \n",
            "  inflating: train/Common Chickweed/145b9d73c.png  \n",
            "  inflating: train/Common Chickweed/146feb316.png  \n",
            "  inflating: train/Common Chickweed/149f0527e.png  \n",
            "  inflating: train/Common Chickweed/14b7eab5d.png  \n",
            "  inflating: train/Common Chickweed/151a67732.png  \n",
            "  inflating: train/Common Chickweed/1599e5ec9.png  \n",
            "  inflating: train/Common Chickweed/15cbad146.png  \n",
            "  inflating: train/Common Chickweed/15f6bed07.png  \n",
            "  inflating: train/Common Chickweed/15ff4b029.png  \n",
            "  inflating: train/Common Chickweed/16033c75c.png  \n",
            "  inflating: train/Common Chickweed/1640f4ae3.png  \n",
            "  inflating: train/Common Chickweed/168b7d0ec.png  \n",
            "  inflating: train/Common Chickweed/168fed6c3.png  \n",
            "  inflating: train/Common Chickweed/169afb6aa.png  \n",
            "  inflating: train/Common Chickweed/16b1530ce.png  \n",
            "  inflating: train/Common Chickweed/16c5adff0.png  \n",
            "  inflating: train/Common Chickweed/16f17c7d1.png  \n",
            "  inflating: train/Common Chickweed/17bd14c74.png  \n",
            "  inflating: train/Common Chickweed/186660f3e.png  \n",
            "  inflating: train/Common Chickweed/18757e215.png  \n",
            "  inflating: train/Common Chickweed/18b1f733a.png  \n",
            "  inflating: train/Common Chickweed/19a44418c.png  \n",
            "  inflating: train/Common Chickweed/19f62aae6.png  \n",
            "  inflating: train/Common Chickweed/1a459e1de.png  \n",
            "  inflating: train/Common Chickweed/1a4f7a1e1.png  \n",
            "  inflating: train/Common Chickweed/1a5c28e0a.png  \n",
            "  inflating: train/Common Chickweed/1a81a6532.png  \n",
            "  inflating: train/Common Chickweed/1b32b40d8.png  \n",
            "  inflating: train/Common Chickweed/1b76d1681.png  \n",
            "  inflating: train/Common Chickweed/1bb666d37.png  \n",
            "  inflating: train/Common Chickweed/1c05c26a1.png  \n",
            "  inflating: train/Common Chickweed/1c1cce1e6.png  \n",
            "  inflating: train/Common Chickweed/1d00f7fab.png  \n",
            "  inflating: train/Common Chickweed/1df8e3b62.png  \n",
            "  inflating: train/Common Chickweed/1e21aff50.png  \n",
            "  inflating: train/Common Chickweed/1efb03a94.png  \n",
            "  inflating: train/Common Chickweed/2071d617e.png  \n",
            "  inflating: train/Common Chickweed/20a04ea79.png  \n",
            "  inflating: train/Common Chickweed/20d3a67d3.png  \n",
            "  inflating: train/Common Chickweed/2133c16c5.png  \n",
            "  inflating: train/Common Chickweed/21535707f.png  \n",
            "  inflating: train/Common Chickweed/21689ecb6.png  \n",
            "  inflating: train/Common Chickweed/21ace47d3.png  \n",
            "  inflating: train/Common Chickweed/2241265bb.png  \n",
            "  inflating: train/Common Chickweed/22e7c17b2.png  \n",
            "  inflating: train/Common Chickweed/22f1fd645.png  \n",
            "  inflating: train/Common Chickweed/23316b4b0.png  \n",
            "  inflating: train/Common Chickweed/23686333f.png  \n",
            "  inflating: train/Common Chickweed/2424bfd5a.png  \n",
            "  inflating: train/Common Chickweed/24a0d8df6.png  \n",
            "  inflating: train/Common Chickweed/2625375c8.png  \n",
            "  inflating: train/Common Chickweed/264e8b9b5.png  \n",
            "  inflating: train/Common Chickweed/27a78683b.png  \n",
            "  inflating: train/Common Chickweed/27d08b6f9.png  \n",
            "  inflating: train/Common Chickweed/27f0e13ae.png  \n",
            "  inflating: train/Common Chickweed/28285eb94.png  \n",
            "  inflating: train/Common Chickweed/289e929b2.png  \n",
            "  inflating: train/Common Chickweed/29c8ca750.png  \n",
            "  inflating: train/Common Chickweed/29cc438e4.png  \n",
            "  inflating: train/Common Chickweed/29d790068.png  \n",
            "  inflating: train/Common Chickweed/2a6305dc1.png  \n",
            "  inflating: train/Common Chickweed/2aa88416e.png  \n",
            "  inflating: train/Common Chickweed/2b7768772.png  \n",
            "  inflating: train/Common Chickweed/2bf2f5ff0.png  \n",
            "  inflating: train/Common Chickweed/2caa226b8.png  \n",
            "  inflating: train/Common Chickweed/2d131b7e6.png  \n",
            "  inflating: train/Common Chickweed/2d4a05fa0.png  \n",
            "  inflating: train/Common Chickweed/2d4fc5fe9.png  \n",
            "  inflating: train/Common Chickweed/2de5e6eaf.png  \n",
            "  inflating: train/Common Chickweed/2e025ece6.png  \n",
            "  inflating: train/Common Chickweed/2e4aad6ec.png  \n",
            "  inflating: train/Common Chickweed/2e5123448.png  \n",
            "  inflating: train/Common Chickweed/2e795ccf1.png  \n",
            "  inflating: train/Common Chickweed/2f60156c7.png  \n",
            "  inflating: train/Common Chickweed/2f963cc5b.png  \n",
            "  inflating: train/Common Chickweed/301114e3e.png  \n",
            "  inflating: train/Common Chickweed/303835197.png  \n",
            "  inflating: train/Common Chickweed/306e7dbd9.png  \n",
            "  inflating: train/Common Chickweed/30a166665.png  \n",
            "  inflating: train/Common Chickweed/310656b36.png  \n",
            "  inflating: train/Common Chickweed/31b2aa8b5.png  \n",
            "  inflating: train/Common Chickweed/32077f532.png  \n",
            "  inflating: train/Common Chickweed/326192149.png  \n",
            "  inflating: train/Common Chickweed/34455bf75.png  \n",
            "  inflating: train/Common Chickweed/35e31b2b5.png  \n",
            "  inflating: train/Common Chickweed/35f374f74.png  \n",
            "  inflating: train/Common Chickweed/36a913120.png  \n",
            "  inflating: train/Common Chickweed/36f392b83.png  \n",
            "  inflating: train/Common Chickweed/3740d90d5.png  \n",
            "  inflating: train/Common Chickweed/3777e7b53.png  \n",
            "  inflating: train/Common Chickweed/378a40743.png  \n",
            "  inflating: train/Common Chickweed/37b291dab.png  \n",
            "  inflating: train/Common Chickweed/37c20876e.png  \n",
            "  inflating: train/Common Chickweed/37cea3ddd.png  \n",
            "  inflating: train/Common Chickweed/3847a86c2.png  \n",
            "  inflating: train/Common Chickweed/387ad12e8.png  \n",
            "  inflating: train/Common Chickweed/38fb092f0.png  \n",
            "  inflating: train/Common Chickweed/39c7819ff.png  \n",
            "  inflating: train/Common Chickweed/3a2a3ddb9.png  \n",
            "  inflating: train/Common Chickweed/3a8202693.png  \n",
            "  inflating: train/Common Chickweed/3ab7ac054.png  \n",
            "  inflating: train/Common Chickweed/3cc8e571f.png  \n",
            "  inflating: train/Common Chickweed/3cf9ae5a3.png  \n",
            "  inflating: train/Common Chickweed/3e85b8e5c.png  \n",
            "  inflating: train/Common Chickweed/3ea21cf79.png  \n",
            "  inflating: train/Common Chickweed/3ee3ef6a3.png  \n",
            "  inflating: train/Common Chickweed/3f018d6bd.png  \n",
            "  inflating: train/Common Chickweed/3f5dfb308.png  \n",
            "  inflating: train/Common Chickweed/3f84b7aae.png  \n",
            "  inflating: train/Common Chickweed/40274f484.png  \n",
            "  inflating: train/Common Chickweed/40c5757c0.png  \n",
            "  inflating: train/Common Chickweed/410876711.png  \n",
            "  inflating: train/Common Chickweed/41140fa6a.png  \n",
            "  inflating: train/Common Chickweed/41e3d1276.png  \n",
            "  inflating: train/Common Chickweed/422cf9f7d.png  \n",
            "  inflating: train/Common Chickweed/4284d5831.png  \n",
            "  inflating: train/Common Chickweed/430ef0660.png  \n",
            "  inflating: train/Common Chickweed/436ac575f.png  \n",
            "  inflating: train/Common Chickweed/43e643775.png  \n",
            "  inflating: train/Common Chickweed/440d51444.png  \n",
            "  inflating: train/Common Chickweed/4426efc94.png  \n",
            "  inflating: train/Common Chickweed/45204fe38.png  \n",
            "  inflating: train/Common Chickweed/46b9f0a87.png  \n",
            "  inflating: train/Common Chickweed/47b316d8f.png  \n",
            "  inflating: train/Common Chickweed/4825503ed.png  \n",
            "  inflating: train/Common Chickweed/4861b377f.png  \n",
            "  inflating: train/Common Chickweed/486885e8d.png  \n",
            "  inflating: train/Common Chickweed/48bc50005.png  \n",
            "  inflating: train/Common Chickweed/495d1a520.png  \n",
            "  inflating: train/Common Chickweed/4a1121f7d.png  \n",
            "  inflating: train/Common Chickweed/4adc3cf32.png  \n",
            "  inflating: train/Common Chickweed/4b33a6880.png  \n",
            "  inflating: train/Common Chickweed/4c4b75233.png  \n",
            "  inflating: train/Common Chickweed/4c7ef6d25.png  \n",
            "  inflating: train/Common Chickweed/4cc8e1334.png  \n",
            "  inflating: train/Common Chickweed/4d6cbfb1e.png  \n",
            "  inflating: train/Common Chickweed/4d7682775.png  \n",
            "  inflating: train/Common Chickweed/4d8c841b3.png  \n",
            "  inflating: train/Common Chickweed/4d9ece5a7.png  \n",
            "  inflating: train/Common Chickweed/4dad46a5a.png  \n",
            "  inflating: train/Common Chickweed/4db7bbfbf.png  \n",
            "  inflating: train/Common Chickweed/4e1e4b61e.png  \n",
            "  inflating: train/Common Chickweed/4e334d9c0.png  \n",
            "  inflating: train/Common Chickweed/4e34f35c1.png  \n",
            "  inflating: train/Common Chickweed/4e65c6702.png  \n",
            "  inflating: train/Common Chickweed/4e726de0b.png  \n",
            "  inflating: train/Common Chickweed/4f3fb3fa7.png  \n",
            "  inflating: train/Common Chickweed/4f89e332f.png  \n",
            "  inflating: train/Common Chickweed/4f9359d80.png  \n",
            "  inflating: train/Common Chickweed/50659ebe2.png  \n",
            "  inflating: train/Common Chickweed/50b0d5abf.png  \n",
            "  inflating: train/Common Chickweed/51122ffd7.png  \n",
            "  inflating: train/Common Chickweed/5139b104e.png  \n",
            "  inflating: train/Common Chickweed/517dd299e.png  \n",
            "  inflating: train/Common Chickweed/518787d0b.png  \n",
            "  inflating: train/Common Chickweed/520224ad0.png  \n",
            "  inflating: train/Common Chickweed/532228e34.png  \n",
            "  inflating: train/Common Chickweed/5324a9ab2.png  \n",
            "  inflating: train/Common Chickweed/533a926b2.png  \n",
            "  inflating: train/Common Chickweed/53585f37d.png  \n",
            "  inflating: train/Common Chickweed/54a3a899b.png  \n",
            "  inflating: train/Common Chickweed/5629b467c.png  \n",
            "  inflating: train/Common Chickweed/5687df8c6.png  \n",
            "  inflating: train/Common Chickweed/56a3e134b.png  \n",
            "  inflating: train/Common Chickweed/56f69db16.png  \n",
            "  inflating: train/Common Chickweed/5713630ed.png  \n",
            "  inflating: train/Common Chickweed/57664aa6b.png  \n",
            "  inflating: train/Common Chickweed/57731eb29.png  \n",
            "  inflating: train/Common Chickweed/5790e7b05.png  \n",
            "  inflating: train/Common Chickweed/57c3c7b86.png  \n",
            "  inflating: train/Common Chickweed/57eaf2687.png  \n",
            "  inflating: train/Common Chickweed/58aeb692f.png  \n",
            "  inflating: train/Common Chickweed/58b405634.png  \n",
            "  inflating: train/Common Chickweed/58f366d30.png  \n",
            "  inflating: train/Common Chickweed/593e2f0e9.png  \n",
            "  inflating: train/Common Chickweed/5a044c06b.png  \n",
            "  inflating: train/Common Chickweed/5ace4b78d.png  \n",
            "  inflating: train/Common Chickweed/5b4414b1e.png  \n",
            "  inflating: train/Common Chickweed/5b4b5f5ca.png  \n",
            "  inflating: train/Common Chickweed/5ba9f6698.png  \n",
            "  inflating: train/Common Chickweed/5c62f507e.png  \n",
            "  inflating: train/Common Chickweed/5c82a988b.png  \n",
            "  inflating: train/Common Chickweed/5cd0ccfc1.png  \n",
            "  inflating: train/Common Chickweed/5dcac8cb4.png  \n",
            "  inflating: train/Common Chickweed/5e0f7d75d.png  \n",
            "  inflating: train/Common Chickweed/5ea01515a.png  \n",
            "  inflating: train/Common Chickweed/5ef4bcfa9.png  \n",
            "  inflating: train/Common Chickweed/5f128ee9b.png  \n",
            "  inflating: train/Common Chickweed/5f2d126eb.png  \n",
            "  inflating: train/Common Chickweed/5f39564f3.png  \n",
            "  inflating: train/Common Chickweed/5fc9fac58.png  \n",
            "  inflating: train/Common Chickweed/5fe2a641d.png  \n",
            "  inflating: train/Common Chickweed/604fddba7.png  \n",
            "  inflating: train/Common Chickweed/60fc956cf.png  \n",
            "  inflating: train/Common Chickweed/611fc426b.png  \n",
            "  inflating: train/Common Chickweed/61a3a0f94.png  \n",
            "  inflating: train/Common Chickweed/61cb94bb2.png  \n",
            "  inflating: train/Common Chickweed/61e613144.png  \n",
            "  inflating: train/Common Chickweed/6232a5fcb.png  \n",
            "  inflating: train/Common Chickweed/62351b320.png  \n",
            "  inflating: train/Common Chickweed/6283e8862.png  \n",
            "  inflating: train/Common Chickweed/63625a31d.png  \n",
            "  inflating: train/Common Chickweed/63ac8cb8b.png  \n",
            "  inflating: train/Common Chickweed/63bb36f63.png  \n",
            "  inflating: train/Common Chickweed/63bbfb907.png  \n",
            "  inflating: train/Common Chickweed/63e5b3269.png  \n",
            "  inflating: train/Common Chickweed/646556430.png  \n",
            "  inflating: train/Common Chickweed/64923f954.png  \n",
            "  inflating: train/Common Chickweed/6504263bb.png  \n",
            "  inflating: train/Common Chickweed/654021d0e.png  \n",
            "  inflating: train/Common Chickweed/654f701ad.png  \n",
            "  inflating: train/Common Chickweed/66c7f42fb.png  \n",
            "  inflating: train/Common Chickweed/66f599454.png  \n",
            "  inflating: train/Common Chickweed/672982f1f.png  \n",
            "  inflating: train/Common Chickweed/672d71ed0.png  \n",
            "  inflating: train/Common Chickweed/67ea1b535.png  \n",
            "  inflating: train/Common Chickweed/687fdd0f0.png  \n",
            "  inflating: train/Common Chickweed/68d61152a.png  \n",
            "  inflating: train/Common Chickweed/6a7ea6097.png  \n",
            "  inflating: train/Common Chickweed/6ab3f3bf8.png  \n",
            "  inflating: train/Common Chickweed/6abbe7528.png  \n",
            "  inflating: train/Common Chickweed/6afe50f7c.png  \n",
            "  inflating: train/Common Chickweed/6b392ca91.png  \n",
            "  inflating: train/Common Chickweed/6b6df19b2.png  \n",
            "  inflating: train/Common Chickweed/6c2a49621.png  \n",
            "  inflating: train/Common Chickweed/6cc932059.png  \n",
            "  inflating: train/Common Chickweed/6d993edb1.png  \n",
            "  inflating: train/Common Chickweed/6dcb4a699.png  \n",
            "  inflating: train/Common Chickweed/6e64646e7.png  \n",
            "  inflating: train/Common Chickweed/6f6c3d6aa.png  \n",
            "  inflating: train/Common Chickweed/70031d587.png  \n",
            "  inflating: train/Common Chickweed/70083d11f.png  \n",
            "  inflating: train/Common Chickweed/709ff44b4.png  \n",
            "  inflating: train/Common Chickweed/70d2d710d.png  \n",
            "  inflating: train/Common Chickweed/7175e9d7d.png  \n",
            "  inflating: train/Common Chickweed/7178d1aa1.png  \n",
            "  inflating: train/Common Chickweed/75742ed25.png  \n",
            "  inflating: train/Common Chickweed/763b0b8cd.png  \n",
            "  inflating: train/Common Chickweed/76deb2f76.png  \n",
            "  inflating: train/Common Chickweed/7723b22fe.png  \n",
            "  inflating: train/Common Chickweed/774bf7020.png  \n",
            "  inflating: train/Common Chickweed/77990844c.png  \n",
            "  inflating: train/Common Chickweed/77b53a29c.png  \n",
            "  inflating: train/Common Chickweed/78604fc59.png  \n",
            "  inflating: train/Common Chickweed/786df0a52.png  \n",
            "  inflating: train/Common Chickweed/78836bb95.png  \n",
            "  inflating: train/Common Chickweed/793d8f855.png  \n",
            "  inflating: train/Common Chickweed/7a2d25120.png  \n",
            "  inflating: train/Common Chickweed/7a597f825.png  \n",
            "  inflating: train/Common Chickweed/7a7c2d6f8.png  \n",
            "  inflating: train/Common Chickweed/7acfcf6cf.png  \n",
            "  inflating: train/Common Chickweed/7b0586012.png  \n",
            "  inflating: train/Common Chickweed/7b1018f5f.png  \n",
            "  inflating: train/Common Chickweed/7b177457d.png  \n",
            "  inflating: train/Common Chickweed/7b4e40d34.png  \n",
            "  inflating: train/Common Chickweed/7b5ff3a68.png  \n",
            "  inflating: train/Common Chickweed/7c867adb0.png  \n",
            "  inflating: train/Common Chickweed/7c933aa92.png  \n",
            "  inflating: train/Common Chickweed/7cb13f366.png  \n",
            "  inflating: train/Common Chickweed/7d7675873.png  \n",
            "  inflating: train/Common Chickweed/7da7bd2e3.png  \n",
            "  inflating: train/Common Chickweed/7dd728ef8.png  \n",
            "  inflating: train/Common Chickweed/7e2d89904.png  \n",
            "  inflating: train/Common Chickweed/7e3aa91d2.png  \n",
            "  inflating: train/Common Chickweed/7e6037c88.png  \n",
            "  inflating: train/Common Chickweed/7ec5a6226.png  \n",
            "  inflating: train/Common Chickweed/7efcaa68a.png  \n",
            "  inflating: train/Common Chickweed/8060d0736.png  \n",
            "  inflating: train/Common Chickweed/808d8b604.png  \n",
            "  inflating: train/Common Chickweed/82525f45c.png  \n",
            "  inflating: train/Common Chickweed/82561432a.png  \n",
            "  inflating: train/Common Chickweed/8349ffb69.png  \n",
            "  inflating: train/Common Chickweed/8384040ba.png  \n",
            "  inflating: train/Common Chickweed/838c25c16.png  \n",
            "  inflating: train/Common Chickweed/839fad8be.png  \n",
            "  inflating: train/Common Chickweed/841b56a7c.png  \n",
            "  inflating: train/Common Chickweed/8422af10c.png  \n",
            "  inflating: train/Common Chickweed/846dfcd48.png  \n",
            "  inflating: train/Common Chickweed/852fd9b8a.png  \n",
            "  inflating: train/Common Chickweed/8530528b4.png  \n",
            "  inflating: train/Common Chickweed/8575a6820.png  \n",
            "  inflating: train/Common Chickweed/85942cd42.png  \n",
            "  inflating: train/Common Chickweed/8594bb8c9.png  \n",
            "  inflating: train/Common Chickweed/879f9bc26.png  \n",
            "  inflating: train/Common Chickweed/880cabce7.png  \n",
            "  inflating: train/Common Chickweed/88314d63e.png  \n",
            "  inflating: train/Common Chickweed/88823573a.png  \n",
            "  inflating: train/Common Chickweed/88e1e3d2b.png  \n",
            "  inflating: train/Common Chickweed/891a5fdca.png  \n",
            "  inflating: train/Common Chickweed/892382b05.png  \n",
            "  inflating: train/Common Chickweed/89fe8aa9d.png  \n",
            "  inflating: train/Common Chickweed/8a22b4f1f.png  \n",
            "  inflating: train/Common Chickweed/8a2539acf.png  \n",
            "  inflating: train/Common Chickweed/8a66600c0.png  \n",
            "  inflating: train/Common Chickweed/8b5f42788.png  \n",
            "  inflating: train/Common Chickweed/8b9cc2752.png  \n",
            "  inflating: train/Common Chickweed/8d1805187.png  \n",
            "  inflating: train/Common Chickweed/8d2429f5b.png  \n",
            "  inflating: train/Common Chickweed/8d6288d95.png  \n",
            "  inflating: train/Common Chickweed/8d7b4ca96.png  \n",
            "  inflating: train/Common Chickweed/8e5447e81.png  \n",
            "  inflating: train/Common Chickweed/8ebf898d1.png  \n",
            "  inflating: train/Common Chickweed/9011a592f.png  \n",
            "  inflating: train/Common Chickweed/91d294b43.png  \n",
            "  inflating: train/Common Chickweed/9204a01d8.png  \n",
            "  inflating: train/Common Chickweed/9253be20b.png  \n",
            "  inflating: train/Common Chickweed/92bd3b2b7.png  \n",
            "  inflating: train/Common Chickweed/92e13ce8b.png  \n",
            "  inflating: train/Common Chickweed/930ebfb48.png  \n",
            "  inflating: train/Common Chickweed/937319dc7.png  \n",
            "  inflating: train/Common Chickweed/9435b2c58.png  \n",
            "  inflating: train/Common Chickweed/94613db53.png  \n",
            "  inflating: train/Common Chickweed/965f8a9c2.png  \n",
            "  inflating: train/Common Chickweed/974108721.png  \n",
            "  inflating: train/Common Chickweed/97ab5baf0.png  \n",
            "  inflating: train/Common Chickweed/983aed879.png  \n",
            "  inflating: train/Common Chickweed/985eb4e09.png  \n",
            "  inflating: train/Common Chickweed/98b18ed7a.png  \n",
            "  inflating: train/Common Chickweed/98f407d78.png  \n",
            "  inflating: train/Common Chickweed/9902103a5.png  \n",
            "  inflating: train/Common Chickweed/990f7310e.png  \n",
            "  inflating: train/Common Chickweed/993fcfaa4.png  \n",
            "  inflating: train/Common Chickweed/99c59a981.png  \n",
            "  inflating: train/Common Chickweed/9ad3aa199.png  \n",
            "  inflating: train/Common Chickweed/9b09003fa.png  \n",
            "  inflating: train/Common Chickweed/9b3f2f7a1.png  \n",
            "  inflating: train/Common Chickweed/9c383ae53.png  \n",
            "  inflating: train/Common Chickweed/9c721a7dc.png  \n",
            "  inflating: train/Common Chickweed/9ca2edaab.png  \n",
            "  inflating: train/Common Chickweed/9ca3fd969.png  \n",
            "  inflating: train/Common Chickweed/9d1b4794a.png  \n",
            "  inflating: train/Common Chickweed/9ea59b076.png  \n",
            "  inflating: train/Common Chickweed/9fb44806e.png  \n",
            "  inflating: train/Common Chickweed/a04f38d7e.png  \n",
            "  inflating: train/Common Chickweed/a0c39c1dd.png  \n",
            "  inflating: train/Common Chickweed/a194c0dd6.png  \n",
            "  inflating: train/Common Chickweed/a23bbb5c9.png  \n",
            "  inflating: train/Common Chickweed/a2478ca57.png  \n",
            "  inflating: train/Common Chickweed/a38aa2204.png  \n",
            "  inflating: train/Common Chickweed/a3e3b178c.png  \n",
            "  inflating: train/Common Chickweed/a42ddba4f.png  \n",
            "  inflating: train/Common Chickweed/a47105ef4.png  \n",
            "  inflating: train/Common Chickweed/a5b9d84a3.png  \n",
            "  inflating: train/Common Chickweed/a669c101f.png  \n",
            "  inflating: train/Common Chickweed/a6c251d63.png  \n",
            "  inflating: train/Common Chickweed/a6d54c45c.png  \n",
            "  inflating: train/Common Chickweed/a8045aea3.png  \n",
            "  inflating: train/Common Chickweed/a855cbc06.png  \n",
            "  inflating: train/Common Chickweed/a85a62939.png  \n",
            "  inflating: train/Common Chickweed/a88bd5aef.png  \n",
            "  inflating: train/Common Chickweed/a8968f15a.png  \n",
            "  inflating: train/Common Chickweed/a96438dae.png  \n",
            "  inflating: train/Common Chickweed/a98a9b092.png  \n",
            "  inflating: train/Common Chickweed/a9e03b3a1.png  \n",
            "  inflating: train/Common Chickweed/a9e3465f8.png  \n",
            "  inflating: train/Common Chickweed/aa28f442c.png  \n",
            "  inflating: train/Common Chickweed/aa83de6bb.png  \n",
            "  inflating: train/Common Chickweed/aa8778e2d.png  \n",
            "  inflating: train/Common Chickweed/aac309dc5.png  \n",
            "  inflating: train/Common Chickweed/ab3d174fc.png  \n",
            "  inflating: train/Common Chickweed/ab6338bd1.png  \n",
            "  inflating: train/Common Chickweed/ab9a60c53.png  \n",
            "  inflating: train/Common Chickweed/aba570b21.png  \n",
            "  inflating: train/Common Chickweed/ac0b860c4.png  \n",
            "  inflating: train/Common Chickweed/ac85f848f.png  \n",
            "  inflating: train/Common Chickweed/acca2b10d.png  \n",
            "  inflating: train/Common Chickweed/ad2df5fa3.png  \n",
            "  inflating: train/Common Chickweed/ad5906a2f.png  \n",
            "  inflating: train/Common Chickweed/adf3032af.png  \n",
            "  inflating: train/Common Chickweed/ae7415e25.png  \n",
            "  inflating: train/Common Chickweed/af005febb.png  \n",
            "  inflating: train/Common Chickweed/af98e2c11.png  \n",
            "  inflating: train/Common Chickweed/afeabc763.png  \n",
            "  inflating: train/Common Chickweed/b0b543038.png  \n",
            "  inflating: train/Common Chickweed/b116fabdd.png  \n",
            "  inflating: train/Common Chickweed/b176751f4.png  \n",
            "  inflating: train/Common Chickweed/b25c85290.png  \n",
            "  inflating: train/Common Chickweed/b2b8055ad.png  \n",
            "  inflating: train/Common Chickweed/b35eaa0a1.png  \n",
            "  inflating: train/Common Chickweed/b40d58c8e.png  \n",
            "  inflating: train/Common Chickweed/b48e67073.png  \n",
            "  inflating: train/Common Chickweed/b4bf76e9f.png  \n",
            "  inflating: train/Common Chickweed/b4e188202.png  \n",
            "  inflating: train/Common Chickweed/b53c5ac08.png  \n",
            "  inflating: train/Common Chickweed/b61695466.png  \n",
            "  inflating: train/Common Chickweed/b8e6c7b66.png  \n",
            "  inflating: train/Common Chickweed/b953bb136.png  \n",
            "  inflating: train/Common Chickweed/b962fdcfc.png  \n",
            "  inflating: train/Common Chickweed/b9c8f7046.png  \n",
            "  inflating: train/Common Chickweed/b9f4cec11.png  \n",
            "  inflating: train/Common Chickweed/bb52da32b.png  \n",
            "  inflating: train/Common Chickweed/bb7ac7386.png  \n",
            "  inflating: train/Common Chickweed/bb824511b.png  \n",
            "  inflating: train/Common Chickweed/bbfa8d1c9.png  \n",
            "  inflating: train/Common Chickweed/bc8889926.png  \n",
            "  inflating: train/Common Chickweed/bc92f8149.png  \n",
            "  inflating: train/Common Chickweed/bcf088ff7.png  \n",
            "  inflating: train/Common Chickweed/bd4304980.png  \n",
            "  inflating: train/Common Chickweed/bd4565df5.png  \n",
            "  inflating: train/Common Chickweed/bddde0312.png  \n",
            "  inflating: train/Common Chickweed/be325b3b9.png  \n",
            "  inflating: train/Common Chickweed/beda08ba5.png  \n",
            "  inflating: train/Common Chickweed/bf0667924.png  \n",
            "  inflating: train/Common Chickweed/bf351fa53.png  \n",
            "  inflating: train/Common Chickweed/bf77ce974.png  \n",
            "  inflating: train/Common Chickweed/bf9f74bd3.png  \n",
            "  inflating: train/Common Chickweed/bfa5d2e2f.png  \n",
            "  inflating: train/Common Chickweed/bfb59c16f.png  \n",
            "  inflating: train/Common Chickweed/c0b85294c.png  \n",
            "  inflating: train/Common Chickweed/c0fd4e4aa.png  \n",
            "  inflating: train/Common Chickweed/c16801386.png  \n",
            "  inflating: train/Common Chickweed/c23f79208.png  \n",
            "  inflating: train/Common Chickweed/c27253052.png  \n",
            "  inflating: train/Common Chickweed/c2ab91ad2.png  \n",
            "  inflating: train/Common Chickweed/c2b5f56d8.png  \n",
            "  inflating: train/Common Chickweed/c2c1a8787.png  \n",
            "  inflating: train/Common Chickweed/c2e069515.png  \n",
            "  inflating: train/Common Chickweed/c2f4c85d1.png  \n",
            "  inflating: train/Common Chickweed/c41644ed6.png  \n",
            "  inflating: train/Common Chickweed/c5535e3b8.png  \n",
            "  inflating: train/Common Chickweed/c5ddfa4a2.png  \n",
            "  inflating: train/Common Chickweed/c67da9c32.png  \n",
            "  inflating: train/Common Chickweed/c6cf113bc.png  \n",
            "  inflating: train/Common Chickweed/c6f9ba1cd.png  \n",
            "  inflating: train/Common Chickweed/c7d7cbd73.png  \n",
            "  inflating: train/Common Chickweed/c7e585163.png  \n",
            "  inflating: train/Common Chickweed/c88a5b482.png  \n",
            "  inflating: train/Common Chickweed/c908a478e.png  \n",
            "  inflating: train/Common Chickweed/c9aea5d7b.png  \n",
            "  inflating: train/Common Chickweed/c9d5d691e.png  \n",
            "  inflating: train/Common Chickweed/caae3c376.png  \n",
            "  inflating: train/Common Chickweed/cae00a248.png  \n",
            "  inflating: train/Common Chickweed/cae5588d0.png  \n",
            "  inflating: train/Common Chickweed/cb199a0d6.png  \n",
            "  inflating: train/Common Chickweed/cb5e1b823.png  \n",
            "  inflating: train/Common Chickweed/cb64ce021.png  \n",
            "  inflating: train/Common Chickweed/cba7f2307.png  \n",
            "  inflating: train/Common Chickweed/cbd1cffdb.png  \n",
            "  inflating: train/Common Chickweed/cc2a8c766.png  \n",
            "  inflating: train/Common Chickweed/ce0fb7c25.png  \n",
            "  inflating: train/Common Chickweed/ce19a867c.png  \n",
            "  inflating: train/Common Chickweed/cf6e5d506.png  \n",
            "  inflating: train/Common Chickweed/cfda6ed03.png  \n",
            "  inflating: train/Common Chickweed/cffd34ae2.png  \n",
            "  inflating: train/Common Chickweed/d018c5d19.png  \n",
            "  inflating: train/Common Chickweed/d021a9611.png  \n",
            "  inflating: train/Common Chickweed/d07b7062b.png  \n",
            "  inflating: train/Common Chickweed/d07da6faf.png  \n",
            "  inflating: train/Common Chickweed/d0bd8f934.png  \n",
            "  inflating: train/Common Chickweed/d13abd726.png  \n",
            "  inflating: train/Common Chickweed/d14306ba9.png  \n",
            "  inflating: train/Common Chickweed/d194fd113.png  \n",
            "  inflating: train/Common Chickweed/d1dd866ee.png  \n",
            "  inflating: train/Common Chickweed/d22156fb3.png  \n",
            "  inflating: train/Common Chickweed/d397d1c9c.png  \n",
            "  inflating: train/Common Chickweed/d4a084033.png  \n",
            "  inflating: train/Common Chickweed/d643fc811.png  \n",
            "  inflating: train/Common Chickweed/d690ebc8b.png  \n",
            "  inflating: train/Common Chickweed/d6f625574.png  \n",
            "  inflating: train/Common Chickweed/d7c27119a.png  \n",
            "  inflating: train/Common Chickweed/d7cf2db87.png  \n",
            "  inflating: train/Common Chickweed/d88a609d1.png  \n",
            "  inflating: train/Common Chickweed/d920f1441.png  \n",
            "  inflating: train/Common Chickweed/d958d346f.png  \n",
            "  inflating: train/Common Chickweed/d974f1537.png  \n",
            "  inflating: train/Common Chickweed/d9d1602c7.png  \n",
            "  inflating: train/Common Chickweed/db4775157.png  \n",
            "  inflating: train/Common Chickweed/dc18969ae.png  \n",
            "  inflating: train/Common Chickweed/dc467ebaa.png  \n",
            "  inflating: train/Common Chickweed/dd3f63ce0.png  \n",
            "  inflating: train/Common Chickweed/dd575058e.png  \n",
            "  inflating: train/Common Chickweed/dd76f845f.png  \n",
            "  inflating: train/Common Chickweed/ddbd81175.png  \n",
            "  inflating: train/Common Chickweed/dfa83c1d5.png  \n",
            "  inflating: train/Common Chickweed/dfc7cb278.png  \n",
            "  inflating: train/Common Chickweed/e00df1f35.png  \n",
            "  inflating: train/Common Chickweed/e18f7124b.png  \n",
            "  inflating: train/Common Chickweed/e1b642054.png  \n",
            "  inflating: train/Common Chickweed/e1c89faa5.png  \n",
            "  inflating: train/Common Chickweed/e1f62327d.png  \n",
            "  inflating: train/Common Chickweed/e231ad747.png  \n",
            "  inflating: train/Common Chickweed/e2e49c76e.png  \n",
            "  inflating: train/Common Chickweed/e32fdd03a.png  \n",
            "  inflating: train/Common Chickweed/e33673091.png  \n",
            "  inflating: train/Common Chickweed/e3f88fcde.png  \n",
            "  inflating: train/Common Chickweed/e44eed3d0.png  \n",
            "  inflating: train/Common Chickweed/e64ffaebb.png  \n",
            "  inflating: train/Common Chickweed/e686b7cb2.png  \n",
            "  inflating: train/Common Chickweed/e768d348e.png  \n",
            "  inflating: train/Common Chickweed/e7792c058.png  \n",
            "  inflating: train/Common Chickweed/e7aca715c.png  \n",
            "  inflating: train/Common Chickweed/e8cef28fa.png  \n",
            "  inflating: train/Common Chickweed/e977a6aa7.png  \n",
            "  inflating: train/Common Chickweed/e9ba4c313.png  \n",
            "  inflating: train/Common Chickweed/ea395710d.png  \n",
            "  inflating: train/Common Chickweed/ea3e13899.png  \n",
            "  inflating: train/Common Chickweed/ea5034835.png  \n",
            "  inflating: train/Common Chickweed/eac94830d.png  \n",
            "  inflating: train/Common Chickweed/eaeb6ab45.png  \n",
            "  inflating: train/Common Chickweed/eb47d3894.png  \n",
            "  inflating: train/Common Chickweed/eb6b7fe4c.png  \n",
            "  inflating: train/Common Chickweed/ebce04afd.png  \n",
            "  inflating: train/Common Chickweed/ed0299f6d.png  \n",
            "  inflating: train/Common Chickweed/eddaf3d47.png  \n",
            "  inflating: train/Common Chickweed/ee1935fb6.png  \n",
            "  inflating: train/Common Chickweed/ee30999d4.png  \n",
            "  inflating: train/Common Chickweed/eebe14887.png  \n",
            "  inflating: train/Common Chickweed/ef255fde9.png  \n",
            "  inflating: train/Common Chickweed/ef9e034f0.png  \n",
            "  inflating: train/Common Chickweed/efd04a634.png  \n",
            "  inflating: train/Common Chickweed/f1258d04a.png  \n",
            "  inflating: train/Common Chickweed/f1530c3e3.png  \n",
            "  inflating: train/Common Chickweed/f17e5a3c3.png  \n",
            "  inflating: train/Common Chickweed/f18f2ca04.png  \n",
            "  inflating: train/Common Chickweed/f1d313b52.png  \n",
            "  inflating: train/Common Chickweed/f1ed58cdb.png  \n",
            "  inflating: train/Common Chickweed/f1f82a44c.png  \n",
            "  inflating: train/Common Chickweed/f1f8a7190.png  \n",
            "  inflating: train/Common Chickweed/f2806a6a2.png  \n",
            "  inflating: train/Common Chickweed/f3521cf79.png  \n",
            "  inflating: train/Common Chickweed/f394b73f6.png  \n",
            "  inflating: train/Common Chickweed/f3e74582f.png  \n",
            "  inflating: train/Common Chickweed/f41055895.png  \n",
            "  inflating: train/Common Chickweed/f47065e0a.png  \n",
            "  inflating: train/Common Chickweed/f50c8181a.png  \n",
            "  inflating: train/Common Chickweed/f63b0be0d.png  \n",
            "  inflating: train/Common Chickweed/f69475d12.png  \n",
            "  inflating: train/Common Chickweed/f71a0f489.png  \n",
            "  inflating: train/Common Chickweed/f7b86986a.png  \n",
            "  inflating: train/Common Chickweed/f84bd627e.png  \n",
            "  inflating: train/Common Chickweed/f956911a6.png  \n",
            "  inflating: train/Common Chickweed/f9b43d3c6.png  \n",
            "  inflating: train/Common Chickweed/fa468d955.png  \n",
            "  inflating: train/Common Chickweed/fb78383ae.png  \n",
            "  inflating: train/Common Chickweed/fcceab9ee.png  \n",
            "  inflating: train/Common Chickweed/fcfb8df3d.png  \n",
            "  inflating: train/Common Chickweed/fd08aae02.png  \n",
            "  inflating: train/Common Chickweed/fe03224a0.png  \n",
            "  inflating: train/Common Chickweed/fe1092cd5.png  \n",
            "  inflating: train/Common Chickweed/fe7373785.png  \n",
            "  inflating: train/Common Chickweed/fe801c9c0.png  \n",
            "  inflating: train/Common Chickweed/feafa22cc.png  \n",
            "  inflating: train/Common Chickweed/feb7699d0.png  \n",
            "  inflating: train/Common Chickweed/fefaeec6d.png  \n",
            "  inflating: train/Common Chickweed/ff934fcc7.png  \n",
            "  inflating: train/Common Chickweed/ffc02550b.png  \n",
            "  inflating: train/Common Chickweed/ffdddcf4e.png  \n",
            "  inflating: train/Common wheat/012db0f43.png  \n",
            "  inflating: train/Common wheat/01a2ae45e.png  \n",
            "  inflating: train/Common wheat/0382d0faf.png  \n",
            "  inflating: train/Common wheat/04468fad4.png  \n",
            "  inflating: train/Common wheat/060e8f499.png  \n",
            "  inflating: train/Common wheat/096ec46ec.png  \n",
            "  inflating: train/Common wheat/0975602f4.png  \n",
            "  inflating: train/Common wheat/0a4a26651.png  \n",
            "  inflating: train/Common wheat/0df5ee8a2.png  \n",
            "  inflating: train/Common wheat/0ec23ca76.png  \n",
            "  inflating: train/Common wheat/0ff464e3e.png  \n",
            "  inflating: train/Common wheat/106bfb13a.png  \n",
            "  inflating: train/Common wheat/108bf8703.png  \n",
            "  inflating: train/Common wheat/11b88da7c.png  \n",
            "  inflating: train/Common wheat/126c8b947.png  \n",
            "  inflating: train/Common wheat/12e0ffb23.png  \n",
            "  inflating: train/Common wheat/136134853.png  \n",
            "  inflating: train/Common wheat/138581771.png  \n",
            "  inflating: train/Common wheat/143774101.png  \n",
            "  inflating: train/Common wheat/1478bcfcd.png  \n",
            "  inflating: train/Common wheat/14d8b012c.png  \n",
            "  inflating: train/Common wheat/158e0e65d.png  \n",
            "  inflating: train/Common wheat/166de2ae2.png  \n",
            "  inflating: train/Common wheat/17c9616c8.png  \n",
            "  inflating: train/Common wheat/188bdc00d.png  \n",
            "  inflating: train/Common wheat/18fe42109.png  \n",
            "  inflating: train/Common wheat/1a5243aa7.png  \n",
            "  inflating: train/Common wheat/1a6add1b0.png  \n",
            "  inflating: train/Common wheat/1a9a859c9.png  \n",
            "  inflating: train/Common wheat/1d80baed6.png  \n",
            "  inflating: train/Common wheat/1e3f2459f.png  \n",
            "  inflating: train/Common wheat/200457edd.png  \n",
            "  inflating: train/Common wheat/2163a30d4.png  \n",
            "  inflating: train/Common wheat/21f36bdf0.png  \n",
            "  inflating: train/Common wheat/22893299b.png  \n",
            "  inflating: train/Common wheat/22ad759f1.png  \n",
            "  inflating: train/Common wheat/23c21923f.png  \n",
            "  inflating: train/Common wheat/297e193dc.png  \n",
            "  inflating: train/Common wheat/2c4412af8.png  \n",
            "  inflating: train/Common wheat/2c4813e68.png  \n",
            "  inflating: train/Common wheat/2d59da822.png  \n",
            "  inflating: train/Common wheat/2e7881f74.png  \n",
            "  inflating: train/Common wheat/2eb324e28.png  \n",
            "  inflating: train/Common wheat/2f45d4b13.png  \n",
            "  inflating: train/Common wheat/30c632d2a.png  \n",
            "  inflating: train/Common wheat/33870e597.png  \n",
            "  inflating: train/Common wheat/3620378ef.png  \n",
            "  inflating: train/Common wheat/373a8d06b.png  \n",
            "  inflating: train/Common wheat/39f8f1c54.png  \n",
            "  inflating: train/Common wheat/3a40ee68b.png  \n",
            "  inflating: train/Common wheat/3bb7f71ae.png  \n",
            "  inflating: train/Common wheat/3bd84db2c.png  \n",
            "  inflating: train/Common wheat/3dc5876a5.png  \n",
            "  inflating: train/Common wheat/3e23bc37c.png  \n",
            "  inflating: train/Common wheat/4003af513.png  \n",
            "  inflating: train/Common wheat/42098546c.png  \n",
            "  inflating: train/Common wheat/42646d53e.png  \n",
            "  inflating: train/Common wheat/427c3fabc.png  \n",
            "  inflating: train/Common wheat/43795dcec.png  \n",
            "  inflating: train/Common wheat/44c6a7f4c.png  \n",
            "  inflating: train/Common wheat/470263795.png  \n",
            "  inflating: train/Common wheat/4836fe8a1.png  \n",
            "  inflating: train/Common wheat/48d7b23ca.png  \n",
            "  inflating: train/Common wheat/4929747f1.png  \n",
            "  inflating: train/Common wheat/4954fcb0d.png  \n",
            "  inflating: train/Common wheat/4a56f32c6.png  \n",
            "  inflating: train/Common wheat/4a65b156c.png  \n",
            "  inflating: train/Common wheat/4fddf78f3.png  \n",
            "  inflating: train/Common wheat/4fe8caddb.png  \n",
            "  inflating: train/Common wheat/5089de64b.png  \n",
            "  inflating: train/Common wheat/528ef22fe.png  \n",
            "  inflating: train/Common wheat/5386db683.png  \n",
            "  inflating: train/Common wheat/554ec08a6.png  \n",
            "  inflating: train/Common wheat/5689a8503.png  \n",
            "  inflating: train/Common wheat/585041992.png  \n",
            "  inflating: train/Common wheat/5926abc69.png  \n",
            "  inflating: train/Common wheat/597d0557e.png  \n",
            "  inflating: train/Common wheat/5a8ba1976.png  \n",
            "  inflating: train/Common wheat/5bc3608c2.png  \n",
            "  inflating: train/Common wheat/602b2a010.png  \n",
            "  inflating: train/Common wheat/60f3aa0df.png  \n",
            "  inflating: train/Common wheat/62368871d.png  \n",
            "  inflating: train/Common wheat/628294daf.png  \n",
            "  inflating: train/Common wheat/629176d9d.png  \n",
            "  inflating: train/Common wheat/6327092cc.png  \n",
            "  inflating: train/Common wheat/64761ee87.png  \n",
            "  inflating: train/Common wheat/6646c37a8.png  \n",
            "  inflating: train/Common wheat/68c176337.png  \n",
            "  inflating: train/Common wheat/6abbc4d24.png  \n",
            "  inflating: train/Common wheat/6cb186ea3.png  \n",
            "  inflating: train/Common wheat/6cea44433.png  \n",
            "  inflating: train/Common wheat/6d428b06d.png  \n",
            "  inflating: train/Common wheat/6dfb9a152.png  \n",
            "  inflating: train/Common wheat/6e070b7cd.png  \n",
            "  inflating: train/Common wheat/6e083d6ff.png  \n",
            "  inflating: train/Common wheat/71c7dd2a2.png  \n",
            "  inflating: train/Common wheat/725b54918.png  \n",
            "  inflating: train/Common wheat/72cccd8cc.png  \n",
            "  inflating: train/Common wheat/73eaf7247.png  \n",
            "  inflating: train/Common wheat/75d126b6e.png  \n",
            "  inflating: train/Common wheat/77292d8e9.png  \n",
            "  inflating: train/Common wheat/77e9ac562.png  \n",
            "  inflating: train/Common wheat/78e98a1ea.png  \n",
            "  inflating: train/Common wheat/792c9887d.png  \n",
            "  inflating: train/Common wheat/79f70b2d7.png  \n",
            "  inflating: train/Common wheat/7afa50d94.png  \n",
            "  inflating: train/Common wheat/7d6424a0c.png  \n",
            "  inflating: train/Common wheat/7d87648c2.png  \n",
            "  inflating: train/Common wheat/7d9f34d96.png  \n",
            "  inflating: train/Common wheat/80a57b11f.png  \n",
            "  inflating: train/Common wheat/8211489bf.png  \n",
            "  inflating: train/Common wheat/828b49c19.png  \n",
            "  inflating: train/Common wheat/835da3f0f.png  \n",
            "  inflating: train/Common wheat/854146480.png  \n",
            "  inflating: train/Common wheat/861e7b81e.png  \n",
            "  inflating: train/Common wheat/872174e0f.png  \n",
            "  inflating: train/Common wheat/898f2827c.png  \n",
            "  inflating: train/Common wheat/8b6ab9181.png  \n",
            "  inflating: train/Common wheat/8d2bb104d.png  \n",
            "  inflating: train/Common wheat/8d664b2b6.png  \n",
            "  inflating: train/Common wheat/9026da493.png  \n",
            "  inflating: train/Common wheat/91f32c678.png  \n",
            "  inflating: train/Common wheat/926be1c1b.png  \n",
            "  inflating: train/Common wheat/93b577009.png  \n",
            "  inflating: train/Common wheat/94cb20fd9.png  \n",
            "  inflating: train/Common wheat/94cf3c22d.png  \n",
            "  inflating: train/Common wheat/9509c1ad7.png  \n",
            "  inflating: train/Common wheat/96183ee02.png  \n",
            "  inflating: train/Common wheat/975d39600.png  \n",
            "  inflating: train/Common wheat/98756045d.png  \n",
            "  inflating: train/Common wheat/990636602.png  \n",
            "  inflating: train/Common wheat/9933db289.png  \n",
            "  inflating: train/Common wheat/9b3ec2b81.png  \n",
            "  inflating: train/Common wheat/9bcfd14d6.png  \n",
            "  inflating: train/Common wheat/9c8cf72ed.png  \n",
            "  inflating: train/Common wheat/9d99ab3f3.png  \n",
            "  inflating: train/Common wheat/9dde1db08.png  \n",
            "  inflating: train/Common wheat/9eb9a2afa.png  \n",
            "  inflating: train/Common wheat/9efa077b7.png  \n",
            "  inflating: train/Common wheat/a04fbc0cd.png  \n",
            "  inflating: train/Common wheat/a1c72af26.png  \n",
            "  inflating: train/Common wheat/a1f2c5550.png  \n",
            "  inflating: train/Common wheat/a25f5ccb5.png  \n",
            "  inflating: train/Common wheat/a300eb8b2.png  \n",
            "  inflating: train/Common wheat/a40bd448a.png  \n",
            "  inflating: train/Common wheat/a413c7490.png  \n",
            "  inflating: train/Common wheat/a53419433.png  \n",
            "  inflating: train/Common wheat/a5c14c01a.png  \n",
            "  inflating: train/Common wheat/a65244218.png  \n",
            "  inflating: train/Common wheat/a6d21535a.png  \n",
            "  inflating: train/Common wheat/a6d688d93.png  \n",
            "  inflating: train/Common wheat/a8409b0a0.png  \n",
            "  inflating: train/Common wheat/a86689d83.png  \n",
            "  inflating: train/Common wheat/a9bda6650.png  \n",
            "  inflating: train/Common wheat/aa7edaf4c.png  \n",
            "  inflating: train/Common wheat/aabd29f32.png  \n",
            "  inflating: train/Common wheat/ab414e606.png  \n",
            "  inflating: train/Common wheat/ab8a5145f.png  \n",
            "  inflating: train/Common wheat/ac564c384.png  \n",
            "  inflating: train/Common wheat/ac8a31903.png  \n",
            "  inflating: train/Common wheat/aea5d6bb9.png  \n",
            "  inflating: train/Common wheat/b032c75ba.png  \n",
            "  inflating: train/Common wheat/b2dbd97fe.png  \n",
            "  inflating: train/Common wheat/b2f6f2618.png  \n",
            "  inflating: train/Common wheat/b3166a9b3.png  \n",
            "  inflating: train/Common wheat/b3817bd71.png  \n",
            "  inflating: train/Common wheat/b398cfd9b.png  \n",
            "  inflating: train/Common wheat/b6454a013.png  \n",
            "  inflating: train/Common wheat/b8525b55d.png  \n",
            "  inflating: train/Common wheat/bad392832.png  \n",
            "  inflating: train/Common wheat/be161df32.png  \n",
            "  inflating: train/Common wheat/c01e07e65.png  \n",
            "  inflating: train/Common wheat/c48b788a4.png  \n",
            "  inflating: train/Common wheat/c48e223f7.png  \n",
            "  inflating: train/Common wheat/c643a424f.png  \n",
            "  inflating: train/Common wheat/c6a4e9525.png  \n",
            "  inflating: train/Common wheat/c7b35625d.png  \n",
            "  inflating: train/Common wheat/c8af8bb05.png  \n",
            "  inflating: train/Common wheat/c9115289b.png  \n",
            "  inflating: train/Common wheat/c9562464a.png  \n",
            "  inflating: train/Common wheat/c97e74bd0.png  \n",
            "  inflating: train/Common wheat/ca5707e7b.png  \n",
            "  inflating: train/Common wheat/ca6c95ade.png  \n",
            "  inflating: train/Common wheat/cac003231.png  \n",
            "  inflating: train/Common wheat/caf1f8700.png  \n",
            "  inflating: train/Common wheat/cb0bc5c02.png  \n",
            "  inflating: train/Common wheat/ccc61dee9.png  \n",
            "  inflating: train/Common wheat/cf0197941.png  \n",
            "  inflating: train/Common wheat/d0237f972.png  \n",
            "  inflating: train/Common wheat/d068da45d.png  \n",
            "  inflating: train/Common wheat/d14f74215.png  \n",
            "  inflating: train/Common wheat/d174ffbad.png  \n",
            "  inflating: train/Common wheat/d1e4fc2b4.png  \n",
            "  inflating: train/Common wheat/d21231f1d.png  \n",
            "  inflating: train/Common wheat/d42042a90.png  \n",
            "  inflating: train/Common wheat/d95c8b624.png  \n",
            "  inflating: train/Common wheat/d9e0c6708.png  \n",
            "  inflating: train/Common wheat/d9e6014ea.png  \n",
            "  inflating: train/Common wheat/dd79d08bb.png  \n",
            "  inflating: train/Common wheat/df389ab82.png  \n",
            "  inflating: train/Common wheat/df584ca28.png  \n",
            "  inflating: train/Common wheat/e244e2544.png  \n",
            "  inflating: train/Common wheat/e475abfde.png  \n",
            "  inflating: train/Common wheat/e73947cd6.png  \n",
            "  inflating: train/Common wheat/eb08e6794.png  \n",
            "  inflating: train/Common wheat/ebdd5e41a.png  \n",
            "  inflating: train/Common wheat/ec0d252e7.png  \n",
            "  inflating: train/Common wheat/ed7f60848.png  \n",
            "  inflating: train/Common wheat/ee3810dd9.png  \n",
            "  inflating: train/Common wheat/eed3033a3.png  \n",
            "  inflating: train/Common wheat/f08107bad.png  \n",
            "  inflating: train/Common wheat/f28fbdf77.png  \n",
            "  inflating: train/Common wheat/f3743e142.png  \n",
            "  inflating: train/Common wheat/f48b02a74.png  \n",
            "  inflating: train/Common wheat/f77095d24.png  \n",
            "  inflating: train/Common wheat/f83183d25.png  \n",
            "  inflating: train/Common wheat/fc03ee09b.png  \n",
            "  inflating: train/Common wheat/fc387c3ba.png  \n",
            "  inflating: train/Common wheat/fc95f5ed2.png  \n",
            "  inflating: train/Common wheat/fd0e54fc5.png  \n",
            "  inflating: train/Common wheat/fe8affe51.png  \n",
            "  inflating: train/Fat Hen/00268e97d.png  \n",
            "  inflating: train/Fat Hen/008f1b92d.png  \n",
            "  inflating: train/Fat Hen/009004da3.png  \n",
            "  inflating: train/Fat Hen/01396e759.png  \n",
            "  inflating: train/Fat Hen/01437202b.png  \n",
            "  inflating: train/Fat Hen/0157c4199.png  \n",
            "  inflating: train/Fat Hen/020ac5b06.png  \n",
            "  inflating: train/Fat Hen/02a67d111.png  \n",
            "  inflating: train/Fat Hen/03100eb7e.png  \n",
            "  inflating: train/Fat Hen/038850f48.png  \n",
            "  inflating: train/Fat Hen/0486762e1.png  \n",
            "  inflating: train/Fat Hen/066395555.png  \n",
            "  inflating: train/Fat Hen/075cb6666.png  \n",
            "  inflating: train/Fat Hen/07636139b.png  \n",
            "  inflating: train/Fat Hen/077190c7a.png  \n",
            "  inflating: train/Fat Hen/07d939b1e.png  \n",
            "  inflating: train/Fat Hen/08fe5538d.png  \n",
            "  inflating: train/Fat Hen/0a1480ed8.png  \n",
            "  inflating: train/Fat Hen/0a4ab470e.png  \n",
            "  inflating: train/Fat Hen/0c03386a9.png  \n",
            "  inflating: train/Fat Hen/0c5368b03.png  \n",
            "  inflating: train/Fat Hen/0d5ece722.png  \n",
            "  inflating: train/Fat Hen/0dfbbc7fe.png  \n",
            "  inflating: train/Fat Hen/0e4df9f15.png  \n",
            "  inflating: train/Fat Hen/0eeb0c7c1.png  \n",
            "  inflating: train/Fat Hen/0f335c7a7.png  \n",
            "  inflating: train/Fat Hen/100ef76fb.png  \n",
            "  inflating: train/Fat Hen/1012b652c.png  \n",
            "  inflating: train/Fat Hen/10553b4e8.png  \n",
            "  inflating: train/Fat Hen/109e25d2a.png  \n",
            "  inflating: train/Fat Hen/10a61c74f.png  \n",
            "  inflating: train/Fat Hen/11e284489.png  \n",
            "  inflating: train/Fat Hen/11e40532a.png  \n",
            "  inflating: train/Fat Hen/1266ac5cb.png  \n",
            "  inflating: train/Fat Hen/134db7c0e.png  \n",
            "  inflating: train/Fat Hen/13d56e652.png  \n",
            "  inflating: train/Fat Hen/143e09ce6.png  \n",
            "  inflating: train/Fat Hen/149dbba5f.png  \n",
            "  inflating: train/Fat Hen/14f7687d1.png  \n",
            "  inflating: train/Fat Hen/158ac618f.png  \n",
            "  inflating: train/Fat Hen/160dacec0.png  \n",
            "  inflating: train/Fat Hen/165407507.png  \n",
            "  inflating: train/Fat Hen/1688c9a5f.png  \n",
            "  inflating: train/Fat Hen/1797f5460.png  \n",
            "  inflating: train/Fat Hen/180f41d53.png  \n",
            "  inflating: train/Fat Hen/184f64b26.png  \n",
            "  inflating: train/Fat Hen/190b948d5.png  \n",
            "  inflating: train/Fat Hen/193f4c946.png  \n",
            "  inflating: train/Fat Hen/19bd579f5.png  \n",
            "  inflating: train/Fat Hen/19c84bc7c.png  \n",
            "  inflating: train/Fat Hen/19fc23481.png  \n",
            "  inflating: train/Fat Hen/1a5354a5b.png  \n",
            "  inflating: train/Fat Hen/1bcdf1f95.png  \n",
            "  inflating: train/Fat Hen/1bdf63082.png  \n",
            "  inflating: train/Fat Hen/1c4f59d14.png  \n",
            "  inflating: train/Fat Hen/1d9743b35.png  \n",
            "  inflating: train/Fat Hen/1df9e7301.png  \n",
            "  inflating: train/Fat Hen/1e9c7a82f.png  \n",
            "  inflating: train/Fat Hen/1eb0d7c5f.png  \n",
            "  inflating: train/Fat Hen/1fb586773.png  \n",
            "  inflating: train/Fat Hen/207d4e9ce.png  \n",
            "  inflating: train/Fat Hen/211c3f24f.png  \n",
            "  inflating: train/Fat Hen/214f2dbcb.png  \n",
            "  inflating: train/Fat Hen/2160ec61c.png  \n",
            "  inflating: train/Fat Hen/216b1daa0.png  \n",
            "  inflating: train/Fat Hen/22cd06b64.png  \n",
            "  inflating: train/Fat Hen/22d4656ff.png  \n",
            "  inflating: train/Fat Hen/23525c578.png  \n",
            "  inflating: train/Fat Hen/23a4cbe43.png  \n",
            "  inflating: train/Fat Hen/2412cb19b.png  \n",
            "  inflating: train/Fat Hen/2509e5bc5.png  \n",
            "  inflating: train/Fat Hen/25ff4825d.png  \n",
            "  inflating: train/Fat Hen/26220c5b7.png  \n",
            "  inflating: train/Fat Hen/268e72cee.png  \n",
            "  inflating: train/Fat Hen/2719ff172.png  \n",
            "  inflating: train/Fat Hen/2760cb984.png  \n",
            "  inflating: train/Fat Hen/27d310b2a.png  \n",
            "  inflating: train/Fat Hen/28fad5a34.png  \n",
            "  inflating: train/Fat Hen/2918206b5.png  \n",
            "  inflating: train/Fat Hen/295ddd0b8.png  \n",
            "  inflating: train/Fat Hen/2a56e7fec.png  \n",
            "  inflating: train/Fat Hen/2b16fb022.png  \n",
            "  inflating: train/Fat Hen/2b17dc2ba.png  \n",
            "  inflating: train/Fat Hen/2c25bee0b.png  \n",
            "  inflating: train/Fat Hen/2cab73c25.png  \n",
            "  inflating: train/Fat Hen/2d34a64af.png  \n",
            "  inflating: train/Fat Hen/2d6ffd537.png  \n",
            "  inflating: train/Fat Hen/2dea1d8a6.png  \n",
            "  inflating: train/Fat Hen/2df7f8513.png  \n",
            "  inflating: train/Fat Hen/2eb6d7f8e.png  \n",
            "  inflating: train/Fat Hen/2f1633635.png  \n",
            "  inflating: train/Fat Hen/2f80532a2.png  \n",
            "  inflating: train/Fat Hen/302a3a8ba.png  \n",
            "  inflating: train/Fat Hen/302f8959d.png  \n",
            "  inflating: train/Fat Hen/326c83b87.png  \n",
            "  inflating: train/Fat Hen/33c6627d2.png  \n",
            "  inflating: train/Fat Hen/34cec646b.png  \n",
            "  inflating: train/Fat Hen/34eb21afb.png  \n",
            "  inflating: train/Fat Hen/35083f3c2.png  \n",
            "  inflating: train/Fat Hen/35655c594.png  \n",
            "  inflating: train/Fat Hen/3572c1951.png  \n",
            "  inflating: train/Fat Hen/35ae5f914.png  \n",
            "  inflating: train/Fat Hen/370af1cdd.png  \n",
            "  inflating: train/Fat Hen/370b70617.png  \n",
            "  inflating: train/Fat Hen/38cc62ed1.png  \n",
            "  inflating: train/Fat Hen/3ac434ccd.png  \n",
            "  inflating: train/Fat Hen/3ae60f149.png  \n",
            "  inflating: train/Fat Hen/3b244dc9d.png  \n",
            "  inflating: train/Fat Hen/3b6a97588.png  \n",
            "  inflating: train/Fat Hen/3c18b504d.png  \n",
            "  inflating: train/Fat Hen/3c2e5c7ad.png  \n",
            "  inflating: train/Fat Hen/3c51bc884.png  \n",
            "  inflating: train/Fat Hen/3c5e42c9a.png  \n",
            "  inflating: train/Fat Hen/3cafef989.png  \n",
            "  inflating: train/Fat Hen/3d07c273b.png  \n",
            "  inflating: train/Fat Hen/3d2f3ad0f.png  \n",
            "  inflating: train/Fat Hen/3dbdd9fe7.png  \n",
            "  inflating: train/Fat Hen/3fe21a3d4.png  \n",
            "  inflating: train/Fat Hen/3ffc2f66d.png  \n",
            "  inflating: train/Fat Hen/40321aa58.png  \n",
            "  inflating: train/Fat Hen/4058dd2cd.png  \n",
            "  inflating: train/Fat Hen/40721b002.png  \n",
            "  inflating: train/Fat Hen/420e17575.png  \n",
            "  inflating: train/Fat Hen/427bbd2fb.png  \n",
            "  inflating: train/Fat Hen/42912adf2.png  \n",
            "  inflating: train/Fat Hen/43633755b.png  \n",
            "  inflating: train/Fat Hen/4374a9eef.png  \n",
            "  inflating: train/Fat Hen/44db98326.png  \n",
            "  inflating: train/Fat Hen/4570becfb.png  \n",
            "  inflating: train/Fat Hen/45c197012.png  \n",
            "  inflating: train/Fat Hen/46fa84dad.png  \n",
            "  inflating: train/Fat Hen/496ca82ae.png  \n",
            "  inflating: train/Fat Hen/4979cba5a.png  \n",
            "  inflating: train/Fat Hen/4a320f8be.png  \n",
            "  inflating: train/Fat Hen/4a44a00eb.png  \n",
            "  inflating: train/Fat Hen/4b5f2e2a0.png  \n",
            "  inflating: train/Fat Hen/4c832853a.png  \n",
            "  inflating: train/Fat Hen/4c835827d.png  \n",
            "  inflating: train/Fat Hen/4d2efbb45.png  \n",
            "  inflating: train/Fat Hen/4d7fd4020.png  \n",
            "  inflating: train/Fat Hen/4e2f543a2.png  \n",
            "  inflating: train/Fat Hen/4f0092891.png  \n",
            "  inflating: train/Fat Hen/4f1566bb4.png  \n",
            "  inflating: train/Fat Hen/4f2f35ea4.png  \n",
            "  inflating: train/Fat Hen/510f3ba19.png  \n",
            "  inflating: train/Fat Hen/524942b65.png  \n",
            "  inflating: train/Fat Hen/5281f6020.png  \n",
            "  inflating: train/Fat Hen/5298e390e.png  \n",
            "  inflating: train/Fat Hen/52bcf3283.png  \n",
            "  inflating: train/Fat Hen/52c144a42.png  \n",
            "  inflating: train/Fat Hen/52e6df585.png  \n",
            "  inflating: train/Fat Hen/52e82d773.png  \n",
            "  inflating: train/Fat Hen/5357849ba.png  \n",
            "  inflating: train/Fat Hen/53c6c6221.png  \n",
            "  inflating: train/Fat Hen/55fef4910.png  \n",
            "  inflating: train/Fat Hen/565d13821.png  \n",
            "  inflating: train/Fat Hen/56d88c09d.png  \n",
            "  inflating: train/Fat Hen/56ee7f90f.png  \n",
            "  inflating: train/Fat Hen/57914613f.png  \n",
            "  inflating: train/Fat Hen/58028a2fb.png  \n",
            "  inflating: train/Fat Hen/59b1f956d.png  \n",
            "  inflating: train/Fat Hen/5a1183fff.png  \n",
            "  inflating: train/Fat Hen/5a4dfea94.png  \n",
            "  inflating: train/Fat Hen/5aa1ee0b1.png  \n",
            "  inflating: train/Fat Hen/5aae43c81.png  \n",
            "  inflating: train/Fat Hen/5bdfa3a44.png  \n",
            "  inflating: train/Fat Hen/5c2f52e61.png  \n",
            "  inflating: train/Fat Hen/5c3a4a6ed.png  \n",
            "  inflating: train/Fat Hen/5ca463797.png  \n",
            "  inflating: train/Fat Hen/5cc2862dc.png  \n",
            "  inflating: train/Fat Hen/5d9dd8926.png  \n",
            "  inflating: train/Fat Hen/5dc10869f.png  \n",
            "  inflating: train/Fat Hen/5e2cfbee3.png  \n",
            "  inflating: train/Fat Hen/5e72c6e8f.png  \n",
            "  inflating: train/Fat Hen/5f1fd3c70.png  \n",
            "  inflating: train/Fat Hen/5f24e1079.png  \n",
            "  inflating: train/Fat Hen/5f2ad45c4.png  \n",
            "  inflating: train/Fat Hen/5f5015641.png  \n",
            "  inflating: train/Fat Hen/5f6634fa4.png  \n",
            "  inflating: train/Fat Hen/5f6ad629d.png  \n",
            "  inflating: train/Fat Hen/60f67d18f.png  \n",
            "  inflating: train/Fat Hen/614b0792a.png  \n",
            "  inflating: train/Fat Hen/61d4251ca.png  \n",
            "  inflating: train/Fat Hen/61f732692.png  \n",
            "  inflating: train/Fat Hen/61fd68900.png  \n",
            "  inflating: train/Fat Hen/629185134.png  \n",
            "  inflating: train/Fat Hen/62f3abbd1.png  \n",
            "  inflating: train/Fat Hen/64034603a.png  \n",
            "  inflating: train/Fat Hen/64f46676b.png  \n",
            "  inflating: train/Fat Hen/655471b48.png  \n",
            "  inflating: train/Fat Hen/65d2f9cc0.png  \n",
            "  inflating: train/Fat Hen/661a6866f.png  \n",
            "  inflating: train/Fat Hen/6644e24c8.png  \n",
            "  inflating: train/Fat Hen/67659c7df.png  \n",
            "  inflating: train/Fat Hen/67d48e1b4.png  \n",
            "  inflating: train/Fat Hen/67e5eeea7.png  \n",
            "  inflating: train/Fat Hen/685091401.png  \n",
            "  inflating: train/Fat Hen/6ac7f6411.png  \n",
            "  inflating: train/Fat Hen/6b175ade7.png  \n",
            "  inflating: train/Fat Hen/6b6061001.png  \n",
            "  inflating: train/Fat Hen/6cfecf7c5.png  \n",
            "  inflating: train/Fat Hen/6d9dafb55.png  \n",
            "  inflating: train/Fat Hen/6dc0cf03c.png  \n",
            "  inflating: train/Fat Hen/6e54ab8b9.png  \n",
            "  inflating: train/Fat Hen/6e6b9a20f.png  \n",
            "  inflating: train/Fat Hen/6e735b605.png  \n",
            "  inflating: train/Fat Hen/6f099ece0.png  \n",
            "  inflating: train/Fat Hen/6f7c00476.png  \n",
            "  inflating: train/Fat Hen/7026b68e9.png  \n",
            "  inflating: train/Fat Hen/704b0fe2c.png  \n",
            "  inflating: train/Fat Hen/7085a144b.png  \n",
            "  inflating: train/Fat Hen/70f84707a.png  \n",
            "  inflating: train/Fat Hen/7195021a5.png  \n",
            "  inflating: train/Fat Hen/72103b235.png  \n",
            "  inflating: train/Fat Hen/73600b4ed.png  \n",
            "  inflating: train/Fat Hen/75739ff4d.png  \n",
            "  inflating: train/Fat Hen/7637818c7.png  \n",
            "  inflating: train/Fat Hen/765502bd2.png  \n",
            "  inflating: train/Fat Hen/7678bf0c5.png  \n",
            "  inflating: train/Fat Hen/77fcd0e5d.png  \n",
            "  inflating: train/Fat Hen/786319e1b.png  \n",
            "  inflating: train/Fat Hen/7894abcbb.png  \n",
            "  inflating: train/Fat Hen/79933186c.png  \n",
            "  inflating: train/Fat Hen/79cec7209.png  \n",
            "  inflating: train/Fat Hen/7abe4427d.png  \n",
            "  inflating: train/Fat Hen/7b0bd3d8d.png  \n",
            "  inflating: train/Fat Hen/7bb3a73ca.png  \n",
            "  inflating: train/Fat Hen/7ce630c2a.png  \n",
            "  inflating: train/Fat Hen/7e4b6625f.png  \n",
            "  inflating: train/Fat Hen/7e98e4aa6.png  \n",
            "  inflating: train/Fat Hen/7f731311e.png  \n",
            "  inflating: train/Fat Hen/803474779.png  \n",
            "  inflating: train/Fat Hen/817eefb90.png  \n",
            "  inflating: train/Fat Hen/81af6efb2.png  \n",
            "  inflating: train/Fat Hen/81db6f8a2.png  \n",
            "  inflating: train/Fat Hen/83be0f22d.png  \n",
            "  inflating: train/Fat Hen/83f907552.png  \n",
            "  inflating: train/Fat Hen/840018e57.png  \n",
            "  inflating: train/Fat Hen/84097b0b7.png  \n",
            "  inflating: train/Fat Hen/842f5555e.png  \n",
            "  inflating: train/Fat Hen/854fdbe6a.png  \n",
            "  inflating: train/Fat Hen/85a848757.png  \n",
            "  inflating: train/Fat Hen/85e6fa66d.png  \n",
            "  inflating: train/Fat Hen/8611e1c13.png  \n",
            "  inflating: train/Fat Hen/86536f469.png  \n",
            "  inflating: train/Fat Hen/8689b3765.png  \n",
            "  inflating: train/Fat Hen/886db2427.png  \n",
            "  inflating: train/Fat Hen/88e4077c5.png  \n",
            "  inflating: train/Fat Hen/88ea1ed2a.png  \n",
            "  inflating: train/Fat Hen/8a748cf8b.png  \n",
            "  inflating: train/Fat Hen/8a8b1f0fd.png  \n",
            "  inflating: train/Fat Hen/8b3943e1e.png  \n",
            "  inflating: train/Fat Hen/8cb53bccc.png  \n",
            "  inflating: train/Fat Hen/8e0c81139.png  \n",
            "  inflating: train/Fat Hen/8eed146cd.png  \n",
            "  inflating: train/Fat Hen/8f56f8b0e.png  \n",
            "  inflating: train/Fat Hen/8f6b56be9.png  \n",
            "  inflating: train/Fat Hen/9064640e8.png  \n",
            "  inflating: train/Fat Hen/9168bde0c.png  \n",
            "  inflating: train/Fat Hen/91a5d8724.png  \n",
            "  inflating: train/Fat Hen/91bc7d91c.png  \n",
            "  inflating: train/Fat Hen/91c33cc1c.png  \n",
            "  inflating: train/Fat Hen/91e05c100.png  \n",
            "  inflating: train/Fat Hen/9284ed9c9.png  \n",
            "  inflating: train/Fat Hen/92954f77f.png  \n",
            "  inflating: train/Fat Hen/9297b85f6.png  \n",
            "  inflating: train/Fat Hen/946b48f62.png  \n",
            "  inflating: train/Fat Hen/94ff7314b.png  \n",
            "  inflating: train/Fat Hen/952f0a5fa.png  \n",
            "  inflating: train/Fat Hen/95f035f39.png  \n",
            "  inflating: train/Fat Hen/9708f9c0e.png  \n",
            "  inflating: train/Fat Hen/9812f690c.png  \n",
            "  inflating: train/Fat Hen/98cf53a24.png  \n",
            "  inflating: train/Fat Hen/994001cab.png  \n",
            "  inflating: train/Fat Hen/9961b2091.png  \n",
            "  inflating: train/Fat Hen/997fc29b0.png  \n",
            "  inflating: train/Fat Hen/9a8531ba0.png  \n",
            "  inflating: train/Fat Hen/9d25b73be.png  \n",
            "  inflating: train/Fat Hen/9d2684128.png  \n",
            "  inflating: train/Fat Hen/9d4722e74.png  \n",
            "  inflating: train/Fat Hen/9dc631eaa.png  \n",
            "  inflating: train/Fat Hen/9e59fcd81.png  \n",
            "  inflating: train/Fat Hen/9e8727146.png  \n",
            "  inflating: train/Fat Hen/9f2435711.png  \n",
            "  inflating: train/Fat Hen/9f4788348.png  \n",
            "  inflating: train/Fat Hen/a02a0c284.png  \n",
            "  inflating: train/Fat Hen/a140930c0.png  \n",
            "  inflating: train/Fat Hen/a1971cff3.png  \n",
            "  inflating: train/Fat Hen/a1a3e2515.png  \n",
            "  inflating: train/Fat Hen/a258e82f2.png  \n",
            "  inflating: train/Fat Hen/a309725cd.png  \n",
            "  inflating: train/Fat Hen/a3e1e7ffd.png  \n",
            "  inflating: train/Fat Hen/a4d7cddff.png  \n",
            "  inflating: train/Fat Hen/a57f8af28.png  \n",
            "  inflating: train/Fat Hen/a5aa3564a.png  \n",
            "  inflating: train/Fat Hen/a5c5b9386.png  \n",
            "  inflating: train/Fat Hen/a5ca3b845.png  \n",
            "  inflating: train/Fat Hen/a74864d83.png  \n",
            "  inflating: train/Fat Hen/a7547b978.png  \n",
            "  inflating: train/Fat Hen/a7d5da828.png  \n",
            "  inflating: train/Fat Hen/a7e5ce252.png  \n",
            "  inflating: train/Fat Hen/a7fef768d.png  \n",
            "  inflating: train/Fat Hen/a86b9c0cd.png  \n",
            "  inflating: train/Fat Hen/aa0f9c454.png  \n",
            "  inflating: train/Fat Hen/aa4c94174.png  \n",
            "  inflating: train/Fat Hen/aaf5833f8.png  \n",
            "  inflating: train/Fat Hen/ac502b28e.png  \n",
            "  inflating: train/Fat Hen/ac50d45a7.png  \n",
            "  inflating: train/Fat Hen/ac6625b0f.png  \n",
            "  inflating: train/Fat Hen/ac88a6662.png  \n",
            "  inflating: train/Fat Hen/ad58587fb.png  \n",
            "  inflating: train/Fat Hen/ad5ee2832.png  \n",
            "  inflating: train/Fat Hen/afac20b21.png  \n",
            "  inflating: train/Fat Hen/b0398be3d.png  \n",
            "  inflating: train/Fat Hen/b039fd244.png  \n",
            "  inflating: train/Fat Hen/b07391323.png  \n",
            "  inflating: train/Fat Hen/b2f74bf81.png  \n",
            "  inflating: train/Fat Hen/b3e1dda94.png  \n",
            "  inflating: train/Fat Hen/b43d0a350.png  \n",
            "  inflating: train/Fat Hen/b4e6ee3d2.png  \n",
            "  inflating: train/Fat Hen/b4e738f6b.png  \n",
            "  inflating: train/Fat Hen/b4fd78671.png  \n",
            "  inflating: train/Fat Hen/b59aec1e1.png  \n",
            "  inflating: train/Fat Hen/b88da8e21.png  \n",
            "  inflating: train/Fat Hen/b89d9f403.png  \n",
            "  inflating: train/Fat Hen/b8e176ae8.png  \n",
            "  inflating: train/Fat Hen/b8e3dd9ae.png  \n",
            "  inflating: train/Fat Hen/ba5c8f87f.png  \n",
            "  inflating: train/Fat Hen/ba9dc6742.png  \n",
            "  inflating: train/Fat Hen/bbcb520e8.png  \n",
            "  inflating: train/Fat Hen/bbcc5bbb2.png  \n",
            "  inflating: train/Fat Hen/bbee46239.png  \n",
            "  inflating: train/Fat Hen/bc10332f0.png  \n",
            "  inflating: train/Fat Hen/bcb9e211a.png  \n",
            "  inflating: train/Fat Hen/bcbbdd3bd.png  \n",
            "  inflating: train/Fat Hen/bd3b1cb2e.png  \n",
            "  inflating: train/Fat Hen/bd4003440.png  \n",
            "  inflating: train/Fat Hen/bd70765e4.png  \n",
            "  inflating: train/Fat Hen/bd8a05057.png  \n",
            "  inflating: train/Fat Hen/bdc98dfaf.png  \n",
            "  inflating: train/Fat Hen/bdf58ae69.png  \n",
            "  inflating: train/Fat Hen/be71f0cc0.png  \n",
            "  inflating: train/Fat Hen/beafe5bb1.png  \n",
            "  inflating: train/Fat Hen/beb11008f.png  \n",
            "  inflating: train/Fat Hen/bf17c8991.png  \n",
            "  inflating: train/Fat Hen/bf37d581c.png  \n",
            "  inflating: train/Fat Hen/bfaab608c.png  \n",
            "  inflating: train/Fat Hen/bfdeafadd.png  \n",
            "  inflating: train/Fat Hen/c068b9fb7.png  \n",
            "  inflating: train/Fat Hen/c1350e0ae.png  \n",
            "  inflating: train/Fat Hen/c17447438.png  \n",
            "  inflating: train/Fat Hen/c1d528ca2.png  \n",
            "  inflating: train/Fat Hen/c206f4602.png  \n",
            "  inflating: train/Fat Hen/c315f4c96.png  \n",
            "  inflating: train/Fat Hen/c354695d7.png  \n",
            "  inflating: train/Fat Hen/c386194a6.png  \n",
            "  inflating: train/Fat Hen/c3a35fbde.png  \n",
            "  inflating: train/Fat Hen/c3aa41593.png  \n",
            "  inflating: train/Fat Hen/c4152caea.png  \n",
            "  inflating: train/Fat Hen/c41bd998c.png  \n",
            "  inflating: train/Fat Hen/c44243e23.png  \n",
            "  inflating: train/Fat Hen/c4b7cf7f5.png  \n",
            "  inflating: train/Fat Hen/c5a0bd47e.png  \n",
            "  inflating: train/Fat Hen/c679a9396.png  \n",
            "  inflating: train/Fat Hen/c734bade3.png  \n",
            "  inflating: train/Fat Hen/c740aa55a.png  \n",
            "  inflating: train/Fat Hen/c75a48496.png  \n",
            "  inflating: train/Fat Hen/c8ce795f0.png  \n",
            "  inflating: train/Fat Hen/c8e77995b.png  \n",
            "  inflating: train/Fat Hen/c926ade1d.png  \n",
            "  inflating: train/Fat Hen/c96ca4b01.png  \n",
            "  inflating: train/Fat Hen/c99df077e.png  \n",
            "  inflating: train/Fat Hen/c9e6722d4.png  \n",
            "  inflating: train/Fat Hen/caca90aca.png  \n",
            "  inflating: train/Fat Hen/cb698f0ea.png  \n",
            "  inflating: train/Fat Hen/cbdb42238.png  \n",
            "  inflating: train/Fat Hen/cbf4c5ad2.png  \n",
            "  inflating: train/Fat Hen/cc40c32ed.png  \n",
            "  inflating: train/Fat Hen/cc590cddc.png  \n",
            "  inflating: train/Fat Hen/cca9e1864.png  \n",
            "  inflating: train/Fat Hen/ccb763490.png  \n",
            "  inflating: train/Fat Hen/ccc42874e.png  \n",
            "  inflating: train/Fat Hen/cd16fe7e4.png  \n",
            "  inflating: train/Fat Hen/ce0d8c3b2.png  \n",
            "  inflating: train/Fat Hen/ce49ecca3.png  \n",
            "  inflating: train/Fat Hen/cf149b185.png  \n",
            "  inflating: train/Fat Hen/d09f9b311.png  \n",
            "  inflating: train/Fat Hen/d0c3b7610.png  \n",
            "  inflating: train/Fat Hen/d10feb105.png  \n",
            "  inflating: train/Fat Hen/d199b75a6.png  \n",
            "  inflating: train/Fat Hen/d1d6c2904.png  \n",
            "  inflating: train/Fat Hen/d210b0125.png  \n",
            "  inflating: train/Fat Hen/d233d03bb.png  \n",
            "  inflating: train/Fat Hen/d2434e784.png  \n",
            "  inflating: train/Fat Hen/d2e16e332.png  \n",
            "  inflating: train/Fat Hen/d3e5f8c5e.png  \n",
            "  inflating: train/Fat Hen/d402cc5ca.png  \n",
            "  inflating: train/Fat Hen/d47349f62.png  \n",
            "  inflating: train/Fat Hen/d569925d7.png  \n",
            "  inflating: train/Fat Hen/d614cd719.png  \n",
            "  inflating: train/Fat Hen/d6576bc79.png  \n",
            "  inflating: train/Fat Hen/d674d9e99.png  \n",
            "  inflating: train/Fat Hen/d6a104d24.png  \n",
            "  inflating: train/Fat Hen/d870ecd2c.png  \n",
            "  inflating: train/Fat Hen/d87eb38df.png  \n",
            "  inflating: train/Fat Hen/d99fa0e0b.png  \n",
            "  inflating: train/Fat Hen/db673dc46.png  \n",
            "  inflating: train/Fat Hen/dbb008ffc.png  \n",
            "  inflating: train/Fat Hen/df17b3ba5.png  \n",
            "  inflating: train/Fat Hen/df62f1c7d.png  \n",
            "  inflating: train/Fat Hen/df9ce5e1f.png  \n",
            "  inflating: train/Fat Hen/dff210db9.png  \n",
            "  inflating: train/Fat Hen/e00ed0e9e.png  \n",
            "  inflating: train/Fat Hen/e0308226e.png  \n",
            "  inflating: train/Fat Hen/e0cab89f3.png  \n",
            "  inflating: train/Fat Hen/e1783ecc3.png  \n",
            "  inflating: train/Fat Hen/e2462573b.png  \n",
            "  inflating: train/Fat Hen/e27bb5522.png  \n",
            "  inflating: train/Fat Hen/e3279d465.png  \n",
            "  inflating: train/Fat Hen/e34aec452.png  \n",
            "  inflating: train/Fat Hen/e4541fd1c.png  \n",
            "  inflating: train/Fat Hen/e5e1416a2.png  \n",
            "  inflating: train/Fat Hen/e5e1f6d81.png  \n",
            "  inflating: train/Fat Hen/e6b756e98.png  \n",
            "  inflating: train/Fat Hen/e6befd0af.png  \n",
            "  inflating: train/Fat Hen/e74f35d5d.png  \n",
            "  inflating: train/Fat Hen/e777e1303.png  \n",
            "  inflating: train/Fat Hen/e7d278e0e.png  \n",
            "  inflating: train/Fat Hen/e7fbf693d.png  \n",
            "  inflating: train/Fat Hen/e805aa0a2.png  \n",
            "  inflating: train/Fat Hen/e8c85738d.png  \n",
            "  inflating: train/Fat Hen/e973ac6bc.png  \n",
            "  inflating: train/Fat Hen/e9f00ae2c.png  \n",
            "  inflating: train/Fat Hen/ea239a8cd.png  \n",
            "  inflating: train/Fat Hen/ea5ac32c1.png  \n",
            "  inflating: train/Fat Hen/ea6e91be6.png  \n",
            "  inflating: train/Fat Hen/ea775ab85.png  \n",
            "  inflating: train/Fat Hen/ea7e6ec7f.png  \n",
            "  inflating: train/Fat Hen/eace614b4.png  \n",
            "  inflating: train/Fat Hen/eb6811993.png  \n",
            "  inflating: train/Fat Hen/ec7cde707.png  \n",
            "  inflating: train/Fat Hen/ec81b8667.png  \n",
            "  inflating: train/Fat Hen/ed22ec740.png  \n",
            "  inflating: train/Fat Hen/ef1303b4a.png  \n",
            "  inflating: train/Fat Hen/ef1f257d9.png  \n",
            "  inflating: train/Fat Hen/efaab2370.png  \n",
            "  inflating: train/Fat Hen/f052e1db1.png  \n",
            "  inflating: train/Fat Hen/f0923dcd6.png  \n",
            "  inflating: train/Fat Hen/f09c51a67.png  \n",
            "  inflating: train/Fat Hen/f0dbc5e1e.png  \n",
            "  inflating: train/Fat Hen/f0e75e3bf.png  \n",
            "  inflating: train/Fat Hen/f180203df.png  \n",
            "  inflating: train/Fat Hen/f18419d0c.png  \n",
            "  inflating: train/Fat Hen/f1b52dd6f.png  \n",
            "  inflating: train/Fat Hen/f2b9bb477.png  \n",
            "  inflating: train/Fat Hen/f33cee49e.png  \n",
            "  inflating: train/Fat Hen/f348e35e9.png  \n",
            "  inflating: train/Fat Hen/f36b25868.png  \n",
            "  inflating: train/Fat Hen/f3f6559bf.png  \n",
            "  inflating: train/Fat Hen/f44059ad0.png  \n",
            "  inflating: train/Fat Hen/f4cdd6b9e.png  \n",
            "  inflating: train/Fat Hen/f59177ec7.png  \n",
            "  inflating: train/Fat Hen/f605d4fb8.png  \n",
            "  inflating: train/Fat Hen/f86c14eaf.png  \n",
            "  inflating: train/Fat Hen/fa57ed595.png  \n",
            "  inflating: train/Fat Hen/fbd8e91d3.png  \n",
            "  inflating: train/Fat Hen/fbd995178.png  \n",
            "  inflating: train/Fat Hen/fdc1eedc6.png  \n",
            "  inflating: train/Fat Hen/fdfb296da.png  \n",
            "  inflating: train/Fat Hen/fe23b8530.png  \n",
            "  inflating: train/Fat Hen/fe2ad3d8c.png  \n",
            "  inflating: train/Fat Hen/fea20eec7.png  \n",
            "  inflating: train/Fat Hen/fee90e281.png  \n",
            "  inflating: train/Fat Hen/ff202577d.png  \n",
            "  inflating: train/Fat Hen/ff9f29145.png  \n",
            "  inflating: train/Loose Silky-bent/0012f11c4.png  \n",
            "  inflating: train/Loose Silky-bent/00442de98.png  \n",
            "  inflating: train/Loose Silky-bent/006eac3d9.png  \n",
            "  inflating: train/Loose Silky-bent/009d8d799.png  \n",
            "  inflating: train/Loose Silky-bent/0140dfc02.png  \n",
            "  inflating: train/Loose Silky-bent/01897adc9.png  \n",
            "  inflating: train/Loose Silky-bent/01ad79c2e.png  \n",
            "  inflating: train/Loose Silky-bent/02b15993e.png  \n",
            "  inflating: train/Loose Silky-bent/0367e0199.png  \n",
            "  inflating: train/Loose Silky-bent/04a16ff3b.png  \n",
            "  inflating: train/Loose Silky-bent/05067feb7.png  \n",
            "  inflating: train/Loose Silky-bent/054ed75f8.png  \n",
            "  inflating: train/Loose Silky-bent/065757bb0.png  \n",
            "  inflating: train/Loose Silky-bent/066868de0.png  \n",
            "  inflating: train/Loose Silky-bent/06bbb09fb.png  \n",
            "  inflating: train/Loose Silky-bent/07ca815ef.png  \n",
            "  inflating: train/Loose Silky-bent/080cea66f.png  \n",
            "  inflating: train/Loose Silky-bent/0822463cb.png  \n",
            "  inflating: train/Loose Silky-bent/0865fb4d3.png  \n",
            "  inflating: train/Loose Silky-bent/08fa31aa6.png  \n",
            "  inflating: train/Loose Silky-bent/0962f70a2.png  \n",
            "  inflating: train/Loose Silky-bent/09f47b868.png  \n",
            "  inflating: train/Loose Silky-bent/0a7723f3b.png  \n",
            "  inflating: train/Loose Silky-bent/0b1651517.png  \n",
            "  inflating: train/Loose Silky-bent/0bc960ef0.png  \n",
            "  inflating: train/Loose Silky-bent/0bcf22873.png  \n",
            "  inflating: train/Loose Silky-bent/0c27150b6.png  \n",
            "  inflating: train/Loose Silky-bent/0cc00364e.png  \n",
            "  inflating: train/Loose Silky-bent/0cf9eb4ab.png  \n",
            "  inflating: train/Loose Silky-bent/0d96cc35d.png  \n",
            "  inflating: train/Loose Silky-bent/0db2f82ee.png  \n",
            "  inflating: train/Loose Silky-bent/0e2983c97.png  \n",
            "  inflating: train/Loose Silky-bent/0f47f8a00.png  \n",
            "  inflating: train/Loose Silky-bent/0f96904f8.png  \n",
            "  inflating: train/Loose Silky-bent/0f9d5c657.png  \n",
            "  inflating: train/Loose Silky-bent/10250b3ac.png  \n",
            "  inflating: train/Loose Silky-bent/107afc1c0.png  \n",
            "  inflating: train/Loose Silky-bent/113b4a298.png  \n",
            "  inflating: train/Loose Silky-bent/1266e39c7.png  \n",
            "  inflating: train/Loose Silky-bent/13c51b503.png  \n",
            "  inflating: train/Loose Silky-bent/14469febc.png  \n",
            "  inflating: train/Loose Silky-bent/14a9e3564.png  \n",
            "  inflating: train/Loose Silky-bent/14bea63ea.png  \n",
            "  inflating: train/Loose Silky-bent/15b1ea490.png  \n",
            "  inflating: train/Loose Silky-bent/15b5a4c2c.png  \n",
            "  inflating: train/Loose Silky-bent/15fa7044b.png  \n",
            "  inflating: train/Loose Silky-bent/1656d2df4.png  \n",
            "  inflating: train/Loose Silky-bent/16ca17775.png  \n",
            "  inflating: train/Loose Silky-bent/17309f0c2.png  \n",
            "  inflating: train/Loose Silky-bent/176578292.png  \n",
            "  inflating: train/Loose Silky-bent/178b92275.png  \n",
            "  inflating: train/Loose Silky-bent/17b2de877.png  \n",
            "  inflating: train/Loose Silky-bent/17d3e7e2c.png  \n",
            "  inflating: train/Loose Silky-bent/1804ef033.png  \n",
            "  inflating: train/Loose Silky-bent/180a7bc07.png  \n",
            "  inflating: train/Loose Silky-bent/181fb1703.png  \n",
            "  inflating: train/Loose Silky-bent/19c867cbf.png  \n",
            "  inflating: train/Loose Silky-bent/1a6a98258.png  \n",
            "  inflating: train/Loose Silky-bent/1b4404d36.png  \n",
            "  inflating: train/Loose Silky-bent/1c0f5cdf6.png  \n",
            "  inflating: train/Loose Silky-bent/1caa729b0.png  \n",
            "  inflating: train/Loose Silky-bent/1d11a167c.png  \n",
            "  inflating: train/Loose Silky-bent/1db16c185.png  \n",
            "  inflating: train/Loose Silky-bent/1dc7d9cc4.png  \n",
            "  inflating: train/Loose Silky-bent/1eddf5d80.png  \n",
            "  inflating: train/Loose Silky-bent/1f154441a.png  \n",
            "  inflating: train/Loose Silky-bent/1f57bde3c.png  \n",
            "  inflating: train/Loose Silky-bent/1f5b705c0.png  \n",
            "  inflating: train/Loose Silky-bent/1f7a21bce.png  \n",
            "  inflating: train/Loose Silky-bent/1f8562553.png  \n",
            "  inflating: train/Loose Silky-bent/1fcfb8677.png  \n",
            "  inflating: train/Loose Silky-bent/209691da0.png  \n",
            "  inflating: train/Loose Silky-bent/20df07699.png  \n",
            "  inflating: train/Loose Silky-bent/20f5b6513.png  \n",
            "  inflating: train/Loose Silky-bent/217b3e661.png  \n",
            "  inflating: train/Loose Silky-bent/217c20421.png  \n",
            "  inflating: train/Loose Silky-bent/21f99c5c2.png  \n",
            "  inflating: train/Loose Silky-bent/2220076ca.png  \n",
            "  inflating: train/Loose Silky-bent/2240f47fd.png  \n",
            "  inflating: train/Loose Silky-bent/224274f93.png  \n",
            "  inflating: train/Loose Silky-bent/230282373.png  \n",
            "  inflating: train/Loose Silky-bent/230461e8b.png  \n",
            "  inflating: train/Loose Silky-bent/2399027ab.png  \n",
            "  inflating: train/Loose Silky-bent/240ad1daf.png  \n",
            "  inflating: train/Loose Silky-bent/2424d70aa.png  \n",
            "  inflating: train/Loose Silky-bent/25b804889.png  \n",
            "  inflating: train/Loose Silky-bent/25c3965e9.png  \n",
            "  inflating: train/Loose Silky-bent/25c91550e.png  \n",
            "  inflating: train/Loose Silky-bent/267ced076.png  \n",
            "  inflating: train/Loose Silky-bent/26ec4cf7d.png  \n",
            "  inflating: train/Loose Silky-bent/26f6c56c0.png  \n",
            "  inflating: train/Loose Silky-bent/273a79f87.png  \n",
            "  inflating: train/Loose Silky-bent/2773b77ab.png  \n",
            "  inflating: train/Loose Silky-bent/278d7ff5d.png  \n",
            "  inflating: train/Loose Silky-bent/27b301a16.png  \n",
            "  inflating: train/Loose Silky-bent/280af96c0.png  \n",
            "  inflating: train/Loose Silky-bent/281f52467.png  \n",
            "  inflating: train/Loose Silky-bent/298d541a6.png  \n",
            "  inflating: train/Loose Silky-bent/299fcd56d.png  \n",
            "  inflating: train/Loose Silky-bent/29b0930d2.png  \n",
            "  inflating: train/Loose Silky-bent/2a62cd782.png  \n",
            "  inflating: train/Loose Silky-bent/2aab840e1.png  \n",
            "  inflating: train/Loose Silky-bent/2b85ff393.png  \n",
            "  inflating: train/Loose Silky-bent/2e4ca7f12.png  \n",
            "  inflating: train/Loose Silky-bent/2e6952708.png  \n",
            "  inflating: train/Loose Silky-bent/2e69d1685.png  \n",
            "  inflating: train/Loose Silky-bent/30512e18a.png  \n",
            "  inflating: train/Loose Silky-bent/30df8fb8d.png  \n",
            "  inflating: train/Loose Silky-bent/30f633411.png  \n",
            "  inflating: train/Loose Silky-bent/322d8de2d.png  \n",
            "  inflating: train/Loose Silky-bent/323963103.png  \n",
            "  inflating: train/Loose Silky-bent/32c19240b.png  \n",
            "  inflating: train/Loose Silky-bent/330630db5.png  \n",
            "  inflating: train/Loose Silky-bent/333f36618.png  \n",
            "  inflating: train/Loose Silky-bent/33709e412.png  \n",
            "  inflating: train/Loose Silky-bent/3459d1d3f.png  \n",
            "  inflating: train/Loose Silky-bent/34634ddc3.png  \n",
            "  inflating: train/Loose Silky-bent/34b65beb4.png  \n",
            "  inflating: train/Loose Silky-bent/351a76eaf.png  \n",
            "  inflating: train/Loose Silky-bent/3599550ec.png  \n",
            "  inflating: train/Loose Silky-bent/36c96eb0f.png  \n",
            "  inflating: train/Loose Silky-bent/378507a71.png  \n",
            "  inflating: train/Loose Silky-bent/37b0e4e91.png  \n",
            "  inflating: train/Loose Silky-bent/38156c37f.png  \n",
            "  inflating: train/Loose Silky-bent/386f3eb28.png  \n",
            "  inflating: train/Loose Silky-bent/397583358.png  \n",
            "  inflating: train/Loose Silky-bent/39e081b5f.png  \n",
            "  inflating: train/Loose Silky-bent/3a39e5bc5.png  \n",
            "  inflating: train/Loose Silky-bent/3affdd752.png  \n",
            "  inflating: train/Loose Silky-bent/3b5cbd2cb.png  \n",
            "  inflating: train/Loose Silky-bent/3c46bd512.png  \n",
            "  inflating: train/Loose Silky-bent/3cac767c2.png  \n",
            "  inflating: train/Loose Silky-bent/3cc7097d1.png  \n",
            "  inflating: train/Loose Silky-bent/3d109d1b0.png  \n",
            "  inflating: train/Loose Silky-bent/3d9f8222d.png  \n",
            "  inflating: train/Loose Silky-bent/3dbdb27b4.png  \n",
            "  inflating: train/Loose Silky-bent/3dcfb94a9.png  \n",
            "  inflating: train/Loose Silky-bent/3dd3fc94d.png  \n",
            "  inflating: train/Loose Silky-bent/3e2609d6a.png  \n",
            "  inflating: train/Loose Silky-bent/3f130d295.png  \n",
            "  inflating: train/Loose Silky-bent/3f13f9bae.png  \n",
            "  inflating: train/Loose Silky-bent/3f8342a84.png  \n",
            "  inflating: train/Loose Silky-bent/3fb49152c.png  \n",
            "  inflating: train/Loose Silky-bent/3fd2b83fc.png  \n",
            "  inflating: train/Loose Silky-bent/3fe014d30.png  \n",
            "  inflating: train/Loose Silky-bent/4075e8be5.png  \n",
            "  inflating: train/Loose Silky-bent/40a8eb603.png  \n",
            "  inflating: train/Loose Silky-bent/413a7e4be.png  \n",
            "  inflating: train/Loose Silky-bent/41e86ec95.png  \n",
            "  inflating: train/Loose Silky-bent/4239dbbcd.png  \n",
            "  inflating: train/Loose Silky-bent/424cd33c0.png  \n",
            "  inflating: train/Loose Silky-bent/430dac673.png  \n",
            "  inflating: train/Loose Silky-bent/433e4a3ff.png  \n",
            "  inflating: train/Loose Silky-bent/43645a24d.png  \n",
            "  inflating: train/Loose Silky-bent/4387157f1.png  \n",
            "  inflating: train/Loose Silky-bent/43a753822.png  \n",
            "  inflating: train/Loose Silky-bent/4481bff7c.png  \n",
            "  inflating: train/Loose Silky-bent/44f0c7749.png  \n",
            "  inflating: train/Loose Silky-bent/452d301a7.png  \n",
            "  inflating: train/Loose Silky-bent/4609dd9e5.png  \n",
            "  inflating: train/Loose Silky-bent/463ec2b22.png  \n",
            "  inflating: train/Loose Silky-bent/465a268eb.png  \n",
            "  inflating: train/Loose Silky-bent/46ae3ab32.png  \n",
            "  inflating: train/Loose Silky-bent/46d1f9c49.png  \n",
            "  inflating: train/Loose Silky-bent/46db35467.png  \n",
            "  inflating: train/Loose Silky-bent/46ebc27d5.png  \n",
            "  inflating: train/Loose Silky-bent/4749f4ce1.png  \n",
            "  inflating: train/Loose Silky-bent/47f57446f.png  \n",
            "  inflating: train/Loose Silky-bent/481d8400d.png  \n",
            "  inflating: train/Loose Silky-bent/482a936fc.png  \n",
            "  inflating: train/Loose Silky-bent/48e5bd4e8.png  \n",
            "  inflating: train/Loose Silky-bent/48ed4a318.png  \n",
            "  inflating: train/Loose Silky-bent/49febdc76.png  \n",
            "  inflating: train/Loose Silky-bent/4a0df8be4.png  \n",
            "  inflating: train/Loose Silky-bent/4a660a07c.png  \n",
            "  inflating: train/Loose Silky-bent/4a82673d7.png  \n",
            "  inflating: train/Loose Silky-bent/4ad784313.png  \n",
            "  inflating: train/Loose Silky-bent/4b7940ed6.png  \n",
            "  inflating: train/Loose Silky-bent/4bbc1a820.png  \n",
            "  inflating: train/Loose Silky-bent/4bc5d1c38.png  \n",
            "  inflating: train/Loose Silky-bent/4bf7c65dd.png  \n",
            "  inflating: train/Loose Silky-bent/4c0447f59.png  \n",
            "  inflating: train/Loose Silky-bent/4c28a9000.png  \n",
            "  inflating: train/Loose Silky-bent/4c800c3db.png  \n",
            "  inflating: train/Loose Silky-bent/4d1241f40.png  \n",
            "  inflating: train/Loose Silky-bent/4d2719b52.png  \n",
            "  inflating: train/Loose Silky-bent/4d3517fc8.png  \n",
            "  inflating: train/Loose Silky-bent/4d6cf6289.png  \n",
            "  inflating: train/Loose Silky-bent/4da43b2ae.png  \n",
            "  inflating: train/Loose Silky-bent/4e4f34aeb.png  \n",
            "  inflating: train/Loose Silky-bent/4eb1a1b05.png  \n",
            "  inflating: train/Loose Silky-bent/4f5d80c92.png  \n",
            "  inflating: train/Loose Silky-bent/4fb56230e.png  \n",
            "  inflating: train/Loose Silky-bent/4fd64a851.png  \n",
            "  inflating: train/Loose Silky-bent/5012867e0.png  \n",
            "  inflating: train/Loose Silky-bent/501d5f235.png  \n",
            "  inflating: train/Loose Silky-bent/50792a0df.png  \n",
            "  inflating: train/Loose Silky-bent/5089f984e.png  \n",
            "  inflating: train/Loose Silky-bent/50badc5fe.png  \n",
            "  inflating: train/Loose Silky-bent/51cbc9929.png  \n",
            "  inflating: train/Loose Silky-bent/52949d529.png  \n",
            "  inflating: train/Loose Silky-bent/52f42c8a5.png  \n",
            "  inflating: train/Loose Silky-bent/53366d2eb.png  \n",
            "  inflating: train/Loose Silky-bent/5425cd169.png  \n",
            "  inflating: train/Loose Silky-bent/543887ef6.png  \n",
            "  inflating: train/Loose Silky-bent/549a41c42.png  \n",
            "  inflating: train/Loose Silky-bent/54f9d6792.png  \n",
            "  inflating: train/Loose Silky-bent/559dae100.png  \n",
            "  inflating: train/Loose Silky-bent/5684aa726.png  \n",
            "  inflating: train/Loose Silky-bent/56ac466ec.png  \n",
            "  inflating: train/Loose Silky-bent/57d19be1b.png  \n",
            "  inflating: train/Loose Silky-bent/57d839cf9.png  \n",
            "  inflating: train/Loose Silky-bent/57df08978.png  \n",
            "  inflating: train/Loose Silky-bent/581941d43.png  \n",
            "  inflating: train/Loose Silky-bent/58258e061.png  \n",
            "  inflating: train/Loose Silky-bent/58b8aa1f3.png  \n",
            "  inflating: train/Loose Silky-bent/58d70ef74.png  \n",
            "  inflating: train/Loose Silky-bent/58e87687a.png  \n",
            "  inflating: train/Loose Silky-bent/58f28c75f.png  \n",
            "  inflating: train/Loose Silky-bent/595d95e1f.png  \n",
            "  inflating: train/Loose Silky-bent/5a141b349.png  \n",
            "  inflating: train/Loose Silky-bent/5a60a6eb4.png  \n",
            "  inflating: train/Loose Silky-bent/5a9e89ee1.png  \n",
            "  inflating: train/Loose Silky-bent/5b45f1ab3.png  \n",
            "  inflating: train/Loose Silky-bent/5b5ad325a.png  \n",
            "  inflating: train/Loose Silky-bent/5b94989ce.png  \n",
            "  inflating: train/Loose Silky-bent/5bb2ae820.png  \n",
            "  inflating: train/Loose Silky-bent/5bd4637e1.png  \n",
            "  inflating: train/Loose Silky-bent/5c0097147.png  \n",
            "  inflating: train/Loose Silky-bent/5c44bf540.png  \n",
            "  inflating: train/Loose Silky-bent/5c6fb9849.png  \n",
            "  inflating: train/Loose Silky-bent/5ca0fcfba.png  \n",
            "  inflating: train/Loose Silky-bent/5cb45767e.png  \n",
            "  inflating: train/Loose Silky-bent/5d3b1db09.png  \n",
            "  inflating: train/Loose Silky-bent/5db7b4179.png  \n",
            "  inflating: train/Loose Silky-bent/5dbd18569.png  \n",
            "  inflating: train/Loose Silky-bent/5deb3574c.png  \n",
            "  inflating: train/Loose Silky-bent/5ded4ca05.png  \n",
            "  inflating: train/Loose Silky-bent/5e5d9f92f.png  \n",
            "  inflating: train/Loose Silky-bent/5f29a86e2.png  \n",
            "  inflating: train/Loose Silky-bent/5f3a942e4.png  \n",
            "  inflating: train/Loose Silky-bent/5f6ba8739.png  \n",
            "  inflating: train/Loose Silky-bent/5fa5b108a.png  \n",
            "  inflating: train/Loose Silky-bent/5fda67f10.png  \n",
            "  inflating: train/Loose Silky-bent/6033b17af.png  \n",
            "  inflating: train/Loose Silky-bent/60633ad65.png  \n",
            "  inflating: train/Loose Silky-bent/611680af6.png  \n",
            "  inflating: train/Loose Silky-bent/61a45cb4d.png  \n",
            "  inflating: train/Loose Silky-bent/6202edcfd.png  \n",
            "  inflating: train/Loose Silky-bent/620fdcc9b.png  \n",
            "  inflating: train/Loose Silky-bent/6210b8de1.png  \n",
            "  inflating: train/Loose Silky-bent/621c601f1.png  \n",
            "  inflating: train/Loose Silky-bent/623b479d5.png  \n",
            "  inflating: train/Loose Silky-bent/624288db1.png  \n",
            "  inflating: train/Loose Silky-bent/629d857bf.png  \n",
            "  inflating: train/Loose Silky-bent/6325fd82b.png  \n",
            "  inflating: train/Loose Silky-bent/6347090c6.png  \n",
            "  inflating: train/Loose Silky-bent/63d2782fc.png  \n",
            "  inflating: train/Loose Silky-bent/6483c1720.png  \n",
            "  inflating: train/Loose Silky-bent/64a4abfca.png  \n",
            "  inflating: train/Loose Silky-bent/650dd0e5a.png  \n",
            "  inflating: train/Loose Silky-bent/6530d3c24.png  \n",
            "  inflating: train/Loose Silky-bent/658956a0b.png  \n",
            "  inflating: train/Loose Silky-bent/65c207aa6.png  \n",
            "  inflating: train/Loose Silky-bent/6604aadc8.png  \n",
            "  inflating: train/Loose Silky-bent/667950000.png  \n",
            "  inflating: train/Loose Silky-bent/68085b562.png  \n",
            "  inflating: train/Loose Silky-bent/685074d33.png  \n",
            "  inflating: train/Loose Silky-bent/6850ccd12.png  \n",
            "  inflating: train/Loose Silky-bent/69505e138.png  \n",
            "  inflating: train/Loose Silky-bent/698877cd3.png  \n",
            "  inflating: train/Loose Silky-bent/698ce4468.png  \n",
            "  inflating: train/Loose Silky-bent/69a3ef92d.png  \n",
            "  inflating: train/Loose Silky-bent/6a6b4d0ad.png  \n",
            "  inflating: train/Loose Silky-bent/6aa8cc9d9.png  \n",
            "  inflating: train/Loose Silky-bent/6ad2ae242.png  \n",
            "  inflating: train/Loose Silky-bent/6af4dbc15.png  \n",
            "  inflating: train/Loose Silky-bent/6b006cb85.png  \n",
            "  inflating: train/Loose Silky-bent/6bbc3e3dd.png  \n",
            "  inflating: train/Loose Silky-bent/6bf6d3f24.png  \n",
            "  inflating: train/Loose Silky-bent/6c4d469c8.png  \n",
            "  inflating: train/Loose Silky-bent/6c9de038f.png  \n",
            "  inflating: train/Loose Silky-bent/6ca0979d0.png  \n",
            "  inflating: train/Loose Silky-bent/6d0ad6911.png  \n",
            "  inflating: train/Loose Silky-bent/6d2f11b51.png  \n",
            "  inflating: train/Loose Silky-bent/6d78ef24f.png  \n",
            "  inflating: train/Loose Silky-bent/6e04be655.png  \n",
            "  inflating: train/Loose Silky-bent/6ec8464dd.png  \n",
            "  inflating: train/Loose Silky-bent/6f112750f.png  \n",
            "  inflating: train/Loose Silky-bent/6f9567a5b.png  \n",
            "  inflating: train/Loose Silky-bent/70ea37ffc.png  \n",
            "  inflating: train/Loose Silky-bent/72579dc61.png  \n",
            "  inflating: train/Loose Silky-bent/72a81627f.png  \n",
            "  inflating: train/Loose Silky-bent/730b45269.png  \n",
            "  inflating: train/Loose Silky-bent/7324d3cc4.png  \n",
            "  inflating: train/Loose Silky-bent/73a56527d.png  \n",
            "  inflating: train/Loose Silky-bent/73a9fb9aa.png  \n",
            "  inflating: train/Loose Silky-bent/73f67dafe.png  \n",
            "  inflating: train/Loose Silky-bent/73fd71a9d.png  \n",
            "  inflating: train/Loose Silky-bent/74012c9fe.png  \n",
            "  inflating: train/Loose Silky-bent/74425d617.png  \n",
            "  inflating: train/Loose Silky-bent/746275a76.png  \n",
            "  inflating: train/Loose Silky-bent/75902535c.png  \n",
            "  inflating: train/Loose Silky-bent/759d40ddf.png  \n",
            "  inflating: train/Loose Silky-bent/7720c60d0.png  \n",
            "  inflating: train/Loose Silky-bent/77291b3ad.png  \n",
            "  inflating: train/Loose Silky-bent/77b0abed9.png  \n",
            "  inflating: train/Loose Silky-bent/77b27933d.png  \n",
            "  inflating: train/Loose Silky-bent/77dc3df93.png  \n",
            "  inflating: train/Loose Silky-bent/77ea88509.png  \n",
            "  inflating: train/Loose Silky-bent/780b7ab63.png  \n",
            "  inflating: train/Loose Silky-bent/7861c340d.png  \n",
            "  inflating: train/Loose Silky-bent/7890b9f6a.png  \n",
            "  inflating: train/Loose Silky-bent/79506c07d.png  \n",
            "  inflating: train/Loose Silky-bent/79860d6ef.png  \n",
            "  inflating: train/Loose Silky-bent/798eeca3b.png  \n",
            "  inflating: train/Loose Silky-bent/79cc28175.png  \n",
            "  inflating: train/Loose Silky-bent/7a237e6bb.png  \n",
            "  inflating: train/Loose Silky-bent/7a6ab6192.png  \n",
            "  inflating: train/Loose Silky-bent/7a8370ede.png  \n",
            "  inflating: train/Loose Silky-bent/7b4598d18.png  \n",
            "  inflating: train/Loose Silky-bent/7b48abef4.png  \n",
            "  inflating: train/Loose Silky-bent/7c1dbc5e1.png  \n",
            "  inflating: train/Loose Silky-bent/7c8202c53.png  \n",
            "  inflating: train/Loose Silky-bent/7d36fb3eb.png  \n",
            "  inflating: train/Loose Silky-bent/7d4cbdef9.png  \n",
            "  inflating: train/Loose Silky-bent/7d5f89994.png  \n",
            "  inflating: train/Loose Silky-bent/7dafa389c.png  \n",
            "  inflating: train/Loose Silky-bent/7ebd4f913.png  \n",
            "  inflating: train/Loose Silky-bent/80cf1232d.png  \n",
            "  inflating: train/Loose Silky-bent/80d615a4a.png  \n",
            "  inflating: train/Loose Silky-bent/80f4097f5.png  \n",
            "  inflating: train/Loose Silky-bent/811e2b508.png  \n",
            "  inflating: train/Loose Silky-bent/82bf95b2c.png  \n",
            "  inflating: train/Loose Silky-bent/82e4a193e.png  \n",
            "  inflating: train/Loose Silky-bent/830455097.png  \n",
            "  inflating: train/Loose Silky-bent/834903369.png  \n",
            "  inflating: train/Loose Silky-bent/843adc7e9.png  \n",
            "  inflating: train/Loose Silky-bent/8521a1b1a.png  \n",
            "  inflating: train/Loose Silky-bent/858541bfd.png  \n",
            "  inflating: train/Loose Silky-bent/863e7ae9b.png  \n",
            "  inflating: train/Loose Silky-bent/86516aeee.png  \n",
            "  inflating: train/Loose Silky-bent/86761b812.png  \n",
            "  inflating: train/Loose Silky-bent/86dfb25ea.png  \n",
            "  inflating: train/Loose Silky-bent/86f407dd7.png  \n",
            "  inflating: train/Loose Silky-bent/870cbbd42.png  \n",
            "  inflating: train/Loose Silky-bent/8742a85f0.png  \n",
            "  inflating: train/Loose Silky-bent/874893994.png  \n",
            "  inflating: train/Loose Silky-bent/8759db7d3.png  \n",
            "  inflating: train/Loose Silky-bent/875ef92d7.png  \n",
            "  inflating: train/Loose Silky-bent/87ab65336.png  \n",
            "  inflating: train/Loose Silky-bent/881498f15.png  \n",
            "  inflating: train/Loose Silky-bent/8844e8224.png  \n",
            "  inflating: train/Loose Silky-bent/88a48f427.png  \n",
            "  inflating: train/Loose Silky-bent/88aa00385.png  \n",
            "  inflating: train/Loose Silky-bent/8935e4b49.png  \n",
            "  inflating: train/Loose Silky-bent/897db9248.png  \n",
            "  inflating: train/Loose Silky-bent/89aa6c641.png  \n",
            "  inflating: train/Loose Silky-bent/8a53a8d6f.png  \n",
            "  inflating: train/Loose Silky-bent/8ab06ae81.png  \n",
            "  inflating: train/Loose Silky-bent/8bfc50c8e.png  \n",
            "  inflating: train/Loose Silky-bent/8c22f7d82.png  \n",
            "  inflating: train/Loose Silky-bent/8c796e67b.png  \n",
            "  inflating: train/Loose Silky-bent/8cf31d387.png  \n",
            "  inflating: train/Loose Silky-bent/8dcfdf361.png  \n",
            "  inflating: train/Loose Silky-bent/8e5f8c5e7.png  \n",
            "  inflating: train/Loose Silky-bent/8e8904d72.png  \n",
            "  inflating: train/Loose Silky-bent/8ee4c4f97.png  \n",
            "  inflating: train/Loose Silky-bent/8f12e398f.png  \n",
            "  inflating: train/Loose Silky-bent/8f1f0f8e6.png  \n",
            "  inflating: train/Loose Silky-bent/8f803c74c.png  \n",
            "  inflating: train/Loose Silky-bent/8fab11dfb.png  \n",
            "  inflating: train/Loose Silky-bent/8fb04526c.png  \n",
            "  inflating: train/Loose Silky-bent/9030f88a0.png  \n",
            "  inflating: train/Loose Silky-bent/9046f0356.png  \n",
            "  inflating: train/Loose Silky-bent/91c7359d1.png  \n",
            "  inflating: train/Loose Silky-bent/92835e424.png  \n",
            "  inflating: train/Loose Silky-bent/9289f58fd.png  \n",
            "  inflating: train/Loose Silky-bent/9306b0731.png  \n",
            "  inflating: train/Loose Silky-bent/93806ccdb.png  \n",
            "  inflating: train/Loose Silky-bent/93d283bce.png  \n",
            "  inflating: train/Loose Silky-bent/93d9858f0.png  \n",
            "  inflating: train/Loose Silky-bent/93f8327c5.png  \n",
            "  inflating: train/Loose Silky-bent/943edde10.png  \n",
            "  inflating: train/Loose Silky-bent/9495b8cda.png  \n",
            "  inflating: train/Loose Silky-bent/95273b4a1.png  \n",
            "  inflating: train/Loose Silky-bent/953a9aefd.png  \n",
            "  inflating: train/Loose Silky-bent/9589aea99.png  \n",
            "  inflating: train/Loose Silky-bent/95b1c779f.png  \n",
            "  inflating: train/Loose Silky-bent/96345de76.png  \n",
            "  inflating: train/Loose Silky-bent/96504ba69.png  \n",
            "  inflating: train/Loose Silky-bent/96def553d.png  \n",
            "  inflating: train/Loose Silky-bent/970834d96.png  \n",
            "  inflating: train/Loose Silky-bent/972c2c8f7.png  \n",
            "  inflating: train/Loose Silky-bent/973d6c568.png  \n",
            "  inflating: train/Loose Silky-bent/9774bec4b.png  \n",
            "  inflating: train/Loose Silky-bent/978ca7607.png  \n",
            "  inflating: train/Loose Silky-bent/97e8ebbbe.png  \n",
            "  inflating: train/Loose Silky-bent/980937153.png  \n",
            "  inflating: train/Loose Silky-bent/98fe7c3ae.png  \n",
            "  inflating: train/Loose Silky-bent/997ffd9ae.png  \n",
            "  inflating: train/Loose Silky-bent/9a2b47af1.png  \n",
            "  inflating: train/Loose Silky-bent/9aac949ec.png  \n",
            "  inflating: train/Loose Silky-bent/9b7799b3c.png  \n",
            "  inflating: train/Loose Silky-bent/9bf1450b6.png  \n",
            "  inflating: train/Loose Silky-bent/9bfa9d719.png  \n",
            "  inflating: train/Loose Silky-bent/9c05edbce.png  \n",
            "  inflating: train/Loose Silky-bent/9c46a8faf.png  \n",
            "  inflating: train/Loose Silky-bent/9cf880bda.png  \n",
            "  inflating: train/Loose Silky-bent/9d1161173.png  \n",
            "  inflating: train/Loose Silky-bent/9dc674d5d.png  \n",
            "  inflating: train/Loose Silky-bent/9dee801a8.png  \n",
            "  inflating: train/Loose Silky-bent/9e2523c37.png  \n",
            "  inflating: train/Loose Silky-bent/9f1e7cc01.png  \n",
            "  inflating: train/Loose Silky-bent/a1210bea8.png  \n",
            "  inflating: train/Loose Silky-bent/a17554c02.png  \n",
            "  inflating: train/Loose Silky-bent/a20b31b68.png  \n",
            "  inflating: train/Loose Silky-bent/a2bdb69e0.png  \n",
            "  inflating: train/Loose Silky-bent/a34a04820.png  \n",
            "  inflating: train/Loose Silky-bent/a3fe95411.png  \n",
            "  inflating: train/Loose Silky-bent/a4204df25.png  \n",
            "  inflating: train/Loose Silky-bent/a43014876.png  \n",
            "  inflating: train/Loose Silky-bent/a49930f40.png  \n",
            "  inflating: train/Loose Silky-bent/a4dcf7616.png  \n",
            "  inflating: train/Loose Silky-bent/a5429dac9.png  \n",
            "  inflating: train/Loose Silky-bent/a5bb0ae6a.png  \n",
            "  inflating: train/Loose Silky-bent/a5e621f3f.png  \n",
            "  inflating: train/Loose Silky-bent/a60d26781.png  \n",
            "  inflating: train/Loose Silky-bent/a6c5cd3b8.png  \n",
            "  inflating: train/Loose Silky-bent/a719f6d93.png  \n",
            "  inflating: train/Loose Silky-bent/a7467a0ae.png  \n",
            "  inflating: train/Loose Silky-bent/a8085a6fc.png  \n",
            "  inflating: train/Loose Silky-bent/a89d6d72f.png  \n",
            "  inflating: train/Loose Silky-bent/a905c1080.png  \n",
            "  inflating: train/Loose Silky-bent/a954a2f55.png  \n",
            "  inflating: train/Loose Silky-bent/a9600a185.png  \n",
            "  inflating: train/Loose Silky-bent/a96276a20.png  \n",
            "  inflating: train/Loose Silky-bent/aa7620ab1.png  \n",
            "  inflating: train/Loose Silky-bent/ab47c78db.png  \n",
            "  inflating: train/Loose Silky-bent/ab66b14f0.png  \n",
            "  inflating: train/Loose Silky-bent/abe1fdd5c.png  \n",
            "  inflating: train/Loose Silky-bent/ac6377773.png  \n",
            "  inflating: train/Loose Silky-bent/ac8027304.png  \n",
            "  inflating: train/Loose Silky-bent/accab3c58.png  \n",
            "  inflating: train/Loose Silky-bent/ad648af69.png  \n",
            "  inflating: train/Loose Silky-bent/ae0dc5708.png  \n",
            "  inflating: train/Loose Silky-bent/ae1c97a42.png  \n",
            "  inflating: train/Loose Silky-bent/af2465f9e.png  \n",
            "  inflating: train/Loose Silky-bent/af355776d.png  \n",
            "  inflating: train/Loose Silky-bent/af36b12f3.png  \n",
            "  inflating: train/Loose Silky-bent/af48352aa.png  \n",
            "  inflating: train/Loose Silky-bent/b01d0bbea.png  \n",
            "  inflating: train/Loose Silky-bent/b0aee48c3.png  \n",
            "  inflating: train/Loose Silky-bent/b0b2c3bdd.png  \n",
            "  inflating: train/Loose Silky-bent/b0cebe0d0.png  \n",
            "  inflating: train/Loose Silky-bent/b204c0ea0.png  \n",
            "  inflating: train/Loose Silky-bent/b2b62db50.png  \n",
            "  inflating: train/Loose Silky-bent/b2f98cf38.png  \n",
            "  inflating: train/Loose Silky-bent/b3184549e.png  \n",
            "  inflating: train/Loose Silky-bent/b32f49c26.png  \n",
            "  inflating: train/Loose Silky-bent/b396ce363.png  \n",
            "  inflating: train/Loose Silky-bent/b39cf3ed0.png  \n",
            "  inflating: train/Loose Silky-bent/b3b03d5b6.png  \n",
            "  inflating: train/Loose Silky-bent/b3f997421.png  \n",
            "  inflating: train/Loose Silky-bent/b40d772a4.png  \n",
            "  inflating: train/Loose Silky-bent/b468e967c.png  \n",
            "  inflating: train/Loose Silky-bent/b49c19b6a.png  \n",
            "  inflating: train/Loose Silky-bent/b4aeeab74.png  \n",
            "  inflating: train/Loose Silky-bent/b5585594a.png  \n",
            "  inflating: train/Loose Silky-bent/b5f074be0.png  \n",
            "  inflating: train/Loose Silky-bent/b6466cc30.png  \n",
            "  inflating: train/Loose Silky-bent/b67fe9b5f.png  \n",
            "  inflating: train/Loose Silky-bent/b6c2e77db.png  \n",
            "  inflating: train/Loose Silky-bent/b6c7bb164.png  \n",
            "  inflating: train/Loose Silky-bent/b735abe17.png  \n",
            "  inflating: train/Loose Silky-bent/b7c642fb5.png  \n",
            "  inflating: train/Loose Silky-bent/b7e5943b5.png  \n",
            "  inflating: train/Loose Silky-bent/b81690ccf.png  \n",
            "  inflating: train/Loose Silky-bent/b8301c724.png  \n",
            "  inflating: train/Loose Silky-bent/b83843fd5.png  \n",
            "  inflating: train/Loose Silky-bent/b983cb52d.png  \n",
            "  inflating: train/Loose Silky-bent/b9c2cd1c5.png  \n",
            "  inflating: train/Loose Silky-bent/bb63894a1.png  \n",
            "  inflating: train/Loose Silky-bent/bc20079b6.png  \n",
            "  inflating: train/Loose Silky-bent/bc3fc4221.png  \n",
            "  inflating: train/Loose Silky-bent/bc72693f3.png  \n",
            "  inflating: train/Loose Silky-bent/bcc2bbc1d.png  \n",
            "  inflating: train/Loose Silky-bent/bcf82b0f4.png  \n",
            "  inflating: train/Loose Silky-bent/bd7ea42d4.png  \n",
            "  inflating: train/Loose Silky-bent/bdd598dec.png  \n",
            "  inflating: train/Loose Silky-bent/be4702751.png  \n",
            "  inflating: train/Loose Silky-bent/bee138256.png  \n",
            "  inflating: train/Loose Silky-bent/c05473022.png  \n",
            "  inflating: train/Loose Silky-bent/c09b10389.png  \n",
            "  inflating: train/Loose Silky-bent/c0f87cf17.png  \n",
            "  inflating: train/Loose Silky-bent/c14ba6a17.png  \n",
            "  inflating: train/Loose Silky-bent/c167ddff0.png  \n",
            "  inflating: train/Loose Silky-bent/c18683c6b.png  \n",
            "  inflating: train/Loose Silky-bent/c1af927f2.png  \n",
            "  inflating: train/Loose Silky-bent/c1d37bdb2.png  \n",
            "  inflating: train/Loose Silky-bent/c21fd1c63.png  \n",
            "  inflating: train/Loose Silky-bent/c30c4c58e.png  \n",
            "  inflating: train/Loose Silky-bent/c334c6791.png  \n",
            "  inflating: train/Loose Silky-bent/c35d5c3c3.png  \n",
            "  inflating: train/Loose Silky-bent/c3b34e88c.png  \n",
            "  inflating: train/Loose Silky-bent/c421f4454.png  \n",
            "  inflating: train/Loose Silky-bent/c4b6392d0.png  \n",
            "  inflating: train/Loose Silky-bent/c513114b7.png  \n",
            "  inflating: train/Loose Silky-bent/c5688e1f8.png  \n",
            "  inflating: train/Loose Silky-bent/c5b5a2f6f.png  \n",
            "  inflating: train/Loose Silky-bent/c5bb1b336.png  \n",
            "  inflating: train/Loose Silky-bent/c611dbbd0.png  \n",
            "  inflating: train/Loose Silky-bent/c629a95e2.png  \n",
            "  inflating: train/Loose Silky-bent/c646f63c7.png  \n",
            "  inflating: train/Loose Silky-bent/c6cf972b8.png  \n",
            "  inflating: train/Loose Silky-bent/c6f54e816.png  \n",
            "  inflating: train/Loose Silky-bent/c88c61e2b.png  \n",
            "  inflating: train/Loose Silky-bent/c932d467d.png  \n",
            "  inflating: train/Loose Silky-bent/ca34fdafb.png  \n",
            "  inflating: train/Loose Silky-bent/ca57fa675.png  \n",
            "  inflating: train/Loose Silky-bent/cb8ebfa74.png  \n",
            "  inflating: train/Loose Silky-bent/cc3f7e692.png  \n",
            "  inflating: train/Loose Silky-bent/cc4f96146.png  \n",
            "  inflating: train/Loose Silky-bent/cc5e3a6b0.png  \n",
            "  inflating: train/Loose Silky-bent/cc6eb9e5e.png  \n",
            "  inflating: train/Loose Silky-bent/cd74f8747.png  \n",
            "  inflating: train/Loose Silky-bent/cdc38fb13.png  \n",
            "  inflating: train/Loose Silky-bent/ce70bc8a7.png  \n",
            "  inflating: train/Loose Silky-bent/cffbd81fb.png  \n",
            "  inflating: train/Loose Silky-bent/d0228efc3.png  \n",
            "  inflating: train/Loose Silky-bent/d09db3735.png  \n",
            "  inflating: train/Loose Silky-bent/d106fca01.png  \n",
            "  inflating: train/Loose Silky-bent/d160d5d69.png  \n",
            "  inflating: train/Loose Silky-bent/d1653b4c3.png  \n",
            "  inflating: train/Loose Silky-bent/d1a8e6b41.png  \n",
            "  inflating: train/Loose Silky-bent/d1d87fa3e.png  \n",
            "  inflating: train/Loose Silky-bent/d211c4f89.png  \n",
            "  inflating: train/Loose Silky-bent/d22fac46d.png  \n",
            "  inflating: train/Loose Silky-bent/d25c7447d.png  \n",
            "  inflating: train/Loose Silky-bent/d260a6bc4.png  \n",
            "  inflating: train/Loose Silky-bent/d30fdc9aa.png  \n",
            "  inflating: train/Loose Silky-bent/d31724b7f.png  \n",
            "  inflating: train/Loose Silky-bent/d36c7675c.png  \n",
            "  inflating: train/Loose Silky-bent/d37099c79.png  \n",
            "  inflating: train/Loose Silky-bent/d3bc96906.png  \n",
            "  inflating: train/Loose Silky-bent/d41f0cd9c.png  \n",
            "  inflating: train/Loose Silky-bent/d4715e76f.png  \n",
            "  inflating: train/Loose Silky-bent/d4bda0d92.png  \n",
            "  inflating: train/Loose Silky-bent/d4d65f637.png  \n",
            "  inflating: train/Loose Silky-bent/d4e30eb20.png  \n",
            "  inflating: train/Loose Silky-bent/d5f5108df.png  \n",
            "  inflating: train/Loose Silky-bent/d62f8f844.png  \n",
            "  inflating: train/Loose Silky-bent/d64d0d197.png  \n",
            "  inflating: train/Loose Silky-bent/d681d1b9b.png  \n",
            "  inflating: train/Loose Silky-bent/d6c2bcfe8.png  \n",
            "  inflating: train/Loose Silky-bent/d6e1e93a8.png  \n",
            "  inflating: train/Loose Silky-bent/d734ed06a.png  \n",
            "  inflating: train/Loose Silky-bent/d766d9692.png  \n",
            "  inflating: train/Loose Silky-bent/d7954bb16.png  \n",
            "  inflating: train/Loose Silky-bent/d7fe62bae.png  \n",
            "  inflating: train/Loose Silky-bent/d89fa34ab.png  \n",
            "  inflating: train/Loose Silky-bent/d9061cf70.png  \n",
            "  inflating: train/Loose Silky-bent/d94aa2cfc.png  \n",
            "  inflating: train/Loose Silky-bent/da007048a.png  \n",
            "  inflating: train/Loose Silky-bent/da09ecea8.png  \n",
            "  inflating: train/Loose Silky-bent/db5a0970c.png  \n",
            "  inflating: train/Loose Silky-bent/db5bc3276.png  \n",
            "  inflating: train/Loose Silky-bent/dbcc62b36.png  \n",
            "  inflating: train/Loose Silky-bent/dbe5c4ab1.png  \n",
            "  inflating: train/Loose Silky-bent/dc623538a.png  \n",
            "  inflating: train/Loose Silky-bent/dcdd20842.png  \n",
            "  inflating: train/Loose Silky-bent/dd4251469.png  \n",
            "  inflating: train/Loose Silky-bent/ddf6295e6.png  \n",
            "  inflating: train/Loose Silky-bent/de82f96f1.png  \n",
            "  inflating: train/Loose Silky-bent/de8b3b16b.png  \n",
            "  inflating: train/Loose Silky-bent/dee2a5307.png  \n",
            "  inflating: train/Loose Silky-bent/df27638ee.png  \n",
            "  inflating: train/Loose Silky-bent/df28839c9.png  \n",
            "  inflating: train/Loose Silky-bent/df615189b.png  \n",
            "  inflating: train/Loose Silky-bent/dfbdda5d7.png  \n",
            "  inflating: train/Loose Silky-bent/e0c207a01.png  \n",
            "  inflating: train/Loose Silky-bent/e0f5afd45.png  \n",
            "  inflating: train/Loose Silky-bent/e1063a052.png  \n",
            "  inflating: train/Loose Silky-bent/e26600777.png  \n",
            "  inflating: train/Loose Silky-bent/e311f2112.png  \n",
            "  inflating: train/Loose Silky-bent/e31c8a1c7.png  \n",
            "  inflating: train/Loose Silky-bent/e4e781114.png  \n",
            "  inflating: train/Loose Silky-bent/e513efb9a.png  \n",
            "  inflating: train/Loose Silky-bent/e5140f585.png  \n",
            "  inflating: train/Loose Silky-bent/e51811621.png  \n",
            "  inflating: train/Loose Silky-bent/e55c8ded3.png  \n",
            "  inflating: train/Loose Silky-bent/e5fef8fc7.png  \n",
            "  inflating: train/Loose Silky-bent/e60e41d92.png  \n",
            "  inflating: train/Loose Silky-bent/e616fd2f3.png  \n",
            "  inflating: train/Loose Silky-bent/e70f5f158.png  \n",
            "  inflating: train/Loose Silky-bent/e7d948ade.png  \n",
            "  inflating: train/Loose Silky-bent/e8258bf4a.png  \n",
            "  inflating: train/Loose Silky-bent/e89b48837.png  \n",
            "  inflating: train/Loose Silky-bent/e913e5ca6.png  \n",
            "  inflating: train/Loose Silky-bent/e984f580b.png  \n",
            "  inflating: train/Loose Silky-bent/e9c9b0bac.png  \n",
            "  inflating: train/Loose Silky-bent/e9e0e6feb.png  \n",
            "  inflating: train/Loose Silky-bent/e9ef76a8d.png  \n",
            "  inflating: train/Loose Silky-bent/ea5f902eb.png  \n",
            "  inflating: train/Loose Silky-bent/eaebb4472.png  \n",
            "  inflating: train/Loose Silky-bent/eafe89ea6.png  \n",
            "  inflating: train/Loose Silky-bent/eca81d5e8.png  \n",
            "  inflating: train/Loose Silky-bent/ed443bd4c.png  \n",
            "  inflating: train/Loose Silky-bent/eda07616f.png  \n",
            "  inflating: train/Loose Silky-bent/ee23cb68c.png  \n",
            "  inflating: train/Loose Silky-bent/ee4a45967.png  \n",
            "  inflating: train/Loose Silky-bent/ee72ab40b.png  \n",
            "  inflating: train/Loose Silky-bent/eeddd27da.png  \n",
            "  inflating: train/Loose Silky-bent/efd3b6718.png  \n",
            "  inflating: train/Loose Silky-bent/f099e9a3c.png  \n",
            "  inflating: train/Loose Silky-bent/f1a620ad7.png  \n",
            "  inflating: train/Loose Silky-bent/f25b51762.png  \n",
            "  inflating: train/Loose Silky-bent/f284e84bc.png  \n",
            "  inflating: train/Loose Silky-bent/f2a87f91f.png  \n",
            "  inflating: train/Loose Silky-bent/f2b027c00.png  \n",
            "  inflating: train/Loose Silky-bent/f3166a830.png  \n",
            "  inflating: train/Loose Silky-bent/f3d10b887.png  \n",
            "  inflating: train/Loose Silky-bent/f40d45ecd.png  \n",
            "  inflating: train/Loose Silky-bent/f4a2acf34.png  \n",
            "  inflating: train/Loose Silky-bent/f5602e151.png  \n",
            "  inflating: train/Loose Silky-bent/f5c98860a.png  \n",
            "  inflating: train/Loose Silky-bent/f6439330f.png  \n",
            "  inflating: train/Loose Silky-bent/f6a02772a.png  \n",
            "  inflating: train/Loose Silky-bent/f6a86d328.png  \n",
            "  inflating: train/Loose Silky-bent/f6ab6c208.png  \n",
            "  inflating: train/Loose Silky-bent/f75f5a9d1.png  \n",
            "  inflating: train/Loose Silky-bent/f76dc25a9.png  \n",
            "  inflating: train/Loose Silky-bent/f770f10e4.png  \n",
            "  inflating: train/Loose Silky-bent/f83bb45eb.png  \n",
            "  inflating: train/Loose Silky-bent/f89f44126.png  \n",
            "  inflating: train/Loose Silky-bent/f992263d7.png  \n",
            "  inflating: train/Loose Silky-bent/f9a8a0e39.png  \n",
            "  inflating: train/Loose Silky-bent/f9aca9f1f.png  \n",
            "  inflating: train/Loose Silky-bent/f9d597956.png  \n",
            "  inflating: train/Loose Silky-bent/fa99ad032.png  \n",
            "  inflating: train/Loose Silky-bent/fbe2de7e7.png  \n",
            "  inflating: train/Loose Silky-bent/fc2b27fff.png  \n",
            "  inflating: train/Loose Silky-bent/fced10d82.png  \n",
            "  inflating: train/Loose Silky-bent/fd0430c6e.png  \n",
            "  inflating: train/Loose Silky-bent/fdb42d1ce.png  \n",
            "  inflating: train/Loose Silky-bent/fdf2e6159.png  \n",
            "  inflating: train/Loose Silky-bent/fe2016da9.png  \n",
            "  inflating: train/Loose Silky-bent/fe23251ed.png  \n",
            "  inflating: train/Loose Silky-bent/fe621cd95.png  \n",
            "  inflating: train/Loose Silky-bent/fe944dee4.png  \n",
            "  inflating: train/Loose Silky-bent/ff1f224a5.png  \n",
            "  inflating: train/Loose Silky-bent/ff771d06e.png  \n",
            "  inflating: train/Loose Silky-bent/ffbc907bc.png  \n",
            "  inflating: train/Loose Silky-bent/ffccdc58a.png  \n",
            "  inflating: train/Maize/006196e1c.png  \n",
            "  inflating: train/Maize/0086c28b2.png  \n",
            "  inflating: train/Maize/00a18f05e.png  \n",
            "  inflating: train/Maize/0150b5b7e.png  \n",
            "  inflating: train/Maize/01642cae8.png  \n",
            "  inflating: train/Maize/0184ec53f.png  \n",
            "  inflating: train/Maize/030e7f9ef.png  \n",
            "  inflating: train/Maize/0419f5bbc.png  \n",
            "  inflating: train/Maize/051ea51d0.png  \n",
            "  inflating: train/Maize/06e1ee6e1.png  \n",
            "  inflating: train/Maize/084d21b80.png  \n",
            "  inflating: train/Maize/0bdaf1d8f.png  \n",
            "  inflating: train/Maize/0cd0d9b8c.png  \n",
            "  inflating: train/Maize/0f16cf10a.png  \n",
            "  inflating: train/Maize/107bd7230.png  \n",
            "  inflating: train/Maize/11323514a.png  \n",
            "  inflating: train/Maize/115808876.png  \n",
            "  inflating: train/Maize/11dc03905.png  \n",
            "  inflating: train/Maize/12025fcc1.png  \n",
            "  inflating: train/Maize/16511dd22.png  \n",
            "  inflating: train/Maize/16e38d4af.png  \n",
            "  inflating: train/Maize/1898398b8.png  \n",
            "  inflating: train/Maize/18a235096.png  \n",
            "  inflating: train/Maize/18b6755a1.png  \n",
            "  inflating: train/Maize/194d34152.png  \n",
            "  inflating: train/Maize/19f880311.png  \n",
            "  inflating: train/Maize/1b1ab91eb.png  \n",
            "  inflating: train/Maize/1d21b25f9.png  \n",
            "  inflating: train/Maize/1d8e6686f.png  \n",
            "  inflating: train/Maize/2187ec5fa.png  \n",
            "  inflating: train/Maize/21acedc9b.png  \n",
            "  inflating: train/Maize/23775190c.png  \n",
            "  inflating: train/Maize/23a60f63f.png  \n",
            "  inflating: train/Maize/25c87a3cc.png  \n",
            "  inflating: train/Maize/266211c3c.png  \n",
            "  inflating: train/Maize/269a34bb9.png  \n",
            "  inflating: train/Maize/270e121c0.png  \n",
            "  inflating: train/Maize/27a1dbcdc.png  \n",
            "  inflating: train/Maize/280c7b4d2.png  \n",
            "  inflating: train/Maize/2b7549816.png  \n",
            "  inflating: train/Maize/2b968a9e0.png  \n",
            "  inflating: train/Maize/2bc5905e2.png  \n",
            "  inflating: train/Maize/2d8376136.png  \n",
            "  inflating: train/Maize/2d9adb800.png  \n",
            "  inflating: train/Maize/2e08d8597.png  \n",
            "  inflating: train/Maize/30d126c24.png  \n",
            "  inflating: train/Maize/30f76315b.png  \n",
            "  inflating: train/Maize/31d92d77d.png  \n",
            "  inflating: train/Maize/3213aec10.png  \n",
            "  inflating: train/Maize/34c035273.png  \n",
            "  inflating: train/Maize/362eee504.png  \n",
            "  inflating: train/Maize/365ff4533.png  \n",
            "  inflating: train/Maize/37156c661.png  \n",
            "  inflating: train/Maize/388317d6c.png  \n",
            "  inflating: train/Maize/3a6d4d007.png  \n",
            "  inflating: train/Maize/3ac5a3def.png  \n",
            "  inflating: train/Maize/3ad5d809c.png  \n",
            "  inflating: train/Maize/3b6bc2e4c.png  \n",
            "  inflating: train/Maize/3cb4f2c65.png  \n",
            "  inflating: train/Maize/3cf92e356.png  \n",
            "  inflating: train/Maize/3ef465952.png  \n",
            "  inflating: train/Maize/44a9a599d.png  \n",
            "  inflating: train/Maize/44d562432.png  \n",
            "  inflating: train/Maize/489da47a5.png  \n",
            "  inflating: train/Maize/49e58b605.png  \n",
            "  inflating: train/Maize/4a1929689.png  \n",
            "  inflating: train/Maize/4ae8bc1c6.png  \n",
            "  inflating: train/Maize/4b5aa18ad.png  \n",
            "  inflating: train/Maize/4b825327e.png  \n",
            "  inflating: train/Maize/4ba437481.png  \n",
            "  inflating: train/Maize/4e04b01b7.png  \n",
            "  inflating: train/Maize/4ef677ce4.png  \n",
            "  inflating: train/Maize/4f6b0dc39.png  \n",
            "  inflating: train/Maize/50c80a5e4.png  \n",
            "  inflating: train/Maize/5112e0b90.png  \n",
            "  inflating: train/Maize/5150527da.png  \n",
            "  inflating: train/Maize/5251d7bbf.png  \n",
            "  inflating: train/Maize/5363a9f84.png  \n",
            "  inflating: train/Maize/5453bff0b.png  \n",
            "  inflating: train/Maize/55083690a.png  \n",
            "  inflating: train/Maize/5622305e8.png  \n",
            "  inflating: train/Maize/5675e608c.png  \n",
            "  inflating: train/Maize/5885bb41b.png  \n",
            "  inflating: train/Maize/59f972ef4.png  \n",
            "  inflating: train/Maize/5a7d10c3d.png  \n",
            "  inflating: train/Maize/5b94fab70.png  \n",
            "  inflating: train/Maize/5c7abc049.png  \n",
            "  inflating: train/Maize/5c826024a.png  \n",
            "  inflating: train/Maize/5d641d627.png  \n",
            "  inflating: train/Maize/5d6e3b183.png  \n",
            "  inflating: train/Maize/5dfa39111.png  \n",
            "  inflating: train/Maize/5fdb1dccf.png  \n",
            "  inflating: train/Maize/606d5c83f.png  \n",
            "  inflating: train/Maize/61c7d55b0.png  \n",
            "  inflating: train/Maize/6219b36db.png  \n",
            "  inflating: train/Maize/62a1614a2.png  \n",
            "  inflating: train/Maize/642e055ee.png  \n",
            "  inflating: train/Maize/648ddc558.png  \n",
            "  inflating: train/Maize/6495c3f49.png  \n",
            "  inflating: train/Maize/649b7af36.png  \n",
            "  inflating: train/Maize/64a6f6f65.png  \n",
            "  inflating: train/Maize/65b4ebc41.png  \n",
            "  inflating: train/Maize/65ba0f497.png  \n",
            "  inflating: train/Maize/665b921f2.png  \n",
            "  inflating: train/Maize/66a87d834.png  \n",
            "  inflating: train/Maize/66e90b455.png  \n",
            "  inflating: train/Maize/67c3663a8.png  \n",
            "  inflating: train/Maize/69f06fcbc.png  \n",
            "  inflating: train/Maize/6d54412c3.png  \n",
            "  inflating: train/Maize/6e9ff31e7.png  \n",
            "  inflating: train/Maize/70b85477d.png  \n",
            "  inflating: train/Maize/7389aea72.png  \n",
            "  inflating: train/Maize/7446f241c.png  \n",
            "  inflating: train/Maize/749646c56.png  \n",
            "  inflating: train/Maize/7736aedff.png  \n",
            "  inflating: train/Maize/77a5a4b51.png  \n",
            "  inflating: train/Maize/798852825.png  \n",
            "  inflating: train/Maize/7f23a2f31.png  \n",
            "  inflating: train/Maize/80fa83273.png  \n",
            "  inflating: train/Maize/814e74c21.png  \n",
            "  inflating: train/Maize/822d1163b.png  \n",
            "  inflating: train/Maize/8439a7dad.png  \n",
            "  inflating: train/Maize/84ba8107f.png  \n",
            "  inflating: train/Maize/85ae913c3.png  \n",
            "  inflating: train/Maize/85c25b5a2.png  \n",
            "  inflating: train/Maize/87a3254c9.png  \n",
            "  inflating: train/Maize/87bbc41eb.png  \n",
            "  inflating: train/Maize/8a257f6f6.png  \n",
            "  inflating: train/Maize/8bfe6860d.png  \n",
            "  inflating: train/Maize/8c6fba454.png  \n",
            "  inflating: train/Maize/8c70f6c91.png  \n",
            "  inflating: train/Maize/8c71f9b84.png  \n",
            "  inflating: train/Maize/8caddf555.png  \n",
            "  inflating: train/Maize/8cd93b279.png  \n",
            "  inflating: train/Maize/8d2004e7a.png  \n",
            "  inflating: train/Maize/905f9f385.png  \n",
            "  inflating: train/Maize/907817b91.png  \n",
            "  inflating: train/Maize/919e154c2.png  \n",
            "  inflating: train/Maize/91a9d5b9d.png  \n",
            "  inflating: train/Maize/92c06eaca.png  \n",
            "  inflating: train/Maize/96d476790.png  \n",
            "  inflating: train/Maize/97da6a579.png  \n",
            "  inflating: train/Maize/982130e94.png  \n",
            "  inflating: train/Maize/988113525.png  \n",
            "  inflating: train/Maize/9977e50bc.png  \n",
            "  inflating: train/Maize/9d1e59e00.png  \n",
            "  inflating: train/Maize/9d3514473.png  \n",
            "  inflating: train/Maize/9d588ace5.png  \n",
            "  inflating: train/Maize/9fc63a912.png  \n",
            "  inflating: train/Maize/a1d7080b1.png  \n",
            "  inflating: train/Maize/a30702538.png  \n",
            "  inflating: train/Maize/a353bdbd9.png  \n",
            "  inflating: train/Maize/a37e7b7db.png  \n",
            "  inflating: train/Maize/a47bdd663.png  \n",
            "  inflating: train/Maize/a4e71c6c3.png  \n",
            "  inflating: train/Maize/a53a61390.png  \n",
            "  inflating: train/Maize/a5c2eec2d.png  \n",
            "  inflating: train/Maize/a6e44df4f.png  \n",
            "  inflating: train/Maize/a76555ad2.png  \n",
            "  inflating: train/Maize/abadd72ab.png  \n",
            "  inflating: train/Maize/ac3de5925.png  \n",
            "  inflating: train/Maize/af014d45e.png  \n",
            "  inflating: train/Maize/b0df48778.png  \n",
            "  inflating: train/Maize/b273efc22.png  \n",
            "  inflating: train/Maize/b496ed692.png  \n",
            "  inflating: train/Maize/b62a6a471.png  \n",
            "  inflating: train/Maize/b72a19852.png  \n",
            "  inflating: train/Maize/b8b2658d9.png  \n",
            "  inflating: train/Maize/b9ffd31a7.png  \n",
            "  inflating: train/Maize/bb9f796ce.png  \n",
            "  inflating: train/Maize/bede27c87.png  \n",
            "  inflating: train/Maize/bf22827fb.png  \n",
            "  inflating: train/Maize/c05a8d7e1.png  \n",
            "  inflating: train/Maize/c08835318.png  \n",
            "  inflating: train/Maize/c5bcc4cb3.png  \n",
            "  inflating: train/Maize/c621da984.png  \n",
            "  inflating: train/Maize/c6dc512e9.png  \n",
            "  inflating: train/Maize/c6fabfb59.png  \n",
            "  inflating: train/Maize/c9b1820d0.png  \n",
            "  inflating: train/Maize/cc3674663.png  \n",
            "  inflating: train/Maize/cccbcc365.png  \n",
            "  inflating: train/Maize/cd2b68ba2.png  \n",
            "  inflating: train/Maize/cdcb0f735.png  \n",
            "  inflating: train/Maize/ce76cb1e5.png  \n",
            "  inflating: train/Maize/cf7b09a39.png  \n",
            "  inflating: train/Maize/d3963a326.png  \n",
            "  inflating: train/Maize/d3cf21397.png  \n",
            "  inflating: train/Maize/d8ae9a82b.png  \n",
            "  inflating: train/Maize/d99129a77.png  \n",
            "  inflating: train/Maize/dc7a7bdeb.png  \n",
            "  inflating: train/Maize/e17f5daf0.png  \n",
            "  inflating: train/Maize/e30accd2f.png  \n",
            "  inflating: train/Maize/e364ace75.png  \n",
            "  inflating: train/Maize/e43323e28.png  \n",
            "  inflating: train/Maize/e45f92ca7.png  \n",
            "  inflating: train/Maize/e66d87ad7.png  \n",
            "  inflating: train/Maize/e6d8ba41c.png  \n",
            "  inflating: train/Maize/eba973942.png  \n",
            "  inflating: train/Maize/edd80f8a8.png  \n",
            "  inflating: train/Maize/eefba9376.png  \n",
            "  inflating: train/Maize/f135a739f.png  \n",
            "  inflating: train/Maize/f2c22a1bf.png  \n",
            "  inflating: train/Maize/f30971a76.png  \n",
            "  inflating: train/Maize/f34429b11.png  \n",
            "  inflating: train/Maize/f3b0496dd.png  \n",
            "  inflating: train/Maize/f4dd93492.png  \n",
            "  inflating: train/Maize/f5231714b.png  \n",
            "  inflating: train/Maize/f5e10e5ec.png  \n",
            "  inflating: train/Maize/f60369038.png  \n",
            "  inflating: train/Maize/f98add892.png  \n",
            "  inflating: train/Maize/f9ceba607.png  \n",
            "  inflating: train/Maize/fb6328acb.png  \n",
            "  inflating: train/Maize/fb6bcaf52.png  \n",
            "  inflating: train/Maize/fbd09f3a5.png  \n",
            "  inflating: train/Maize/fbed08a0e.png  \n",
            "  inflating: train/Maize/fc02b8466.png  \n",
            "  inflating: train/Maize/fc68c0525.png  \n",
            "  inflating: train/Maize/fd1ef540c.png  \n",
            "  inflating: train/Maize/fdde1498f.png  \n",
            "  inflating: train/Maize/fdebe4158.png  \n",
            "  inflating: train/Maize/ff4b55219.png  \n",
            "  inflating: train/Scentless Mayweed/01358344b.png  \n",
            "  inflating: train/Scentless Mayweed/015215883.png  \n",
            "  inflating: train/Scentless Mayweed/01e90ef62.png  \n",
            "  inflating: train/Scentless Mayweed/025155400.png  \n",
            "  inflating: train/Scentless Mayweed/0258481da.png  \n",
            "  inflating: train/Scentless Mayweed/02ae4e8a3.png  \n",
            "  inflating: train/Scentless Mayweed/0372b48e1.png  \n",
            "  inflating: train/Scentless Mayweed/03906fdd7.png  \n",
            "  inflating: train/Scentless Mayweed/03a92ad22.png  \n",
            "  inflating: train/Scentless Mayweed/03ee6340f.png  \n",
            "  inflating: train/Scentless Mayweed/0438cc647.png  \n",
            "  inflating: train/Scentless Mayweed/04dff1857.png  \n",
            "  inflating: train/Scentless Mayweed/04f89976a.png  \n",
            "  inflating: train/Scentless Mayweed/05ceff7d1.png  \n",
            "  inflating: train/Scentless Mayweed/05f76f20c.png  \n",
            "  inflating: train/Scentless Mayweed/060f69cd9.png  \n",
            "  inflating: train/Scentless Mayweed/061592f02.png  \n",
            "  inflating: train/Scentless Mayweed/06efbd2bf.png  \n",
            "  inflating: train/Scentless Mayweed/078d42d74.png  \n",
            "  inflating: train/Scentless Mayweed/07bed57ea.png  \n",
            "  inflating: train/Scentless Mayweed/083a8c7d2.png  \n",
            "  inflating: train/Scentless Mayweed/086894274.png  \n",
            "  inflating: train/Scentless Mayweed/0919cf5f1.png  \n",
            "  inflating: train/Scentless Mayweed/09653e8fa.png  \n",
            "  inflating: train/Scentless Mayweed/0a2bcaf43.png  \n",
            "  inflating: train/Scentless Mayweed/0ae9acf83.png  \n",
            "  inflating: train/Scentless Mayweed/0b4517fcf.png  \n",
            "  inflating: train/Scentless Mayweed/0bccfe309.png  \n",
            "  inflating: train/Scentless Mayweed/0cbef3166.png  \n",
            "  inflating: train/Scentless Mayweed/0d58d5433.png  \n",
            "  inflating: train/Scentless Mayweed/0d5b79c3c.png  \n",
            "  inflating: train/Scentless Mayweed/0dc27b35d.png  \n",
            "  inflating: train/Scentless Mayweed/0dd73ae6a.png  \n",
            "  inflating: train/Scentless Mayweed/0e2b8c097.png  \n",
            "  inflating: train/Scentless Mayweed/0e4c95d3a.png  \n",
            "  inflating: train/Scentless Mayweed/0e6d5af38.png  \n",
            "  inflating: train/Scentless Mayweed/0e978005a.png  \n",
            "  inflating: train/Scentless Mayweed/0eaa11631.png  \n",
            "  inflating: train/Scentless Mayweed/0efb0719f.png  \n",
            "  inflating: train/Scentless Mayweed/0f9e58b1a.png  \n",
            "  inflating: train/Scentless Mayweed/10c34a660.png  \n",
            "  inflating: train/Scentless Mayweed/10c61689b.png  \n",
            "  inflating: train/Scentless Mayweed/10fc19a04.png  \n",
            "  inflating: train/Scentless Mayweed/11256441a.png  \n",
            "  inflating: train/Scentless Mayweed/1247e28dc.png  \n",
            "  inflating: train/Scentless Mayweed/126d55392.png  \n",
            "  inflating: train/Scentless Mayweed/12752c388.png  \n",
            "  inflating: train/Scentless Mayweed/1300c3cd9.png  \n",
            "  inflating: train/Scentless Mayweed/133553324.png  \n",
            "  inflating: train/Scentless Mayweed/137f5ea2f.png  \n",
            "  inflating: train/Scentless Mayweed/140c4d852.png  \n",
            "  inflating: train/Scentless Mayweed/152cc15e0.png  \n",
            "  inflating: train/Scentless Mayweed/15a668fa4.png  \n",
            "  inflating: train/Scentless Mayweed/163de1bde.png  \n",
            "  inflating: train/Scentless Mayweed/163ffa4d7.png  \n",
            "  inflating: train/Scentless Mayweed/1750dd2c8.png  \n",
            "  inflating: train/Scentless Mayweed/17bf55e51.png  \n",
            "  inflating: train/Scentless Mayweed/18387e60f.png  \n",
            "  inflating: train/Scentless Mayweed/18dbd6d8c.png  \n",
            "  inflating: train/Scentless Mayweed/192bf26cb.png  \n",
            "  inflating: train/Scentless Mayweed/195397e91.png  \n",
            "  inflating: train/Scentless Mayweed/1968974c8.png  \n",
            "  inflating: train/Scentless Mayweed/198f88ef2.png  \n",
            "  inflating: train/Scentless Mayweed/1a570b7db.png  \n",
            "  inflating: train/Scentless Mayweed/1ab17251b.png  \n",
            "  inflating: train/Scentless Mayweed/1ae3fa200.png  \n",
            "  inflating: train/Scentless Mayweed/1c0322399.png  \n",
            "  inflating: train/Scentless Mayweed/1c5665dde.png  \n",
            "  inflating: train/Scentless Mayweed/1c6e51b29.png  \n",
            "  inflating: train/Scentless Mayweed/1c9b1eb8b.png  \n",
            "  inflating: train/Scentless Mayweed/1ea513913.png  \n",
            "  inflating: train/Scentless Mayweed/1ea60c734.png  \n",
            "  inflating: train/Scentless Mayweed/1ea84cfc7.png  \n",
            "  inflating: train/Scentless Mayweed/1ed148332.png  \n",
            "  inflating: train/Scentless Mayweed/1f29d1c75.png  \n",
            "  inflating: train/Scentless Mayweed/1fb56ac4d.png  \n",
            "  inflating: train/Scentless Mayweed/1ffabce72.png  \n",
            "  inflating: train/Scentless Mayweed/202651138.png  \n",
            "  inflating: train/Scentless Mayweed/20f9f0a0c.png  \n",
            "  inflating: train/Scentless Mayweed/21fcc7b8b.png  \n",
            "  inflating: train/Scentless Mayweed/22b53d9e5.png  \n",
            "  inflating: train/Scentless Mayweed/22e662983.png  \n",
            "  inflating: train/Scentless Mayweed/237bcb5cb.png  \n",
            "  inflating: train/Scentless Mayweed/24e93bc82.png  \n",
            "  inflating: train/Scentless Mayweed/271f14517.png  \n",
            "  inflating: train/Scentless Mayweed/273dc0a1c.png  \n",
            "  inflating: train/Scentless Mayweed/275152b11.png  \n",
            "  inflating: train/Scentless Mayweed/278256e68.png  \n",
            "  inflating: train/Scentless Mayweed/28e427bed.png  \n",
            "  inflating: train/Scentless Mayweed/299bd761f.png  \n",
            "  inflating: train/Scentless Mayweed/29b4e98c4.png  \n",
            "  inflating: train/Scentless Mayweed/2a17238d0.png  \n",
            "  inflating: train/Scentless Mayweed/2a3777469.png  \n",
            "  inflating: train/Scentless Mayweed/2a5938731.png  \n",
            "  inflating: train/Scentless Mayweed/2a6edb04e.png  \n",
            "  inflating: train/Scentless Mayweed/2ab910bdf.png  \n",
            "  inflating: train/Scentless Mayweed/2b5ec01e6.png  \n",
            "  inflating: train/Scentless Mayweed/2bb431697.png  \n",
            "  inflating: train/Scentless Mayweed/2bdd11146.png  \n",
            "  inflating: train/Scentless Mayweed/2befc73e1.png  \n",
            "  inflating: train/Scentless Mayweed/2c28f05a9.png  \n",
            "  inflating: train/Scentless Mayweed/2c2da55ca.png  \n",
            "  inflating: train/Scentless Mayweed/2c6d58f91.png  \n",
            "  inflating: train/Scentless Mayweed/2cbb15881.png  \n",
            "  inflating: train/Scentless Mayweed/2de4ec934.png  \n",
            "  inflating: train/Scentless Mayweed/2e25a0433.png  \n",
            "  inflating: train/Scentless Mayweed/2e2bb8089.png  \n",
            "  inflating: train/Scentless Mayweed/2e4ae28f4.png  \n",
            "  inflating: train/Scentless Mayweed/2f2e95f3a.png  \n",
            "  inflating: train/Scentless Mayweed/2f7452cdd.png  \n",
            "  inflating: train/Scentless Mayweed/2f998af91.png  \n",
            "  inflating: train/Scentless Mayweed/2fe02f029.png  \n",
            "  inflating: train/Scentless Mayweed/2fe0fea3e.png  \n",
            "  inflating: train/Scentless Mayweed/303b50030.png  \n",
            "  inflating: train/Scentless Mayweed/304fb9580.png  \n",
            "  inflating: train/Scentless Mayweed/3059893a2.png  \n",
            "  inflating: train/Scentless Mayweed/313242291.png  \n",
            "  inflating: train/Scentless Mayweed/319667b0f.png  \n",
            "  inflating: train/Scentless Mayweed/31bfdd75e.png  \n",
            "  inflating: train/Scentless Mayweed/32bcfcf02.png  \n",
            "  inflating: train/Scentless Mayweed/32dd5e36d.png  \n",
            "  inflating: train/Scentless Mayweed/3362b3eae.png  \n",
            "  inflating: train/Scentless Mayweed/3418f9acf.png  \n",
            "  inflating: train/Scentless Mayweed/3456b7270.png  \n",
            "  inflating: train/Scentless Mayweed/356088e4b.png  \n",
            "  inflating: train/Scentless Mayweed/3585db717.png  \n",
            "  inflating: train/Scentless Mayweed/35ebfa716.png  \n",
            "  inflating: train/Scentless Mayweed/3655d2892.png  \n",
            "  inflating: train/Scentless Mayweed/368fb9035.png  \n",
            "  inflating: train/Scentless Mayweed/371ab36bf.png  \n",
            "  inflating: train/Scentless Mayweed/37573d77c.png  \n",
            "  inflating: train/Scentless Mayweed/379781ae4.png  \n",
            "  inflating: train/Scentless Mayweed/388071819.png  \n",
            "  inflating: train/Scentless Mayweed/38ddbde70.png  \n",
            "  inflating: train/Scentless Mayweed/395ff5971.png  \n",
            "  inflating: train/Scentless Mayweed/3a4f03cad.png  \n",
            "  inflating: train/Scentless Mayweed/3a64d0130.png  \n",
            "  inflating: train/Scentless Mayweed/3ab04bdf0.png  \n",
            "  inflating: train/Scentless Mayweed/3adc567c4.png  \n",
            "  inflating: train/Scentless Mayweed/3b87c72b8.png  \n",
            "  inflating: train/Scentless Mayweed/3bf4f0c04.png  \n",
            "  inflating: train/Scentless Mayweed/3c69e3279.png  \n",
            "  inflating: train/Scentless Mayweed/3d44bca16.png  \n",
            "  inflating: train/Scentless Mayweed/3e7f883f3.png  \n",
            "  inflating: train/Scentless Mayweed/3f6ec61f6.png  \n",
            "  inflating: train/Scentless Mayweed/3f7434daa.png  \n",
            "  inflating: train/Scentless Mayweed/3fb764193.png  \n",
            "  inflating: train/Scentless Mayweed/3fe4205e0.png  \n",
            "  inflating: train/Scentless Mayweed/406f54018.png  \n",
            "  inflating: train/Scentless Mayweed/4076ae5ce.png  \n",
            "  inflating: train/Scentless Mayweed/40eea04f4.png  \n",
            "  inflating: train/Scentless Mayweed/41c73ed49.png  \n",
            "  inflating: train/Scentless Mayweed/4205830a0.png  \n",
            "  inflating: train/Scentless Mayweed/425efb679.png  \n",
            "  inflating: train/Scentless Mayweed/4277f2371.png  \n",
            "  inflating: train/Scentless Mayweed/43442896c.png  \n",
            "  inflating: train/Scentless Mayweed/4358bc332.png  \n",
            "  inflating: train/Scentless Mayweed/4408bae35.png  \n",
            "  inflating: train/Scentless Mayweed/447b46d12.png  \n",
            "  inflating: train/Scentless Mayweed/4487c184f.png  \n",
            "  inflating: train/Scentless Mayweed/44e7a9365.png  \n",
            "  inflating: train/Scentless Mayweed/45d74776a.png  \n",
            "  inflating: train/Scentless Mayweed/4646a4ae6.png  \n",
            "  inflating: train/Scentless Mayweed/478505edb.png  \n",
            "  inflating: train/Scentless Mayweed/47ca1a63c.png  \n",
            "  inflating: train/Scentless Mayweed/493210bcc.png  \n",
            "  inflating: train/Scentless Mayweed/493815dcf.png  \n",
            "  inflating: train/Scentless Mayweed/49a407787.png  \n",
            "  inflating: train/Scentless Mayweed/4ae592695.png  \n",
            "  inflating: train/Scentless Mayweed/4ae939d7d.png  \n",
            "  inflating: train/Scentless Mayweed/4b910467f.png  \n",
            "  inflating: train/Scentless Mayweed/4d2960a00.png  \n",
            "  inflating: train/Scentless Mayweed/50549436d.png  \n",
            "  inflating: train/Scentless Mayweed/517c2f5ab.png  \n",
            "  inflating: train/Scentless Mayweed/5186c72ca.png  \n",
            "  inflating: train/Scentless Mayweed/51a00928f.png  \n",
            "  inflating: train/Scentless Mayweed/51adc8b21.png  \n",
            "  inflating: train/Scentless Mayweed/51e71ddde.png  \n",
            "  inflating: train/Scentless Mayweed/523edb175.png  \n",
            "  inflating: train/Scentless Mayweed/525f70f91.png  \n",
            "  inflating: train/Scentless Mayweed/534691880.png  \n",
            "  inflating: train/Scentless Mayweed/5423cd5b5.png  \n",
            "  inflating: train/Scentless Mayweed/549c186a8.png  \n",
            "  inflating: train/Scentless Mayweed/54f414890.png  \n",
            "  inflating: train/Scentless Mayweed/553e69068.png  \n",
            "  inflating: train/Scentless Mayweed/5542d13dd.png  \n",
            "  inflating: train/Scentless Mayweed/557340b87.png  \n",
            "  inflating: train/Scentless Mayweed/576951fce.png  \n",
            "  inflating: train/Scentless Mayweed/577c79af0.png  \n",
            "  inflating: train/Scentless Mayweed/5822cf4d1.png  \n",
            "  inflating: train/Scentless Mayweed/586524a24.png  \n",
            "  inflating: train/Scentless Mayweed/5906bcd76.png  \n",
            "  inflating: train/Scentless Mayweed/599f49480.png  \n",
            "  inflating: train/Scentless Mayweed/59cb79559.png  \n",
            "  inflating: train/Scentless Mayweed/5add1370e.png  \n",
            "  inflating: train/Scentless Mayweed/5b3fe64c3.png  \n",
            "  inflating: train/Scentless Mayweed/5b7eb7b82.png  \n",
            "  inflating: train/Scentless Mayweed/5c0a9c9c2.png  \n",
            "  inflating: train/Scentless Mayweed/5c79ed488.png  \n",
            "  inflating: train/Scentless Mayweed/5c9defa0f.png  \n",
            "  inflating: train/Scentless Mayweed/5e58beff8.png  \n",
            "  inflating: train/Scentless Mayweed/5e6b8c2a1.png  \n",
            "  inflating: train/Scentless Mayweed/5fa64f7ab.png  \n",
            "  inflating: train/Scentless Mayweed/5ff65344b.png  \n",
            "  inflating: train/Scentless Mayweed/607bb6891.png  \n",
            "  inflating: train/Scentless Mayweed/6132624ad.png  \n",
            "  inflating: train/Scentless Mayweed/61730c6aa.png  \n",
            "  inflating: train/Scentless Mayweed/618922851.png  \n",
            "  inflating: train/Scentless Mayweed/61974ff26.png  \n",
            "  inflating: train/Scentless Mayweed/61a79f4a1.png  \n",
            "  inflating: train/Scentless Mayweed/620457a5f.png  \n",
            "  inflating: train/Scentless Mayweed/6256740a1.png  \n",
            "  inflating: train/Scentless Mayweed/628b08c82.png  \n",
            "  inflating: train/Scentless Mayweed/62d1b63f3.png  \n",
            "  inflating: train/Scentless Mayweed/6334e8b6d.png  \n",
            "  inflating: train/Scentless Mayweed/6398c3458.png  \n",
            "  inflating: train/Scentless Mayweed/63bbcaafe.png  \n",
            "  inflating: train/Scentless Mayweed/64176476e.png  \n",
            "  inflating: train/Scentless Mayweed/64fb55b43.png  \n",
            "  inflating: train/Scentless Mayweed/65148b580.png  \n",
            "  inflating: train/Scentless Mayweed/6540c684a.png  \n",
            "  inflating: train/Scentless Mayweed/65532d3bd.png  \n",
            "  inflating: train/Scentless Mayweed/657eccef3.png  \n",
            "  inflating: train/Scentless Mayweed/66b33c098.png  \n",
            "  inflating: train/Scentless Mayweed/66d94aa80.png  \n",
            "  inflating: train/Scentless Mayweed/66f70d982.png  \n",
            "  inflating: train/Scentless Mayweed/67cd81694.png  \n",
            "  inflating: train/Scentless Mayweed/689c2b218.png  \n",
            "  inflating: train/Scentless Mayweed/68f032e5b.png  \n",
            "  inflating: train/Scentless Mayweed/6901b24e7.png  \n",
            "  inflating: train/Scentless Mayweed/690595798.png  \n",
            "  inflating: train/Scentless Mayweed/6908c743e.png  \n",
            "  inflating: train/Scentless Mayweed/6956b6e58.png  \n",
            "  inflating: train/Scentless Mayweed/69af17ea5.png  \n",
            "  inflating: train/Scentless Mayweed/69da83055.png  \n",
            "  inflating: train/Scentless Mayweed/69ed658dd.png  \n",
            "  inflating: train/Scentless Mayweed/6a45a19ec.png  \n",
            "  inflating: train/Scentless Mayweed/6ae097017.png  \n",
            "  inflating: train/Scentless Mayweed/6af99dc3d.png  \n",
            "  inflating: train/Scentless Mayweed/6c2cef408.png  \n",
            "  inflating: train/Scentless Mayweed/6c404cca3.png  \n",
            "  inflating: train/Scentless Mayweed/6cb03c2ec.png  \n",
            "  inflating: train/Scentless Mayweed/6d0662339.png  \n",
            "  inflating: train/Scentless Mayweed/6d119dc1b.png  \n",
            "  inflating: train/Scentless Mayweed/6d80eac2a.png  \n",
            "  inflating: train/Scentless Mayweed/6df9e4b85.png  \n",
            "  inflating: train/Scentless Mayweed/6f6b0c523.png  \n",
            "  inflating: train/Scentless Mayweed/6f9829b56.png  \n",
            "  inflating: train/Scentless Mayweed/712b2a926.png  \n",
            "  inflating: train/Scentless Mayweed/724363df7.png  \n",
            "  inflating: train/Scentless Mayweed/726c2bcb1.png  \n",
            "  inflating: train/Scentless Mayweed/727fe56ac.png  \n",
            "  inflating: train/Scentless Mayweed/72824c328.png  \n",
            "  inflating: train/Scentless Mayweed/72d1abef3.png  \n",
            "  inflating: train/Scentless Mayweed/731cd4d26.png  \n",
            "  inflating: train/Scentless Mayweed/734fc97ac.png  \n",
            "  inflating: train/Scentless Mayweed/737fe1702.png  \n",
            "  inflating: train/Scentless Mayweed/75632b096.png  \n",
            "  inflating: train/Scentless Mayweed/763ce5af0.png  \n",
            "  inflating: train/Scentless Mayweed/767558fe0.png  \n",
            "  inflating: train/Scentless Mayweed/7790dac42.png  \n",
            "  inflating: train/Scentless Mayweed/77c4b1d27.png  \n",
            "  inflating: train/Scentless Mayweed/7863544c8.png  \n",
            "  inflating: train/Scentless Mayweed/7864499c6.png  \n",
            "  inflating: train/Scentless Mayweed/789f292b3.png  \n",
            "  inflating: train/Scentless Mayweed/78cbf0faf.png  \n",
            "  inflating: train/Scentless Mayweed/78d0fe480.png  \n",
            "  inflating: train/Scentless Mayweed/7917df3ec.png  \n",
            "  inflating: train/Scentless Mayweed/79eb0958f.png  \n",
            "  inflating: train/Scentless Mayweed/7ab5ba33e.png  \n",
            "  inflating: train/Scentless Mayweed/7b1d526d2.png  \n",
            "  inflating: train/Scentless Mayweed/7b95f2701.png  \n",
            "  inflating: train/Scentless Mayweed/7cde4ff67.png  \n",
            "  inflating: train/Scentless Mayweed/7d18ff4c6.png  \n",
            "  inflating: train/Scentless Mayweed/7d1f27d5c.png  \n",
            "  inflating: train/Scentless Mayweed/7d3b1def6.png  \n",
            "  inflating: train/Scentless Mayweed/7d69c71e1.png  \n",
            "  inflating: train/Scentless Mayweed/7dcb6bd4e.png  \n",
            "  inflating: train/Scentless Mayweed/7f3b7d37e.png  \n",
            "  inflating: train/Scentless Mayweed/7f9735492.png  \n",
            "  inflating: train/Scentless Mayweed/7fdcb7bcf.png  \n",
            "  inflating: train/Scentless Mayweed/7ff4c69a1.png  \n",
            "  inflating: train/Scentless Mayweed/809f6f322.png  \n",
            "  inflating: train/Scentless Mayweed/818952af9.png  \n",
            "  inflating: train/Scentless Mayweed/82a15579e.png  \n",
            "  inflating: train/Scentless Mayweed/8334bb575.png  \n",
            "  inflating: train/Scentless Mayweed/84d2e187d.png  \n",
            "  inflating: train/Scentless Mayweed/84e4410c2.png  \n",
            "  inflating: train/Scentless Mayweed/856ab3606.png  \n",
            "  inflating: train/Scentless Mayweed/866893cf2.png  \n",
            "  inflating: train/Scentless Mayweed/870b828ff.png  \n",
            "  inflating: train/Scentless Mayweed/872673b5f.png  \n",
            "  inflating: train/Scentless Mayweed/87c6cf4ed.png  \n",
            "  inflating: train/Scentless Mayweed/883119ddc.png  \n",
            "  inflating: train/Scentless Mayweed/8842741cb.png  \n",
            "  inflating: train/Scentless Mayweed/88b1699ea.png  \n",
            "  inflating: train/Scentless Mayweed/898a9fd7e.png  \n",
            "  inflating: train/Scentless Mayweed/89960b2fb.png  \n",
            "  inflating: train/Scentless Mayweed/8b227f1b2.png  \n",
            "  inflating: train/Scentless Mayweed/8bc379617.png  \n",
            "  inflating: train/Scentless Mayweed/8bf8cb21a.png  \n",
            "  inflating: train/Scentless Mayweed/8c1d546d9.png  \n",
            "  inflating: train/Scentless Mayweed/8c496e84a.png  \n",
            "  inflating: train/Scentless Mayweed/8c742310c.png  \n",
            "  inflating: train/Scentless Mayweed/8ccb53e12.png  \n",
            "  inflating: train/Scentless Mayweed/8d18b1983.png  \n",
            "  inflating: train/Scentless Mayweed/8d2e6cf31.png  \n",
            "  inflating: train/Scentless Mayweed/8d65bde6a.png  \n",
            "  inflating: train/Scentless Mayweed/8e69ea8a0.png  \n",
            "  inflating: train/Scentless Mayweed/8e6c73968.png  \n",
            "  inflating: train/Scentless Mayweed/8ea94061e.png  \n",
            "  inflating: train/Scentless Mayweed/8edb4ab22.png  \n",
            "  inflating: train/Scentless Mayweed/8f2534b22.png  \n",
            "  inflating: train/Scentless Mayweed/8f63f6073.png  \n",
            "  inflating: train/Scentless Mayweed/8fdd4fc59.png  \n",
            "  inflating: train/Scentless Mayweed/8fed2b599.png  \n",
            "  inflating: train/Scentless Mayweed/8ff65b054.png  \n",
            "  inflating: train/Scentless Mayweed/911a91de2.png  \n",
            "  inflating: train/Scentless Mayweed/9151d1f37.png  \n",
            "  inflating: train/Scentless Mayweed/92e4b5803.png  \n",
            "  inflating: train/Scentless Mayweed/936334172.png  \n",
            "  inflating: train/Scentless Mayweed/9469abe24.png  \n",
            "  inflating: train/Scentless Mayweed/948251df3.png  \n",
            "  inflating: train/Scentless Mayweed/94ba133c0.png  \n",
            "  inflating: train/Scentless Mayweed/9524b09c2.png  \n",
            "  inflating: train/Scentless Mayweed/9694fa686.png  \n",
            "  inflating: train/Scentless Mayweed/96c6d3af6.png  \n",
            "  inflating: train/Scentless Mayweed/97ea07cab.png  \n",
            "  inflating: train/Scentless Mayweed/98317755c.png  \n",
            "  inflating: train/Scentless Mayweed/991bc45af.png  \n",
            "  inflating: train/Scentless Mayweed/9985f08e7.png  \n",
            "  inflating: train/Scentless Mayweed/99ec6cf98.png  \n",
            "  inflating: train/Scentless Mayweed/99f32c27f.png  \n",
            "  inflating: train/Scentless Mayweed/9a9058dd6.png  \n",
            "  inflating: train/Scentless Mayweed/9ab3b61db.png  \n",
            "  inflating: train/Scentless Mayweed/9b32813c0.png  \n",
            "  inflating: train/Scentless Mayweed/9b40fd5b4.png  \n",
            "  inflating: train/Scentless Mayweed/9b5bc7b68.png  \n",
            "  inflating: train/Scentless Mayweed/9c6d64872.png  \n",
            "  inflating: train/Scentless Mayweed/9d63dc050.png  \n",
            "  inflating: train/Scentless Mayweed/9dc4c048e.png  \n",
            "  inflating: train/Scentless Mayweed/9e0ef139f.png  \n",
            "  inflating: train/Scentless Mayweed/9e6395cd2.png  \n",
            "  inflating: train/Scentless Mayweed/9f04c1af6.png  \n",
            "  inflating: train/Scentless Mayweed/9f76f53bd.png  \n",
            "  inflating: train/Scentless Mayweed/a0044fa37.png  \n",
            "  inflating: train/Scentless Mayweed/a029e78bb.png  \n",
            "  inflating: train/Scentless Mayweed/a035386ae.png  \n",
            "  inflating: train/Scentless Mayweed/a06111202.png  \n",
            "  inflating: train/Scentless Mayweed/a0a13a1fe.png  \n",
            "  inflating: train/Scentless Mayweed/a0dbc4a62.png  \n",
            "  inflating: train/Scentless Mayweed/a1b632391.png  \n",
            "  inflating: train/Scentless Mayweed/a1eced3b5.png  \n",
            "  inflating: train/Scentless Mayweed/a29a89017.png  \n",
            "  inflating: train/Scentless Mayweed/a3290e2dc.png  \n",
            "  inflating: train/Scentless Mayweed/a390479c5.png  \n",
            "  inflating: train/Scentless Mayweed/a4daf7050.png  \n",
            "  inflating: train/Scentless Mayweed/a5545b963.png  \n",
            "  inflating: train/Scentless Mayweed/a591343fd.png  \n",
            "  inflating: train/Scentless Mayweed/a5976876c.png  \n",
            "  inflating: train/Scentless Mayweed/a5b0c23ea.png  \n",
            "  inflating: train/Scentless Mayweed/a5ccc5275.png  \n",
            "  inflating: train/Scentless Mayweed/a5effc842.png  \n",
            "  inflating: train/Scentless Mayweed/a5f531177.png  \n",
            "  inflating: train/Scentless Mayweed/a62fc0648.png  \n",
            "  inflating: train/Scentless Mayweed/a74ecf401.png  \n",
            "  inflating: train/Scentless Mayweed/a74f659c5.png  \n",
            "  inflating: train/Scentless Mayweed/a75b742cd.png  \n",
            "  inflating: train/Scentless Mayweed/a7ee2d8a4.png  \n",
            "  inflating: train/Scentless Mayweed/a80860b81.png  \n",
            "  inflating: train/Scentless Mayweed/a82d370cd.png  \n",
            "  inflating: train/Scentless Mayweed/a88a52491.png  \n",
            "  inflating: train/Scentless Mayweed/a8d82b05a.png  \n",
            "  inflating: train/Scentless Mayweed/a8f2c9322.png  \n",
            "  inflating: train/Scentless Mayweed/aa4b8d708.png  \n",
            "  inflating: train/Scentless Mayweed/ac997b063.png  \n",
            "  inflating: train/Scentless Mayweed/acd879423.png  \n",
            "  inflating: train/Scentless Mayweed/ad6b8c74c.png  \n",
            "  inflating: train/Scentless Mayweed/ae64f6530.png  \n",
            "  inflating: train/Scentless Mayweed/ae73ac763.png  \n",
            "  inflating: train/Scentless Mayweed/aeff0bf8c.png  \n",
            "  inflating: train/Scentless Mayweed/af5cd5756.png  \n",
            "  inflating: train/Scentless Mayweed/afa4576dc.png  \n",
            "  inflating: train/Scentless Mayweed/b05d99d59.png  \n",
            "  inflating: train/Scentless Mayweed/b0885ca26.png  \n",
            "  inflating: train/Scentless Mayweed/b0e6ba84f.png  \n",
            "  inflating: train/Scentless Mayweed/b15980d50.png  \n",
            "  inflating: train/Scentless Mayweed/b349deff8.png  \n",
            "  inflating: train/Scentless Mayweed/b3ab2acea.png  \n",
            "  inflating: train/Scentless Mayweed/b4383df28.png  \n",
            "  inflating: train/Scentless Mayweed/b50e33f38.png  \n",
            "  inflating: train/Scentless Mayweed/b5a8e95f3.png  \n",
            "  inflating: train/Scentless Mayweed/b5ce957cf.png  \n",
            "  inflating: train/Scentless Mayweed/b5e1f76d3.png  \n",
            "  inflating: train/Scentless Mayweed/b62d20d32.png  \n",
            "  inflating: train/Scentless Mayweed/b72f89405.png  \n",
            "  inflating: train/Scentless Mayweed/b75110745.png  \n",
            "  inflating: train/Scentless Mayweed/b75a19e14.png  \n",
            "  inflating: train/Scentless Mayweed/b7b392448.png  \n",
            "  inflating: train/Scentless Mayweed/b7b9ee150.png  \n",
            "  inflating: train/Scentless Mayweed/b7f695669.png  \n",
            "  inflating: train/Scentless Mayweed/b8664f705.png  \n",
            "  inflating: train/Scentless Mayweed/b8c3b0f4b.png  \n",
            "  inflating: train/Scentless Mayweed/b901006cb.png  \n",
            "  inflating: train/Scentless Mayweed/b931ffcc2.png  \n",
            "  inflating: train/Scentless Mayweed/b95608a3e.png  \n",
            "  inflating: train/Scentless Mayweed/b9fb44d3e.png  \n",
            "  inflating: train/Scentless Mayweed/ba1bc108d.png  \n",
            "  inflating: train/Scentless Mayweed/ba28e9c09.png  \n",
            "  inflating: train/Scentless Mayweed/ba30944b9.png  \n",
            "  inflating: train/Scentless Mayweed/bb82cef14.png  \n",
            "  inflating: train/Scentless Mayweed/bbd2ce539.png  \n",
            "  inflating: train/Scentless Mayweed/bc656f071.png  \n",
            "  inflating: train/Scentless Mayweed/bd745fff9.png  \n",
            "  inflating: train/Scentless Mayweed/bf1ef2197.png  \n",
            "  inflating: train/Scentless Mayweed/bf5217e5b.png  \n",
            "  inflating: train/Scentless Mayweed/c013688a9.png  \n",
            "  inflating: train/Scentless Mayweed/c0810829c.png  \n",
            "  inflating: train/Scentless Mayweed/c13cfa6ca.png  \n",
            "  inflating: train/Scentless Mayweed/c24269f00.png  \n",
            "  inflating: train/Scentless Mayweed/c2c5708be.png  \n",
            "  inflating: train/Scentless Mayweed/c4732c9bf.png  \n",
            "  inflating: train/Scentless Mayweed/c4cfb9893.png  \n",
            "  inflating: train/Scentless Mayweed/c53eb8d52.png  \n",
            "  inflating: train/Scentless Mayweed/c609814b6.png  \n",
            "  inflating: train/Scentless Mayweed/c6e95a334.png  \n",
            "  inflating: train/Scentless Mayweed/c87695e0e.png  \n",
            "  inflating: train/Scentless Mayweed/c895464a5.png  \n",
            "  inflating: train/Scentless Mayweed/c8c89b850.png  \n",
            "  inflating: train/Scentless Mayweed/cba4db693.png  \n",
            "  inflating: train/Scentless Mayweed/ccda21dc0.png  \n",
            "  inflating: train/Scentless Mayweed/cd9bc8c1f.png  \n",
            "  inflating: train/Scentless Mayweed/ce3bee65a.png  \n",
            "  inflating: train/Scentless Mayweed/d0af40a2d.png  \n",
            "  inflating: train/Scentless Mayweed/d1e775b97.png  \n",
            "  inflating: train/Scentless Mayweed/d1f4f2cd7.png  \n",
            "  inflating: train/Scentless Mayweed/d30507567.png  \n",
            "  inflating: train/Scentless Mayweed/d318c0f8d.png  \n",
            "  inflating: train/Scentless Mayweed/d48058bb8.png  \n",
            "  inflating: train/Scentless Mayweed/d512071bd.png  \n",
            "  inflating: train/Scentless Mayweed/d53f66132.png  \n",
            "  inflating: train/Scentless Mayweed/d554d9ad6.png  \n",
            "  inflating: train/Scentless Mayweed/d5785bd03.png  \n",
            "  inflating: train/Scentless Mayweed/d748c7307.png  \n",
            "  inflating: train/Scentless Mayweed/d76c8b573.png  \n",
            "  inflating: train/Scentless Mayweed/d898c90db.png  \n",
            "  inflating: train/Scentless Mayweed/d8dd45bb1.png  \n",
            "  inflating: train/Scentless Mayweed/d9a052c2a.png  \n",
            "  inflating: train/Scentless Mayweed/d9a8c6ed5.png  \n",
            "  inflating: train/Scentless Mayweed/d9ae7cf9d.png  \n",
            "  inflating: train/Scentless Mayweed/da762a730.png  \n",
            "  inflating: train/Scentless Mayweed/da97861ed.png  \n",
            "  inflating: train/Scentless Mayweed/da9acbb63.png  \n",
            "  inflating: train/Scentless Mayweed/dafec933b.png  \n",
            "  inflating: train/Scentless Mayweed/dbd132843.png  \n",
            "  inflating: train/Scentless Mayweed/dbe72b0cd.png  \n",
            "  inflating: train/Scentless Mayweed/dd1e759c0.png  \n",
            "  inflating: train/Scentless Mayweed/ddb46795d.png  \n",
            "  inflating: train/Scentless Mayweed/ddf258405.png  \n",
            "  inflating: train/Scentless Mayweed/de1097101.png  \n",
            "  inflating: train/Scentless Mayweed/de67fb3e7.png  \n",
            "  inflating: train/Scentless Mayweed/deb23c029.png  \n",
            "  inflating: train/Scentless Mayweed/df62ac910.png  \n",
            "  inflating: train/Scentless Mayweed/df6448756.png  \n",
            "  inflating: train/Scentless Mayweed/e07e7b55b.png  \n",
            "  inflating: train/Scentless Mayweed/e08d10fe1.png  \n",
            "  inflating: train/Scentless Mayweed/e1d303581.png  \n",
            "  inflating: train/Scentless Mayweed/e3b88c3c0.png  \n",
            "  inflating: train/Scentless Mayweed/e4f5d3cf6.png  \n",
            "  inflating: train/Scentless Mayweed/e5bb2e2cf.png  \n",
            "  inflating: train/Scentless Mayweed/e5dd11bac.png  \n",
            "  inflating: train/Scentless Mayweed/e61b6ff29.png  \n",
            "  inflating: train/Scentless Mayweed/e63e4f653.png  \n",
            "  inflating: train/Scentless Mayweed/e850905dd.png  \n",
            "  inflating: train/Scentless Mayweed/e9935ccc7.png  \n",
            "  inflating: train/Scentless Mayweed/e9dea2c74.png  \n",
            "  inflating: train/Scentless Mayweed/eb48a3979.png  \n",
            "  inflating: train/Scentless Mayweed/ec6e6cfdb.png  \n",
            "  inflating: train/Scentless Mayweed/ece245e91.png  \n",
            "  inflating: train/Scentless Mayweed/ed9a20b24.png  \n",
            "  inflating: train/Scentless Mayweed/edd5b2b13.png  \n",
            "  inflating: train/Scentless Mayweed/ee04c6528.png  \n",
            "  inflating: train/Scentless Mayweed/eef749129.png  \n",
            "  inflating: train/Scentless Mayweed/ef23c4d59.png  \n",
            "  inflating: train/Scentless Mayweed/ef6841bdb.png  \n",
            "  inflating: train/Scentless Mayweed/efdb1fd72.png  \n",
            "  inflating: train/Scentless Mayweed/f0c068cc9.png  \n",
            "  inflating: train/Scentless Mayweed/f0fcd1a29.png  \n",
            "  inflating: train/Scentless Mayweed/f12c028d0.png  \n",
            "  inflating: train/Scentless Mayweed/f20fced05.png  \n",
            "  inflating: train/Scentless Mayweed/f2e756037.png  \n",
            "  inflating: train/Scentless Mayweed/f3f60c384.png  \n",
            "  inflating: train/Scentless Mayweed/f44371b31.png  \n",
            "  inflating: train/Scentless Mayweed/f52937aa1.png  \n",
            "  inflating: train/Scentless Mayweed/f616c6831.png  \n",
            "  inflating: train/Scentless Mayweed/f7458fedd.png  \n",
            "  inflating: train/Scentless Mayweed/f76772c24.png  \n",
            "  inflating: train/Scentless Mayweed/f7bbaeed3.png  \n",
            "  inflating: train/Scentless Mayweed/f83626ff7.png  \n",
            "  inflating: train/Scentless Mayweed/f86cff347.png  \n",
            "  inflating: train/Scentless Mayweed/f8abf5974.png  \n",
            "  inflating: train/Scentless Mayweed/f8c96bd65.png  \n",
            "  inflating: train/Scentless Mayweed/fa32e1e7f.png  \n",
            "  inflating: train/Scentless Mayweed/fa440ea1b.png  \n",
            "  inflating: train/Scentless Mayweed/fa51a663c.png  \n",
            "  inflating: train/Scentless Mayweed/fa55f4be4.png  \n",
            "  inflating: train/Scentless Mayweed/fa57f2070.png  \n",
            "  inflating: train/Scentless Mayweed/fafcf1ece.png  \n",
            "  inflating: train/Scentless Mayweed/fb1ba1eb6.png  \n",
            "  inflating: train/Scentless Mayweed/fb95e0cfa.png  \n",
            "  inflating: train/Scentless Mayweed/fbeecf2d0.png  \n",
            "  inflating: train/Scentless Mayweed/fbfe32278.png  \n",
            "  inflating: train/Scentless Mayweed/fc3c746c3.png  \n",
            "  inflating: train/Scentless Mayweed/fd0ca2322.png  \n",
            "  inflating: train/Scentless Mayweed/fdc37b24c.png  \n",
            "  inflating: train/Scentless Mayweed/fdc682350.png  \n",
            "  inflating: train/Shepherds Purse/006a4d00d.png  \n",
            "  inflating: train/Shepherds Purse/00dd0d16a.png  \n",
            "  inflating: train/Shepherds Purse/01aef64d2.png  \n",
            "  inflating: train/Shepherds Purse/02afc3d7a.png  \n",
            "  inflating: train/Shepherds Purse/04e064c46.png  \n",
            "  inflating: train/Shepherds Purse/04fafa0d3.png  \n",
            "  inflating: train/Shepherds Purse/05ea7f987.png  \n",
            "  inflating: train/Shepherds Purse/07f867aa5.png  \n",
            "  inflating: train/Shepherds Purse/0ad0ef03e.png  \n",
            "  inflating: train/Shepherds Purse/0b1df6f5a.png  \n",
            "  inflating: train/Shepherds Purse/0bef4ae08.png  \n",
            "  inflating: train/Shepherds Purse/0ca928305.png  \n",
            "  inflating: train/Shepherds Purse/0ddeaa6e7.png  \n",
            "  inflating: train/Shepherds Purse/0e93f4d05.png  \n",
            "  inflating: train/Shepherds Purse/1022cc155.png  \n",
            "  inflating: train/Shepherds Purse/132d3d6e7.png  \n",
            "  inflating: train/Shepherds Purse/143203030.png  \n",
            "  inflating: train/Shepherds Purse/14db20a90.png  \n",
            "  inflating: train/Shepherds Purse/150ab985f.png  \n",
            "  inflating: train/Shepherds Purse/15ca828c8.png  \n",
            "  inflating: train/Shepherds Purse/179cedc9e.png  \n",
            "  inflating: train/Shepherds Purse/18bec843e.png  \n",
            "  inflating: train/Shepherds Purse/19fb8b2cc.png  \n",
            "  inflating: train/Shepherds Purse/1a067ec6e.png  \n",
            "  inflating: train/Shepherds Purse/1b547ac21.png  \n",
            "  inflating: train/Shepherds Purse/1c6a48d4f.png  \n",
            "  inflating: train/Shepherds Purse/1c95a2c57.png  \n",
            "  inflating: train/Shepherds Purse/1cf4ed969.png  \n",
            "  inflating: train/Shepherds Purse/1ea4479e9.png  \n",
            "  inflating: train/Shepherds Purse/20533da92.png  \n",
            "  inflating: train/Shepherds Purse/21cfeb62a.png  \n",
            "  inflating: train/Shepherds Purse/270bc051c.png  \n",
            "  inflating: train/Shepherds Purse/276fd5f88.png  \n",
            "  inflating: train/Shepherds Purse/28a237418.png  \n",
            "  inflating: train/Shepherds Purse/2926b17c0.png  \n",
            "  inflating: train/Shepherds Purse/2a3704b9a.png  \n",
            "  inflating: train/Shepherds Purse/2a5b49dc8.png  \n",
            "  inflating: train/Shepherds Purse/2bf8eac60.png  \n",
            "  inflating: train/Shepherds Purse/31abca20c.png  \n",
            "  inflating: train/Shepherds Purse/33010c8cb.png  \n",
            "  inflating: train/Shepherds Purse/33ea3207a.png  \n",
            "  inflating: train/Shepherds Purse/354c39125.png  \n",
            "  inflating: train/Shepherds Purse/3827a969f.png  \n",
            "  inflating: train/Shepherds Purse/3a98ad207.png  \n",
            "  inflating: train/Shepherds Purse/3d32f86f4.png  \n",
            "  inflating: train/Shepherds Purse/3e0ccc451.png  \n",
            "  inflating: train/Shepherds Purse/3e34f0e4d.png  \n",
            "  inflating: train/Shepherds Purse/3f408c295.png  \n",
            "  inflating: train/Shepherds Purse/3f6c41870.png  \n",
            "  inflating: train/Shepherds Purse/3fd1a2e97.png  \n",
            "  inflating: train/Shepherds Purse/416478ca2.png  \n",
            "  inflating: train/Shepherds Purse/42556bc41.png  \n",
            "  inflating: train/Shepherds Purse/42de1a9d5.png  \n",
            "  inflating: train/Shepherds Purse/46028d2d1.png  \n",
            "  inflating: train/Shepherds Purse/465fc11b5.png  \n",
            "  inflating: train/Shepherds Purse/46c449cdf.png  \n",
            "  inflating: train/Shepherds Purse/4e3c115a4.png  \n",
            "  inflating: train/Shepherds Purse/4e74df4b6.png  \n",
            "  inflating: train/Shepherds Purse/4eebeadb6.png  \n",
            "  inflating: train/Shepherds Purse/4f1efce37.png  \n",
            "  inflating: train/Shepherds Purse/500bc803a.png  \n",
            "  inflating: train/Shepherds Purse/500bd1f17.png  \n",
            "  inflating: train/Shepherds Purse/50ef0e765.png  \n",
            "  inflating: train/Shepherds Purse/52e1bd10e.png  \n",
            "  inflating: train/Shepherds Purse/52f0b365d.png  \n",
            "  inflating: train/Shepherds Purse/53a5c74cd.png  \n",
            "  inflating: train/Shepherds Purse/5512ca7ba.png  \n",
            "  inflating: train/Shepherds Purse/557c7a2aa.png  \n",
            "  inflating: train/Shepherds Purse/56cfc17ec.png  \n",
            "  inflating: train/Shepherds Purse/57c4a1666.png  \n",
            "  inflating: train/Shepherds Purse/57fe6d08a.png  \n",
            "  inflating: train/Shepherds Purse/589fb760c.png  \n",
            "  inflating: train/Shepherds Purse/5a3e2360a.png  \n",
            "  inflating: train/Shepherds Purse/5ec0b1607.png  \n",
            "  inflating: train/Shepherds Purse/5f125eee7.png  \n",
            "  inflating: train/Shepherds Purse/5f731b96c.png  \n",
            "  inflating: train/Shepherds Purse/5f87b1a93.png  \n",
            "  inflating: train/Shepherds Purse/5fec8215c.png  \n",
            "  inflating: train/Shepherds Purse/60ee96ab9.png  \n",
            "  inflating: train/Shepherds Purse/631cac357.png  \n",
            "  inflating: train/Shepherds Purse/6346a5813.png  \n",
            "  inflating: train/Shepherds Purse/63555c98b.png  \n",
            "  inflating: train/Shepherds Purse/638b6a858.png  \n",
            "  inflating: train/Shepherds Purse/63b3aba11.png  \n",
            "  inflating: train/Shepherds Purse/65241684b.png  \n",
            "  inflating: train/Shepherds Purse/657606617.png  \n",
            "  inflating: train/Shepherds Purse/65ab41503.png  \n",
            "  inflating: train/Shepherds Purse/670b97558.png  \n",
            "  inflating: train/Shepherds Purse/682d79dee.png  \n",
            "  inflating: train/Shepherds Purse/688da98a7.png  \n",
            "  inflating: train/Shepherds Purse/6ce2cfd88.png  \n",
            "  inflating: train/Shepherds Purse/6d92a3538.png  \n",
            "  inflating: train/Shepherds Purse/6fe09ced5.png  \n",
            "  inflating: train/Shepherds Purse/6fe76b379.png  \n",
            "  inflating: train/Shepherds Purse/6fe86aa3b.png  \n",
            "  inflating: train/Shepherds Purse/70aedd14d.png  \n",
            "  inflating: train/Shepherds Purse/718cee59f.png  \n",
            "  inflating: train/Shepherds Purse/72b292a3b.png  \n",
            "  inflating: train/Shepherds Purse/72f3f9101.png  \n",
            "  inflating: train/Shepherds Purse/7492af3d3.png  \n",
            "  inflating: train/Shepherds Purse/74bb3f0c1.png  \n",
            "  inflating: train/Shepherds Purse/74be44aef.png  \n",
            "  inflating: train/Shepherds Purse/74fd5da5b.png  \n",
            "  inflating: train/Shepherds Purse/759bf2b19.png  \n",
            "  inflating: train/Shepherds Purse/75d522cd3.png  \n",
            "  inflating: train/Shepherds Purse/77686f343.png  \n",
            "  inflating: train/Shepherds Purse/7a1c3e5b8.png  \n",
            "  inflating: train/Shepherds Purse/7a9a62dac.png  \n",
            "  inflating: train/Shepherds Purse/7d5f8aab4.png  \n",
            "  inflating: train/Shepherds Purse/7fe0c99d5.png  \n",
            "  inflating: train/Shepherds Purse/8056c3590.png  \n",
            "  inflating: train/Shepherds Purse/810110db6.png  \n",
            "  inflating: train/Shepherds Purse/8159eeb82.png  \n",
            "  inflating: train/Shepherds Purse/840fdb281.png  \n",
            "  inflating: train/Shepherds Purse/846e1f63b.png  \n",
            "  inflating: train/Shepherds Purse/847872e08.png  \n",
            "  inflating: train/Shepherds Purse/85c77d977.png  \n",
            "  inflating: train/Shepherds Purse/867bd5d63.png  \n",
            "  inflating: train/Shepherds Purse/87d1a75df.png  \n",
            "  inflating: train/Shepherds Purse/896a23ea3.png  \n",
            "  inflating: train/Shepherds Purse/8a3902c95.png  \n",
            "  inflating: train/Shepherds Purse/8bfd7bba7.png  \n",
            "  inflating: train/Shepherds Purse/8d9512864.png  \n",
            "  inflating: train/Shepherds Purse/8e1efae9e.png  \n",
            "  inflating: train/Shepherds Purse/8e3462539.png  \n",
            "  inflating: train/Shepherds Purse/8f72e841a.png  \n",
            "  inflating: train/Shepherds Purse/905aee3f2.png  \n",
            "  inflating: train/Shepherds Purse/9123349c5.png  \n",
            "  inflating: train/Shepherds Purse/917af97ab.png  \n",
            "  inflating: train/Shepherds Purse/9308e84bc.png  \n",
            "  inflating: train/Shepherds Purse/93d6a65a2.png  \n",
            "  inflating: train/Shepherds Purse/9451dd4d2.png  \n",
            "  inflating: train/Shepherds Purse/94adb2c3b.png  \n",
            "  inflating: train/Shepherds Purse/950c3e930.png  \n",
            "  inflating: train/Shepherds Purse/953ced7c6.png  \n",
            "  inflating: train/Shepherds Purse/95e89ddd3.png  \n",
            "  inflating: train/Shepherds Purse/9617b5025.png  \n",
            "  inflating: train/Shepherds Purse/97bfbdcf0.png  \n",
            "  inflating: train/Shepherds Purse/98431cfc6.png  \n",
            "  inflating: train/Shepherds Purse/995a8bd85.png  \n",
            "  inflating: train/Shepherds Purse/9ac112817.png  \n",
            "  inflating: train/Shepherds Purse/9db0aa841.png  \n",
            "  inflating: train/Shepherds Purse/a0ec33869.png  \n",
            "  inflating: train/Shepherds Purse/a13743637.png  \n",
            "  inflating: train/Shepherds Purse/a27721a9b.png  \n",
            "  inflating: train/Shepherds Purse/a2f172e0a.png  \n",
            "  inflating: train/Shepherds Purse/a4ae6ec8b.png  \n",
            "  inflating: train/Shepherds Purse/a538401ce.png  \n",
            "  inflating: train/Shepherds Purse/a59d03b99.png  \n",
            "  inflating: train/Shepherds Purse/a73168ffd.png  \n",
            "  inflating: train/Shepherds Purse/aad81b27b.png  \n",
            "  inflating: train/Shepherds Purse/ab3a8d4d6.png  \n",
            "  inflating: train/Shepherds Purse/adb2d81e7.png  \n",
            "  inflating: train/Shepherds Purse/aee19a2d1.png  \n",
            "  inflating: train/Shepherds Purse/afb08d410.png  \n",
            "  inflating: train/Shepherds Purse/b085af335.png  \n",
            "  inflating: train/Shepherds Purse/b30a50916.png  \n",
            "  inflating: train/Shepherds Purse/b3e5c949e.png  \n",
            "  inflating: train/Shepherds Purse/b41bee0f3.png  \n",
            "  inflating: train/Shepherds Purse/b54f52362.png  \n",
            "  inflating: train/Shepherds Purse/b5bc245d4.png  \n",
            "  inflating: train/Shepherds Purse/b886d79e6.png  \n",
            "  inflating: train/Shepherds Purse/ba3795245.png  \n",
            "  inflating: train/Shepherds Purse/ba4b5df66.png  \n",
            "  inflating: train/Shepherds Purse/bb8144729.png  \n",
            "  inflating: train/Shepherds Purse/bbb2fe0b4.png  \n",
            "  inflating: train/Shepherds Purse/bbeb86100.png  \n",
            "  inflating: train/Shepherds Purse/bcaf2f2dc.png  \n",
            "  inflating: train/Shepherds Purse/bd1e1acdd.png  \n",
            "  inflating: train/Shepherds Purse/be99eac09.png  \n",
            "  inflating: train/Shepherds Purse/bea1ec909.png  \n",
            "  inflating: train/Shepherds Purse/bfedca16c.png  \n",
            "  inflating: train/Shepherds Purse/c16206dca.png  \n",
            "  inflating: train/Shepherds Purse/c43abfa03.png  \n",
            "  inflating: train/Shepherds Purse/c7a718917.png  \n",
            "  inflating: train/Shepherds Purse/c819c3429.png  \n",
            "  inflating: train/Shepherds Purse/c9457398b.png  \n",
            "  inflating: train/Shepherds Purse/cc25cd1ad.png  \n",
            "  inflating: train/Shepherds Purse/cc2c012f7.png  \n",
            "  inflating: train/Shepherds Purse/cd3e9d61c.png  \n",
            "  inflating: train/Shepherds Purse/cd616827b.png  \n",
            "  inflating: train/Shepherds Purse/cf5ec9251.png  \n",
            "  inflating: train/Shepherds Purse/d26e5c8fc.png  \n",
            "  inflating: train/Shepherds Purse/d308ef21b.png  \n",
            "  inflating: train/Shepherds Purse/d30a460b4.png  \n",
            "  inflating: train/Shepherds Purse/d33d10a18.png  \n",
            "  inflating: train/Shepherds Purse/d4f0adef4.png  \n",
            "  inflating: train/Shepherds Purse/d62879538.png  \n",
            "  inflating: train/Shepherds Purse/d9f0623e1.png  \n",
            "  inflating: train/Shepherds Purse/da6621ac8.png  \n",
            "  inflating: train/Shepherds Purse/dbb602af5.png  \n",
            "  inflating: train/Shepherds Purse/dca1ac479.png  \n",
            "  inflating: train/Shepherds Purse/de0e770b2.png  \n",
            "  inflating: train/Shepherds Purse/de457c61d.png  \n",
            "  inflating: train/Shepherds Purse/de81a9d77.png  \n",
            "  inflating: train/Shepherds Purse/debcefa3f.png  \n",
            "  inflating: train/Shepherds Purse/df362ae68.png  \n",
            "  inflating: train/Shepherds Purse/df376f5c7.png  \n",
            "  inflating: train/Shepherds Purse/e0bc6c676.png  \n",
            "  inflating: train/Shepherds Purse/e19023937.png  \n",
            "  inflating: train/Shepherds Purse/e3d414b44.png  \n",
            "  inflating: train/Shepherds Purse/e41f1b997.png  \n",
            "  inflating: train/Shepherds Purse/e53eefec4.png  \n",
            "  inflating: train/Shepherds Purse/e724165c5.png  \n",
            "  inflating: train/Shepherds Purse/e9b5f28cd.png  \n",
            "  inflating: train/Shepherds Purse/e9eb110ce.png  \n",
            "  inflating: train/Shepherds Purse/eae41be4f.png  \n",
            "  inflating: train/Shepherds Purse/eb9bd2f4a.png  \n",
            "  inflating: train/Shepherds Purse/ecca0b65c.png  \n",
            "  inflating: train/Shepherds Purse/ed2fe7bb9.png  \n",
            "  inflating: train/Shepherds Purse/ee4100782.png  \n",
            "  inflating: train/Shepherds Purse/f00311fd8.png  \n",
            "  inflating: train/Shepherds Purse/f0127f70d.png  \n",
            "  inflating: train/Shepherds Purse/f19c334fe.png  \n",
            "  inflating: train/Shepherds Purse/f2fb09190.png  \n",
            "  inflating: train/Shepherds Purse/f35608a5c.png  \n",
            "  inflating: train/Shepherds Purse/f38565506.png  \n",
            "  inflating: train/Shepherds Purse/f428f843f.png  \n",
            "  inflating: train/Shepherds Purse/f4dc7d0ad.png  \n",
            "  inflating: train/Shepherds Purse/f900b7684.png  \n",
            "  inflating: train/Shepherds Purse/f90b0ca05.png  \n",
            "  inflating: train/Shepherds Purse/f927e469b.png  \n",
            "  inflating: train/Shepherds Purse/f981da6df.png  \n",
            "  inflating: train/Shepherds Purse/fbc21a114.png  \n",
            "  inflating: train/Shepherds Purse/fd0d400b6.png  \n",
            "  inflating: train/Shepherds Purse/fdd0a59e3.png  \n",
            "  inflating: train/Shepherds Purse/fe0f1b30d.png  \n",
            "  inflating: train/Shepherds Purse/fe2bf1883.png  \n",
            "  inflating: train/Shepherds Purse/fe3bcb925.png  \n",
            "  inflating: train/Shepherds Purse/ff46c64b4.png  \n",
            "  inflating: train/Shepherds Purse/ffb54800c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/003402ea0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/00cc58829.png  \n",
            "  inflating: train/Small-flowered Cranesbill/00d030ea0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/00e049fe8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/018c28574.png  \n",
            "  inflating: train/Small-flowered Cranesbill/01e18fd1a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0268ecbe2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/027f505f5.png  \n",
            "  inflating: train/Small-flowered Cranesbill/028329f45.png  \n",
            "  inflating: train/Small-flowered Cranesbill/04559c69c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/055922489.png  \n",
            "  inflating: train/Small-flowered Cranesbill/05598e057.png  \n",
            "  inflating: train/Small-flowered Cranesbill/05e5b5cb6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/06a64ac47.png  \n",
            "  inflating: train/Small-flowered Cranesbill/071907a13.png  \n",
            "  inflating: train/Small-flowered Cranesbill/076b550b6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/085debc4e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0958bdb64.png  \n",
            "  inflating: train/Small-flowered Cranesbill/09be59300.png  \n",
            "  inflating: train/Small-flowered Cranesbill/09d34fe5b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0a2ff1911.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0b26e2d09.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0cae486ac.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0d85a4e16.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0dc692377.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0df57701f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0e7f05ec0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0f2dbf503.png  \n",
            "  inflating: train/Small-flowered Cranesbill/0ff5ac7e1.png  \n",
            "  inflating: train/Small-flowered Cranesbill/10496e1d7.png  \n",
            "  inflating: train/Small-flowered Cranesbill/108976349.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1188ef167.png  \n",
            "  inflating: train/Small-flowered Cranesbill/12ba9357a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/137126847.png  \n",
            "  inflating: train/Small-flowered Cranesbill/13729b12c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/13b87795c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/143596072.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1476d787e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/149f18d8c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/167cf0978.png  \n",
            "  inflating: train/Small-flowered Cranesbill/16ecc632f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/17c80c4fd.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1816ac2d6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/188dcbe7b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/18a460203.png  \n",
            "  inflating: train/Small-flowered Cranesbill/19ae90c5c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/19f14f508.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1a2132c16.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1a73adfd9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1b7110883.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1b71148aa.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1b89f07b8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1c41c5fd4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1da91c4c6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1de219f93.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1ea090972.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1f14ed265.png  \n",
            "  inflating: train/Small-flowered Cranesbill/1f916d537.png  \n",
            "  inflating: train/Small-flowered Cranesbill/21cb65ed2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/21f0b515d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/21f880735.png  \n",
            "  inflating: train/Small-flowered Cranesbill/22429fb19.png  \n",
            "  inflating: train/Small-flowered Cranesbill/23125a26e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/24df8bcfe.png  \n",
            "  inflating: train/Small-flowered Cranesbill/25a9d7da2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/26c48a815.png  \n",
            "  inflating: train/Small-flowered Cranesbill/273fdc7f5.png  \n",
            "  inflating: train/Small-flowered Cranesbill/276bb8993.png  \n",
            "  inflating: train/Small-flowered Cranesbill/27d658aea.png  \n",
            "  inflating: train/Small-flowered Cranesbill/27d971fd0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/28277f807.png  \n",
            "  inflating: train/Small-flowered Cranesbill/28da4b5c3.png  \n",
            "  inflating: train/Small-flowered Cranesbill/28f8c7f43.png  \n",
            "  inflating: train/Small-flowered Cranesbill/29926ea63.png  \n",
            "  inflating: train/Small-flowered Cranesbill/29acf70e6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/29c3df915.png  \n",
            "  inflating: train/Small-flowered Cranesbill/29ed37400.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2a1966ff8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2ab0c21fb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2acee6217.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2b03e0abc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2bf465881.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2bfbed251.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2d2397456.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2d70e8f87.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2db36f1a0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2e03bc81d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2e1dc6e23.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2ed7d6a89.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2ed82c948.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2f83f3ebd.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2fa64903c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/2fa8935f8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3008533bc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/301edf06a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/30e7d6783.png  \n",
            "  inflating: train/Small-flowered Cranesbill/317c8d232.png  \n",
            "  inflating: train/Small-flowered Cranesbill/32c2487ad.png  \n",
            "  inflating: train/Small-flowered Cranesbill/33253201f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/33c5d5c61.png  \n",
            "  inflating: train/Small-flowered Cranesbill/33fa4d48a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3431fcef9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/344329c57.png  \n",
            "  inflating: train/Small-flowered Cranesbill/34a7bd232.png  \n",
            "  inflating: train/Small-flowered Cranesbill/34cdf6c71.png  \n",
            "  inflating: train/Small-flowered Cranesbill/35e257ee6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3670df417.png  \n",
            "  inflating: train/Small-flowered Cranesbill/375e964fd.png  \n",
            "  inflating: train/Small-flowered Cranesbill/38080b297.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3810c2e11.png  \n",
            "  inflating: train/Small-flowered Cranesbill/39070241e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3993a9c5c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/39f6f70e2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3ad4ad205.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3aebd50f0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3b0100994.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3b27d30a3.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3b3ace0ee.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3c6f4fe90.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3cdca6a6c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3ce10885f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3d04b2a31.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3e2d7bdea.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3f9dd7d18.png  \n",
            "  inflating: train/Small-flowered Cranesbill/3fb70e8c5.png  \n",
            "  inflating: train/Small-flowered Cranesbill/400479897.png  \n",
            "  inflating: train/Small-flowered Cranesbill/420457fe9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/42d2369a7.png  \n",
            "  inflating: train/Small-flowered Cranesbill/42db489c6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/436e5809f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/43ffe7711.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4430933ca.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4501821c3.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4507d5e15.png  \n",
            "  inflating: train/Small-flowered Cranesbill/460ac5645.png  \n",
            "  inflating: train/Small-flowered Cranesbill/464dc6968.png  \n",
            "  inflating: train/Small-flowered Cranesbill/464e4863b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4656b881d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/465bff6eb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/46b54b502.png  \n",
            "  inflating: train/Small-flowered Cranesbill/46c4aeda2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4731cb066.png  \n",
            "  inflating: train/Small-flowered Cranesbill/475f43d71.png  \n",
            "  inflating: train/Small-flowered Cranesbill/47aa8024a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/47c9b5639.png  \n",
            "  inflating: train/Small-flowered Cranesbill/487599b7f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/498878207.png  \n",
            "  inflating: train/Small-flowered Cranesbill/499180403.png  \n",
            "  inflating: train/Small-flowered Cranesbill/49aa0022d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4a2f80f95.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4a786b0d4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4a99a5aac.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4aace5bc4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4b62b3e51.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4bc35381d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4be536f13.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4c21d89be.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4c8af2ed3.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4daedff7d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4ef7552f4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/4f0970770.png  \n",
            "  inflating: train/Small-flowered Cranesbill/50a132243.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5178de55d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/517aa9aec.png  \n",
            "  inflating: train/Small-flowered Cranesbill/51846ad2b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/51b943178.png  \n",
            "  inflating: train/Small-flowered Cranesbill/523dae399.png  \n",
            "  inflating: train/Small-flowered Cranesbill/524b4014b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/525ba66d4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/532679a1a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/55c8528fe.png  \n",
            "  inflating: train/Small-flowered Cranesbill/55f1cad85.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5861480ff.png  \n",
            "  inflating: train/Small-flowered Cranesbill/58e22d25a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5a2274460.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5a4584b98.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5b0a87abf.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5b7582e22.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5d1f49b75.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5da2c0d45.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5dbc8a7d9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5e5e9be4e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5e977067e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/5fa533084.png  \n",
            "  inflating: train/Small-flowered Cranesbill/605478787.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6118042fe.png  \n",
            "  inflating: train/Small-flowered Cranesbill/611b0a0ef.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6226031d8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/62e0225ab.png  \n",
            "  inflating: train/Small-flowered Cranesbill/635dc514d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/63e8b0fad.png  \n",
            "  inflating: train/Small-flowered Cranesbill/63ede4ce5.png  \n",
            "  inflating: train/Small-flowered Cranesbill/643b99d1d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/64787c7cb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/65311995c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/655b9fa57.png  \n",
            "  inflating: train/Small-flowered Cranesbill/65d04573e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/663848c02.png  \n",
            "  inflating: train/Small-flowered Cranesbill/665a286fb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/66689513f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/66a4be0df.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6749e8eec.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6751cef57.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6793f76c0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/688b5551d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/68ac98ab6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/691b35920.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6af7caae2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6b666de44.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6b950f5f0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6c03f3f02.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6c7043cc5.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6c713c23b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6c7938a6c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6c7e1ebc2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6d1b9bb16.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6d7fa83ff.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6e37cdeef.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6efe11b4b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6f0082a09.png  \n",
            "  inflating: train/Small-flowered Cranesbill/6f29a7ddb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/70aa9e547.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7129784a9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/72243ee0c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/726ae59a2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/73e994e5e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7405aee1b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/741dc05da.png  \n",
            "  inflating: train/Small-flowered Cranesbill/749f91a6f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/74d9ab3c6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/75107b8b4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/758271672.png  \n",
            "  inflating: train/Small-flowered Cranesbill/75ac0573e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/75ae1bb94.png  \n",
            "  inflating: train/Small-flowered Cranesbill/762fc13b0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/76b2d6bf8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/76e00aac1.png  \n",
            "  inflating: train/Small-flowered Cranesbill/77add63eb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/782fcd40d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/78730a816.png  \n",
            "  inflating: train/Small-flowered Cranesbill/78757ce29.png  \n",
            "  inflating: train/Small-flowered Cranesbill/789711110.png  \n",
            "  inflating: train/Small-flowered Cranesbill/79158945e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/79cc6a8e8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/79cc9641b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7a07efc0c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7adff29ac.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7be34ad47.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7c3ca61ff.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7d4bd6fe6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7d89d7a87.png  \n",
            "  inflating: train/Small-flowered Cranesbill/7e8c89d30.png  \n",
            "  inflating: train/Small-flowered Cranesbill/801c3e668.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8023cf2e2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8032a7f05.png  \n",
            "  inflating: train/Small-flowered Cranesbill/81550ffb9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/816965c05.png  \n",
            "  inflating: train/Small-flowered Cranesbill/81bc7934b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/81d8fc031.png  \n",
            "  inflating: train/Small-flowered Cranesbill/82e3f3c74.png  \n",
            "  inflating: train/Small-flowered Cranesbill/840ca8a36.png  \n",
            "  inflating: train/Small-flowered Cranesbill/845a0ec2c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/847ca1669.png  \n",
            "  inflating: train/Small-flowered Cranesbill/84b27cfd6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/84cf76362.png  \n",
            "  inflating: train/Small-flowered Cranesbill/84e69eaf0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/858547ab8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/85f1d46e7.png  \n",
            "  inflating: train/Small-flowered Cranesbill/85f33fcee.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8668f7d12.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8674eacd1.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8692526d1.png  \n",
            "  inflating: train/Small-flowered Cranesbill/869c32954.png  \n",
            "  inflating: train/Small-flowered Cranesbill/86a747d04.png  \n",
            "  inflating: train/Small-flowered Cranesbill/87429079a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/876391c4f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8788bc3cb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/87cafd4dc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/87e65b6d9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/882227162.png  \n",
            "  inflating: train/Small-flowered Cranesbill/88e24e307.png  \n",
            "  inflating: train/Small-flowered Cranesbill/89825ecdc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/89d260299.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8d3505642.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8d4967856.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8dc954c08.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8e0da1b17.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8e1a744c8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8e9cdb545.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8ee5d6835.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8f463eb5c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/8fd09f9a8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/90246ebf3.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9096d82dc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/909da3410.png  \n",
            "  inflating: train/Small-flowered Cranesbill/90e1ab9bd.png  \n",
            "  inflating: train/Small-flowered Cranesbill/914aa19ae.png  \n",
            "  inflating: train/Small-flowered Cranesbill/91611749b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/91cd35be2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/91fa6a4e8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/922f5f1d1.png  \n",
            "  inflating: train/Small-flowered Cranesbill/92888379a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/931fb856a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/939d7da03.png  \n",
            "  inflating: train/Small-flowered Cranesbill/967163f07.png  \n",
            "  inflating: train/Small-flowered Cranesbill/968f9d619.png  \n",
            "  inflating: train/Small-flowered Cranesbill/96bdc048d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/97aee87ec.png  \n",
            "  inflating: train/Small-flowered Cranesbill/982e2f7be.png  \n",
            "  inflating: train/Small-flowered Cranesbill/98371ada7.png  \n",
            "  inflating: train/Small-flowered Cranesbill/984f52a86.png  \n",
            "  inflating: train/Small-flowered Cranesbill/985e72cfb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9ab1673ae.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9b569f4c2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9bd832240.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9ca3c5573.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9dd370e79.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9e6f71239.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9ecd81c53.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9eed2c8f4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9f4b3c440.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9f87371b2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9fb65eb5d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/9fe156262.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a06803452.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a092428cd.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a0b1ab201.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a11144563.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a134e87e9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a17451743.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a1ab74926.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a1db4ec9e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a1f60fe38.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a2ce7fbfe.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a38a45d9e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a3d92fb5a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a40e87bcf.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a4f26b005.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a6358bb36.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a666b103c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a6c5ada78.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a7864d8f3.png  \n",
            "  inflating: train/Small-flowered Cranesbill/a95fe2665.png  \n",
            "  inflating: train/Small-flowered Cranesbill/aa080431d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/aa6bc0612.png  \n",
            "  inflating: train/Small-flowered Cranesbill/abf3ee5df.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ac4df2b40.png  \n",
            "  inflating: train/Small-flowered Cranesbill/acbd8f766.png  \n",
            "  inflating: train/Small-flowered Cranesbill/acbf61855.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ad8b28544.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ae69bd33e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ae9b5d027.png  \n",
            "  inflating: train/Small-flowered Cranesbill/aeacc554f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/af181cd9c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/af8a6280c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/afee78134.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b0de6838e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b13a3aa91.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b14b1357f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b168dd77c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b1878d0bc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b1d3ecd6c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b20d166dc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b48d3c6cd.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b4b94663f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b4cf2660d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b53a00a5a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b53ebbdc8.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b5454be5c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b5785cf1a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b7a85bf77.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b8ac37dcd.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b8b6da469.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b94448246.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b95a6d369.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b9a3e820f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/b9d09d85e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ba08ca84c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/baff578af.png  \n",
            "  inflating: train/Small-flowered Cranesbill/bc924a3ff.png  \n",
            "  inflating: train/Small-flowered Cranesbill/bd1e9e013.png  \n",
            "  inflating: train/Small-flowered Cranesbill/bf60251b5.png  \n",
            "  inflating: train/Small-flowered Cranesbill/bfd134e4e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c046b4f90.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c203f8b60.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c2143cbb6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c2922dd4b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c2f642125.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c3510d0cc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c38a4c31e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c3eb8fe23.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c423a70c3.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c5eba9d04.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c629f1db5.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c6ffe3328.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c7344c2e3.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c74732b65.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c77195c15.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c97427ee2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/c9fa2e553.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ca507e99a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cae4dac21.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cae73f57f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cb15fb345.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cb65d8e94.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cb6988242.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cbd4362a2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cbe03c877.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ccf00806f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cd47ff628.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cd4bcc457.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ceaf23106.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cf60b7687.png  \n",
            "  inflating: train/Small-flowered Cranesbill/cf9ef8c5e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d0300ef58.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d03c0b230.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d1476bc9f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d1edb5e45.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d21bfbaa7.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d394d353a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d3e90a2e9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d4887c996.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d492da1ba.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d50e5c7e4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d52d13ac9.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d6515ed79.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d6e8dd4ac.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d7314844c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d7ecdaabe.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d8e9efd02.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d9f79c9ce.png  \n",
            "  inflating: train/Small-flowered Cranesbill/d9ff280f1.png  \n",
            "  inflating: train/Small-flowered Cranesbill/da462b870.png  \n",
            "  inflating: train/Small-flowered Cranesbill/db0262690.png  \n",
            "  inflating: train/Small-flowered Cranesbill/db3085018.png  \n",
            "  inflating: train/Small-flowered Cranesbill/db360e6d2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/db3847e4e.png  \n",
            "  inflating: train/Small-flowered Cranesbill/dc7dd83eb.png  \n",
            "  inflating: train/Small-flowered Cranesbill/de59a590a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/de6fa0b6c.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e02e72548.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e0a816c0b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e0ed50f3a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e11916073.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e19c8b666.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e1be6ab61.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e29577320.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e4fd7e115.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e50666c1f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e54767587.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e5c038840.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e682d2f54.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e70d986b6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e787fda09.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e82c4271a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e8a99d459.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e909c5348.png  \n",
            "  inflating: train/Small-flowered Cranesbill/e910500a6.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ea257f436.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ea39d1bf1.png  \n",
            "  inflating: train/Small-flowered Cranesbill/eb8f098fa.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ebbadc4c2.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ec4ad48f5.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ec87dcbbf.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ece5a62ad.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ecf58ad3a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/eea92edcc.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ef72fb02b.png  \n",
            "  inflating: train/Small-flowered Cranesbill/efbf3750d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f012e8350.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f1196e55f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f12648089.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f253f407d.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f263c0f9f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f2d762192.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f2f975384.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f43489876.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f467bd532.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f46902555.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f57d6d212.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f60a53bf4.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f67796f86.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f6fa5e429.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f71a65a08.png  \n",
            "  inflating: train/Small-flowered Cranesbill/f8d9d8885.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fad7f81b7.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fae94cdba.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fb4522b41.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fb60fbf1a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fb73e5716.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fb894c60f.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fbecbb3b7.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fc0a3eeb0.png  \n",
            "  inflating: train/Small-flowered Cranesbill/fe623537a.png  \n",
            "  inflating: train/Small-flowered Cranesbill/feea57708.png  \n",
            "  inflating: train/Small-flowered Cranesbill/ff07b2015.png  \n",
            "  inflating: train/Sugar beet/0026b7a30.png  \n",
            "  inflating: train/Sugar beet/00626e3be.png  \n",
            "  inflating: train/Sugar beet/008753052.png  \n",
            "  inflating: train/Sugar beet/00acffa82.png  \n",
            "  inflating: train/Sugar beet/019e2ca3b.png  \n",
            "  inflating: train/Sugar beet/01f145914.png  \n",
            "  inflating: train/Sugar beet/02460681c.png  \n",
            "  inflating: train/Sugar beet/04c979351.png  \n",
            "  inflating: train/Sugar beet/05f87cff9.png  \n",
            "  inflating: train/Sugar beet/06a859dfd.png  \n",
            "  inflating: train/Sugar beet/071c8beda.png  \n",
            "  inflating: train/Sugar beet/0838d0a77.png  \n",
            "  inflating: train/Sugar beet/089859249.png  \n",
            "  inflating: train/Sugar beet/095ba8b9d.png  \n",
            "  inflating: train/Sugar beet/0ad9c629b.png  \n",
            "  inflating: train/Sugar beet/0af59d58a.png  \n",
            "  inflating: train/Sugar beet/0b992e297.png  \n",
            "  inflating: train/Sugar beet/0cc0786cc.png  \n",
            "  inflating: train/Sugar beet/0dfc2684f.png  \n",
            "  inflating: train/Sugar beet/0f07c34e6.png  \n",
            "  inflating: train/Sugar beet/0f17ed2f7.png  \n",
            "  inflating: train/Sugar beet/0f1ac7c29.png  \n",
            "  inflating: train/Sugar beet/0f30a59c2.png  \n",
            "  inflating: train/Sugar beet/0f8a1f158.png  \n",
            "  inflating: train/Sugar beet/100622e89.png  \n",
            "  inflating: train/Sugar beet/102be9d35.png  \n",
            "  inflating: train/Sugar beet/108770d27.png  \n",
            "  inflating: train/Sugar beet/10f06d712.png  \n",
            "  inflating: train/Sugar beet/11d370bcc.png  \n",
            "  inflating: train/Sugar beet/1290426f8.png  \n",
            "  inflating: train/Sugar beet/12c104178.png  \n",
            "  inflating: train/Sugar beet/1347ac5ac.png  \n",
            "  inflating: train/Sugar beet/134d1d001.png  \n",
            "  inflating: train/Sugar beet/136bbcecf.png  \n",
            "  inflating: train/Sugar beet/13726d79d.png  \n",
            "  inflating: train/Sugar beet/138aec2c1.png  \n",
            "  inflating: train/Sugar beet/13e1f2dfd.png  \n",
            "  inflating: train/Sugar beet/17bcc04ba.png  \n",
            "  inflating: train/Sugar beet/1835ba646.png  \n",
            "  inflating: train/Sugar beet/18b3747da.png  \n",
            "  inflating: train/Sugar beet/18c0f6b8f.png  \n",
            "  inflating: train/Sugar beet/1abf3660d.png  \n",
            "  inflating: train/Sugar beet/1bdfd2206.png  \n",
            "  inflating: train/Sugar beet/1cd8ef887.png  \n",
            "  inflating: train/Sugar beet/1d314afce.png  \n",
            "  inflating: train/Sugar beet/1dd4326a8.png  \n",
            "  inflating: train/Sugar beet/1e44da887.png  \n",
            "  inflating: train/Sugar beet/1ec9ab1b8.png  \n",
            "  inflating: train/Sugar beet/1f12ab6b7.png  \n",
            "  inflating: train/Sugar beet/1fae880b5.png  \n",
            "  inflating: train/Sugar beet/201cdcf53.png  \n",
            "  inflating: train/Sugar beet/21114a782.png  \n",
            "  inflating: train/Sugar beet/218f3938f.png  \n",
            "  inflating: train/Sugar beet/222d5c406.png  \n",
            "  inflating: train/Sugar beet/22d2590d3.png  \n",
            "  inflating: train/Sugar beet/25d60e020.png  \n",
            "  inflating: train/Sugar beet/26eff1e2a.png  \n",
            "  inflating: train/Sugar beet/28acd4b0b.png  \n",
            "  inflating: train/Sugar beet/292e00e4d.png  \n",
            "  inflating: train/Sugar beet/2933913e4.png  \n",
            "  inflating: train/Sugar beet/2982d5c21.png  \n",
            "  inflating: train/Sugar beet/29a0e6bf9.png  \n",
            "  inflating: train/Sugar beet/29bbab238.png  \n",
            "  inflating: train/Sugar beet/29d56fd75.png  \n",
            "  inflating: train/Sugar beet/29f0864b1.png  \n",
            "  inflating: train/Sugar beet/2b51be287.png  \n",
            "  inflating: train/Sugar beet/2c2a8d5a4.png  \n",
            "  inflating: train/Sugar beet/2cb41be46.png  \n",
            "  inflating: train/Sugar beet/2d0c2c01e.png  \n",
            "  inflating: train/Sugar beet/2e4cafdc8.png  \n",
            "  inflating: train/Sugar beet/2ed22fddd.png  \n",
            "  inflating: train/Sugar beet/2efa8dc31.png  \n",
            "  inflating: train/Sugar beet/302d3c176.png  \n",
            "  inflating: train/Sugar beet/303f718cc.png  \n",
            "  inflating: train/Sugar beet/3041d1470.png  \n",
            "  inflating: train/Sugar beet/30ff35160.png  \n",
            "  inflating: train/Sugar beet/3109caad6.png  \n",
            "  inflating: train/Sugar beet/3112fb5d7.png  \n",
            "  inflating: train/Sugar beet/31e905346.png  \n",
            "  inflating: train/Sugar beet/3217c1887.png  \n",
            "  inflating: train/Sugar beet/33ddbf2cc.png  \n",
            "  inflating: train/Sugar beet/341823a91.png  \n",
            "  inflating: train/Sugar beet/344371cbf.png  \n",
            "  inflating: train/Sugar beet/34651cf34.png  \n",
            "  inflating: train/Sugar beet/35bccf6c9.png  \n",
            "  inflating: train/Sugar beet/3767ec44e.png  \n",
            "  inflating: train/Sugar beet/3905347ff.png  \n",
            "  inflating: train/Sugar beet/399e9c2a3.png  \n",
            "  inflating: train/Sugar beet/39d5d935a.png  \n",
            "  inflating: train/Sugar beet/3b3c71f04.png  \n",
            "  inflating: train/Sugar beet/3b5b81454.png  \n",
            "  inflating: train/Sugar beet/3e9d61115.png  \n",
            "  inflating: train/Sugar beet/3f6425004.png  \n",
            "  inflating: train/Sugar beet/3fc131cb9.png  \n",
            "  inflating: train/Sugar beet/3fde34b6c.png  \n",
            "  inflating: train/Sugar beet/4204cd689.png  \n",
            "  inflating: train/Sugar beet/42df035bc.png  \n",
            "  inflating: train/Sugar beet/436368fcf.png  \n",
            "  inflating: train/Sugar beet/449dff6f3.png  \n",
            "  inflating: train/Sugar beet/4729baa43.png  \n",
            "  inflating: train/Sugar beet/48011ba03.png  \n",
            "  inflating: train/Sugar beet/4839e4577.png  \n",
            "  inflating: train/Sugar beet/494e9a680.png  \n",
            "  inflating: train/Sugar beet/4b8daeba3.png  \n",
            "  inflating: train/Sugar beet/4c443aaf8.png  \n",
            "  inflating: train/Sugar beet/4c81ff749.png  \n",
            "  inflating: train/Sugar beet/4c9ea912c.png  \n",
            "  inflating: train/Sugar beet/4d7f9c850.png  \n",
            "  inflating: train/Sugar beet/4dcd59e5b.png  \n",
            "  inflating: train/Sugar beet/4dfe0db2c.png  \n",
            "  inflating: train/Sugar beet/4e8f9790e.png  \n",
            "  inflating: train/Sugar beet/4e96e4f63.png  \n",
            "  inflating: train/Sugar beet/4eb03def0.png  \n",
            "  inflating: train/Sugar beet/4f1a211d3.png  \n",
            "  inflating: train/Sugar beet/4f63858d7.png  \n",
            "  inflating: train/Sugar beet/4fd1fe7ac.png  \n",
            "  inflating: train/Sugar beet/5121d936d.png  \n",
            "  inflating: train/Sugar beet/5255463b3.png  \n",
            "  inflating: train/Sugar beet/5374f4b77.png  \n",
            "  inflating: train/Sugar beet/54f1ef55a.png  \n",
            "  inflating: train/Sugar beet/5582aca59.png  \n",
            "  inflating: train/Sugar beet/564132c99.png  \n",
            "  inflating: train/Sugar beet/56426cc09.png  \n",
            "  inflating: train/Sugar beet/568d1a95b.png  \n",
            "  inflating: train/Sugar beet/56a7b34cf.png  \n",
            "  inflating: train/Sugar beet/5780239a1.png  \n",
            "  inflating: train/Sugar beet/57ebf5927.png  \n",
            "  inflating: train/Sugar beet/586f43dd2.png  \n",
            "  inflating: train/Sugar beet/58debe1da.png  \n",
            "  inflating: train/Sugar beet/594c4d5b0.png  \n",
            "  inflating: train/Sugar beet/59ca13fa5.png  \n",
            "  inflating: train/Sugar beet/5bb9cb07b.png  \n",
            "  inflating: train/Sugar beet/5be7ccae6.png  \n",
            "  inflating: train/Sugar beet/5de141dee.png  \n",
            "  inflating: train/Sugar beet/5df190ffb.png  \n",
            "  inflating: train/Sugar beet/5e10b3707.png  \n",
            "  inflating: train/Sugar beet/5f69d5659.png  \n",
            "  inflating: train/Sugar beet/5ffc8607e.png  \n",
            "  inflating: train/Sugar beet/603618ce8.png  \n",
            "  inflating: train/Sugar beet/6085e20a5.png  \n",
            "  inflating: train/Sugar beet/60884bb5f.png  \n",
            "  inflating: train/Sugar beet/6090033e9.png  \n",
            "  inflating: train/Sugar beet/60bdea399.png  \n",
            "  inflating: train/Sugar beet/620a4a35a.png  \n",
            "  inflating: train/Sugar beet/64e9d6654.png  \n",
            "  inflating: train/Sugar beet/657ba211d.png  \n",
            "  inflating: train/Sugar beet/657feb24d.png  \n",
            "  inflating: train/Sugar beet/65e78e25b.png  \n",
            "  inflating: train/Sugar beet/66bd9c1bf.png  \n",
            "  inflating: train/Sugar beet/67054ec9d.png  \n",
            "  inflating: train/Sugar beet/6724b064e.png  \n",
            "  inflating: train/Sugar beet/6835112e5.png  \n",
            "  inflating: train/Sugar beet/6a97c84ac.png  \n",
            "  inflating: train/Sugar beet/6ae5f4197.png  \n",
            "  inflating: train/Sugar beet/6bb2545d9.png  \n",
            "  inflating: train/Sugar beet/6cd16cd21.png  \n",
            "  inflating: train/Sugar beet/6d18daabd.png  \n",
            "  inflating: train/Sugar beet/6d579671c.png  \n",
            "  inflating: train/Sugar beet/6d623072a.png  \n",
            "  inflating: train/Sugar beet/6d63eb98f.png  \n",
            "  inflating: train/Sugar beet/6d99bec4e.png  \n",
            "  inflating: train/Sugar beet/6ed387106.png  \n",
            "  inflating: train/Sugar beet/6f0536b55.png  \n",
            "  inflating: train/Sugar beet/6fe5df5b7.png  \n",
            "  inflating: train/Sugar beet/700889031.png  \n",
            "  inflating: train/Sugar beet/702261484.png  \n",
            "  inflating: train/Sugar beet/7162a91f1.png  \n",
            "  inflating: train/Sugar beet/71b1b6718.png  \n",
            "  inflating: train/Sugar beet/71d15af49.png  \n",
            "  inflating: train/Sugar beet/7278694d0.png  \n",
            "  inflating: train/Sugar beet/728642410.png  \n",
            "  inflating: train/Sugar beet/73d86199e.png  \n",
            "  inflating: train/Sugar beet/748fd9df6.png  \n",
            "  inflating: train/Sugar beet/754cc58bf.png  \n",
            "  inflating: train/Sugar beet/759e4b4eb.png  \n",
            "  inflating: train/Sugar beet/77df9b3f4.png  \n",
            "  inflating: train/Sugar beet/7801b889c.png  \n",
            "  inflating: train/Sugar beet/78654e936.png  \n",
            "  inflating: train/Sugar beet/7b04759cc.png  \n",
            "  inflating: train/Sugar beet/7b4682569.png  \n",
            "  inflating: train/Sugar beet/7ba30d202.png  \n",
            "  inflating: train/Sugar beet/7c05c4f0e.png  \n",
            "  inflating: train/Sugar beet/7c46a4746.png  \n",
            "  inflating: train/Sugar beet/7c572a30a.png  \n",
            "  inflating: train/Sugar beet/7d72cc9ea.png  \n",
            "  inflating: train/Sugar beet/7dcdd0825.png  \n",
            "  inflating: train/Sugar beet/80ca6a2d4.png  \n",
            "  inflating: train/Sugar beet/8129d97cd.png  \n",
            "  inflating: train/Sugar beet/815cd19f5.png  \n",
            "  inflating: train/Sugar beet/817abcd36.png  \n",
            "  inflating: train/Sugar beet/819c6ac1d.png  \n",
            "  inflating: train/Sugar beet/823b197a3.png  \n",
            "  inflating: train/Sugar beet/82b9d0a49.png  \n",
            "  inflating: train/Sugar beet/8479c6a25.png  \n",
            "  inflating: train/Sugar beet/84d42e202.png  \n",
            "  inflating: train/Sugar beet/85f8f5ae1.png  \n",
            "  inflating: train/Sugar beet/86054cfe8.png  \n",
            "  inflating: train/Sugar beet/866130efe.png  \n",
            "  inflating: train/Sugar beet/86a304fad.png  \n",
            "  inflating: train/Sugar beet/8723bd1e8.png  \n",
            "  inflating: train/Sugar beet/8738994fa.png  \n",
            "  inflating: train/Sugar beet/8761bb6e1.png  \n",
            "  inflating: train/Sugar beet/8784b654a.png  \n",
            "  inflating: train/Sugar beet/87c521db5.png  \n",
            "  inflating: train/Sugar beet/87f4dcfe7.png  \n",
            "  inflating: train/Sugar beet/87fce36af.png  \n",
            "  inflating: train/Sugar beet/88036eac5.png  \n",
            "  inflating: train/Sugar beet/88bd2a757.png  \n",
            "  inflating: train/Sugar beet/899e696a1.png  \n",
            "  inflating: train/Sugar beet/89c1a501a.png  \n",
            "  inflating: train/Sugar beet/89dd2e861.png  \n",
            "  inflating: train/Sugar beet/8a63b4a57.png  \n",
            "  inflating: train/Sugar beet/8a977c224.png  \n",
            "  inflating: train/Sugar beet/8b4c2df1f.png  \n",
            "  inflating: train/Sugar beet/8c1f10989.png  \n",
            "  inflating: train/Sugar beet/8c215ce50.png  \n",
            "  inflating: train/Sugar beet/8d59fc1da.png  \n",
            "  inflating: train/Sugar beet/8ded4af95.png  \n",
            "  inflating: train/Sugar beet/8f099d914.png  \n",
            "  inflating: train/Sugar beet/8fafd3b98.png  \n",
            "  inflating: train/Sugar beet/9031994fc.png  \n",
            "  inflating: train/Sugar beet/9040bcb5b.png  \n",
            "  inflating: train/Sugar beet/904f0f781.png  \n",
            "  inflating: train/Sugar beet/905ce5eb9.png  \n",
            "  inflating: train/Sugar beet/915689d7d.png  \n",
            "  inflating: train/Sugar beet/92038f3df.png  \n",
            "  inflating: train/Sugar beet/922f7543d.png  \n",
            "  inflating: train/Sugar beet/924a65aef.png  \n",
            "  inflating: train/Sugar beet/92b1a10ee.png  \n",
            "  inflating: train/Sugar beet/92be2b26d.png  \n",
            "  inflating: train/Sugar beet/92eac7565.png  \n",
            "  inflating: train/Sugar beet/9360888bb.png  \n",
            "  inflating: train/Sugar beet/94063c53d.png  \n",
            "  inflating: train/Sugar beet/96a82a2a2.png  \n",
            "  inflating: train/Sugar beet/96dfb46de.png  \n",
            "  inflating: train/Sugar beet/978de5160.png  \n",
            "  inflating: train/Sugar beet/979a39f22.png  \n",
            "  inflating: train/Sugar beet/98e87074b.png  \n",
            "  inflating: train/Sugar beet/99926b786.png  \n",
            "  inflating: train/Sugar beet/999f75899.png  \n",
            "  inflating: train/Sugar beet/99a9b1567.png  \n",
            "  inflating: train/Sugar beet/9a8bb2fff.png  \n",
            "  inflating: train/Sugar beet/9af73f331.png  \n",
            "  inflating: train/Sugar beet/9bdf9ba7d.png  \n",
            "  inflating: train/Sugar beet/9d13632b9.png  \n",
            "  inflating: train/Sugar beet/9ee5dbda4.png  \n",
            "  inflating: train/Sugar beet/9f70ba1fb.png  \n",
            "  inflating: train/Sugar beet/a012863ce.png  \n",
            "  inflating: train/Sugar beet/a06757f32.png  \n",
            "  inflating: train/Sugar beet/a59405ba8.png  \n",
            "  inflating: train/Sugar beet/a63ac9b84.png  \n",
            "  inflating: train/Sugar beet/a6805e809.png  \n",
            "  inflating: train/Sugar beet/a77646b15.png  \n",
            "  inflating: train/Sugar beet/a8ff2c49e.png  \n",
            "  inflating: train/Sugar beet/a9b7af892.png  \n",
            "  inflating: train/Sugar beet/a9daba74c.png  \n",
            "  inflating: train/Sugar beet/aa2ab6ee7.png  \n",
            "  inflating: train/Sugar beet/ab3b58928.png  \n",
            "  inflating: train/Sugar beet/ab90aee48.png  \n",
            "  inflating: train/Sugar beet/abd486e7b.png  \n",
            "  inflating: train/Sugar beet/acf5eabef.png  \n",
            "  inflating: train/Sugar beet/ad5397f8a.png  \n",
            "  inflating: train/Sugar beet/ad8a3280e.png  \n",
            "  inflating: train/Sugar beet/afe1cdd50.png  \n",
            "  inflating: train/Sugar beet/b0264558c.png  \n",
            "  inflating: train/Sugar beet/b0d647301.png  \n",
            "  inflating: train/Sugar beet/b2b9468af.png  \n",
            "  inflating: train/Sugar beet/b3bac1203.png  \n",
            "  inflating: train/Sugar beet/b41816608.png  \n",
            "  inflating: train/Sugar beet/b4190fe96.png  \n",
            "  inflating: train/Sugar beet/b44f9e223.png  \n",
            "  inflating: train/Sugar beet/b47d8d8f9.png  \n",
            "  inflating: train/Sugar beet/b8418755e.png  \n",
            "  inflating: train/Sugar beet/b8d4a8a59.png  \n",
            "  inflating: train/Sugar beet/b92458cf6.png  \n",
            "  inflating: train/Sugar beet/b94efd26f.png  \n",
            "  inflating: train/Sugar beet/b98d35523.png  \n",
            "  inflating: train/Sugar beet/ba820bf35.png  \n",
            "  inflating: train/Sugar beet/bb719464c.png  \n",
            "  inflating: train/Sugar beet/bbf48c655.png  \n",
            "  inflating: train/Sugar beet/bd5d6e03f.png  \n",
            "  inflating: train/Sugar beet/be337bf4d.png  \n",
            "  inflating: train/Sugar beet/c024ba394.png  \n",
            "  inflating: train/Sugar beet/c0341c93d.png  \n",
            "  inflating: train/Sugar beet/c06e70a77.png  \n",
            "  inflating: train/Sugar beet/c2c6d48e0.png  \n",
            "  inflating: train/Sugar beet/c3b9a611c.png  \n",
            "  inflating: train/Sugar beet/c3d57ab11.png  \n",
            "  inflating: train/Sugar beet/c405c0e38.png  \n",
            "  inflating: train/Sugar beet/c48b70c02.png  \n",
            "  inflating: train/Sugar beet/c4c87351c.png  \n",
            "  inflating: train/Sugar beet/c5687dbb2.png  \n",
            "  inflating: train/Sugar beet/c6818bcde.png  \n",
            "  inflating: train/Sugar beet/c69dc6f62.png  \n",
            "  inflating: train/Sugar beet/c6b426814.png  \n",
            "  inflating: train/Sugar beet/c80847076.png  \n",
            "  inflating: train/Sugar beet/c833940d5.png  \n",
            "  inflating: train/Sugar beet/cabb25dd8.png  \n",
            "  inflating: train/Sugar beet/cb5ff9c78.png  \n",
            "  inflating: train/Sugar beet/cbed7333d.png  \n",
            "  inflating: train/Sugar beet/cc285d141.png  \n",
            "  inflating: train/Sugar beet/cd8c55d21.png  \n",
            "  inflating: train/Sugar beet/cfb3565b9.png  \n",
            "  inflating: train/Sugar beet/d173b40c2.png  \n",
            "  inflating: train/Sugar beet/d1da687e9.png  \n",
            "  inflating: train/Sugar beet/d2b3e12d7.png  \n",
            "  inflating: train/Sugar beet/d2c9f9921.png  \n",
            "  inflating: train/Sugar beet/d415a63be.png  \n",
            "  inflating: train/Sugar beet/d41a8cb93.png  \n",
            "  inflating: train/Sugar beet/d41b1916c.png  \n",
            "  inflating: train/Sugar beet/d42bcf4bd.png  \n",
            "  inflating: train/Sugar beet/d4998deee.png  \n",
            "  inflating: train/Sugar beet/d4aec4bee.png  \n",
            "  inflating: train/Sugar beet/d4db82d2c.png  \n",
            "  inflating: train/Sugar beet/d558ea002.png  \n",
            "  inflating: train/Sugar beet/d585fe0c7.png  \n",
            "  inflating: train/Sugar beet/d5d014036.png  \n",
            "  inflating: train/Sugar beet/d5d909ed5.png  \n",
            "  inflating: train/Sugar beet/d62099b5b.png  \n",
            "  inflating: train/Sugar beet/d63de4d6b.png  \n",
            "  inflating: train/Sugar beet/d7b6e4654.png  \n",
            "  inflating: train/Sugar beet/d97272f23.png  \n",
            "  inflating: train/Sugar beet/d9f0ac9a7.png  \n",
            "  inflating: train/Sugar beet/d9f471b2d.png  \n",
            "  inflating: train/Sugar beet/da2912b2f.png  \n",
            "  inflating: train/Sugar beet/dc351dfa4.png  \n",
            "  inflating: train/Sugar beet/dc39e01ec.png  \n",
            "  inflating: train/Sugar beet/de1da036c.png  \n",
            "  inflating: train/Sugar beet/e0348224d.png  \n",
            "  inflating: train/Sugar beet/e053702bb.png  \n",
            "  inflating: train/Sugar beet/e0f7fb20b.png  \n",
            "  inflating: train/Sugar beet/e11ffb03d.png  \n",
            "  inflating: train/Sugar beet/e32175ef7.png  \n",
            "  inflating: train/Sugar beet/e49429c11.png  \n",
            "  inflating: train/Sugar beet/e4a149c23.png  \n",
            "  inflating: train/Sugar beet/e4a9bfee6.png  \n",
            "  inflating: train/Sugar beet/e54889b13.png  \n",
            "  inflating: train/Sugar beet/e61d10caf.png  \n",
            "  inflating: train/Sugar beet/e6217a0c4.png  \n",
            "  inflating: train/Sugar beet/e6c1a7bbc.png  \n",
            "  inflating: train/Sugar beet/e7bed72a5.png  \n",
            "  inflating: train/Sugar beet/e7c7bab19.png  \n",
            "  inflating: train/Sugar beet/e81a8dacb.png  \n",
            "  inflating: train/Sugar beet/e832be17c.png  \n",
            "  inflating: train/Sugar beet/e8870a1b1.png  \n",
            "  inflating: train/Sugar beet/e947502b4.png  \n",
            "  inflating: train/Sugar beet/e9492ed97.png  \n",
            "  inflating: train/Sugar beet/e9ed80cf3.png  \n",
            "  inflating: train/Sugar beet/ea224d23a.png  \n",
            "  inflating: train/Sugar beet/ea2b5a53e.png  \n",
            "  inflating: train/Sugar beet/ea6cc9c1b.png  \n",
            "  inflating: train/Sugar beet/ead3ac2e1.png  \n",
            "  inflating: train/Sugar beet/eadf78247.png  \n",
            "  inflating: train/Sugar beet/eb0418d36.png  \n",
            "  inflating: train/Sugar beet/eb2b13b27.png  \n",
            "  inflating: train/Sugar beet/ebcbefdad.png  \n",
            "  inflating: train/Sugar beet/ed71987ae.png  \n",
            "  inflating: train/Sugar beet/ee24d08ce.png  \n",
            "  inflating: train/Sugar beet/ee7e8f2d9.png  \n",
            "  inflating: train/Sugar beet/ee9ea960d.png  \n",
            "  inflating: train/Sugar beet/ef286fe28.png  \n",
            "  inflating: train/Sugar beet/f171da2ef.png  \n",
            "  inflating: train/Sugar beet/f1bb1de07.png  \n",
            "  inflating: train/Sugar beet/f22262822.png  \n",
            "  inflating: train/Sugar beet/f2eeeb0a2.png  \n",
            "  inflating: train/Sugar beet/f30abde8b.png  \n",
            "  inflating: train/Sugar beet/f40d6554c.png  \n",
            "  inflating: train/Sugar beet/f474da0ca.png  \n",
            "  inflating: train/Sugar beet/f506aa640.png  \n",
            "  inflating: train/Sugar beet/f5e7d66b9.png  \n",
            "  inflating: train/Sugar beet/f72176e1a.png  \n",
            "  inflating: train/Sugar beet/f78fcd305.png  \n",
            "  inflating: train/Sugar beet/f7ac77334.png  \n",
            "  inflating: train/Sugar beet/f861c27a9.png  \n",
            "  inflating: train/Sugar beet/f8bf223f3.png  \n",
            "  inflating: train/Sugar beet/f9a842f5f.png  \n",
            "  inflating: train/Sugar beet/faaac2544.png  \n",
            "  inflating: train/Sugar beet/fb57ce659.png  \n",
            "  inflating: train/Sugar beet/fbb26c002.png  \n",
            "  inflating: train/Sugar beet/fbec8cc1a.png  \n",
            "  inflating: train/Sugar beet/fc293eacb.png  \n",
            "  inflating: train/Sugar beet/fc441208c.png  \n",
            "  inflating: train/Sugar beet/fed9406b2.png  \n",
            "  inflating: train/Sugar beet/fef5e7066.png  \n",
            "  inflating: train/Sugar beet/ffa401155.png  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvdAsczKaQ-Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "dcc10b87-bc5e-420d-dc10-9314745c8c07"
      },
      "source": [
        "ls -l"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 68\n",
            "drwx------  4 root root  4096 Aug 23 16:20 \u001b[0m\u001b[01;34mdrive\u001b[0m/\n",
            "drwxr-xr-x  1 root root  4096 Jul 30 16:30 \u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r--  1 root root 19863 Dec 11  2019 sample_submission.csv\n",
            "drwxr-xr-x  2 root root 36864 Aug 23 16:20 \u001b[01;34mtest\u001b[0m/\n",
            "drwxr-xr-x 14 root root  4096 Aug 23 16:21 \u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfmyTLoWeIks",
        "colab_type": "text"
      },
      "source": [
        "It can be observed that there are 2 folders created - test and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR5l8Ex-hA1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "f5e015fe-29f0-4060-e75c-52f717b18bf6"
      },
      "source": [
        "ls -l train"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 204\n",
            "drwxr-xr-x 2 root root 12288 Aug 23 16:20  \u001b[0m\u001b[01;34mBlack-grass\u001b[0m/\n",
            "drwxr-xr-x 2 root root 20480 Aug 23 16:21  \u001b[01;34mCharlock\u001b[0m/\n",
            "drwxr-xr-x 2 root root 12288 Aug 23 16:21  \u001b[01;34mCleavers\u001b[0m/\n",
            "drwxr-xr-x 2 root root 20480 Aug 23 16:21 \u001b[01;34m'Common Chickweed'\u001b[0m/\n",
            "drwxr-xr-x 2 root root 12288 Aug 23 16:21 \u001b[01;34m'Common wheat'\u001b[0m/\n",
            "drwxr-xr-x 2 root root 20480 Aug 23 16:21 \u001b[01;34m'Fat Hen'\u001b[0m/\n",
            "drwxr-xr-x 2 root root 24576 Aug 23 16:21 \u001b[01;34m'Loose Silky-bent'\u001b[0m/\n",
            "drwxr-xr-x 2 root root 12288 Aug 23 16:21  \u001b[01;34mMaize\u001b[0m/\n",
            "drwxr-xr-x 2 root root 20480 Aug 23 16:21 \u001b[01;34m'Scentless Mayweed'\u001b[0m/\n",
            "drwxr-xr-x 2 root root 12288 Aug 23 16:21 \u001b[01;34m'Shepherds Purse'\u001b[0m/\n",
            "drwxr-xr-x 2 root root 20480 Aug 23 16:21 \u001b[01;34m'Small-flowered Cranesbill'\u001b[0m/\n",
            "drwxr-xr-x 2 root root 20480 Aug 23 16:21 \u001b[01;34m'Sugar beet'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGb_o_knhNYS",
        "colab_type": "text"
      },
      "source": [
        "There are 12 categories of seedlings that are available for training purpose. The following are the categories in order:\n",
        "\n",
        "1.\tBlack-grass\n",
        "2.\tCharlock\n",
        "3.\tCleavers\n",
        "4.\tCommon Chickweed\n",
        "5.\tCommon wheat\n",
        "6.\tFat Hen\n",
        "7.\tLoose Silky-bent\n",
        "8.\tMaize\n",
        "9.\tScentless Mayweed\n",
        "10.\tShepherds Purse\n",
        "11.\tSmall-flowered Cranesbill\n",
        "12.\tSugar beet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOj6K3pjV0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Taking all categories in a variable.\n",
        "all_categories = [\"Black-grass\",\"Charlock\",\"Cleavers\",\"Common Chickweed\",\"Common wheat\",\"Fat Hen\",\"Loose Silky-bent\",\"Maize\",\"Scentless Mayweed\",\"Shepherds Purse\",\"Small-flowered Cranesbill\",\"Sugar beet\"]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSaNMyIcjdm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "3bbe8d8c-2e5c-4bd7-ccee-2feae58da010"
      },
      "source": [
        "all_categories"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Black-grass',\n",
              " 'Charlock',\n",
              " 'Cleavers',\n",
              " 'Common Chickweed',\n",
              " 'Common wheat',\n",
              " 'Fat Hen',\n",
              " 'Loose Silky-bent',\n",
              " 'Maize',\n",
              " 'Scentless Mayweed',\n",
              " 'Shepherds Purse',\n",
              " 'Small-flowered Cranesbill',\n",
              " 'Sugar beet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QC-sHnCrjtMI",
        "colab_type": "text"
      },
      "source": [
        "## All imports and constants declarations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtdZxcsAPYBv",
        "colab_type": "text"
      },
      "source": [
        "Import Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjUZ8X3Fc0j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae18G3cFkrF3",
        "colab_type": "text"
      },
      "source": [
        "Other imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjbmmcLckp3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from sklearn import metrics\n",
        "from sklearn import preprocessing\n",
        "from matplotlib import pyplot as plt\n",
        "from google.colab import files\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61FBXAYvffxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define some parameters\n",
        "img_size = 60\n",
        "img_depth = 3"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzvYrYyKlRu7",
        "colab_type": "text"
      },
      "source": [
        "## Building models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsCtO9eZoy6q",
        "colab_type": "text"
      },
      "source": [
        "### 1. Build base model without image augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHzaO4uIfnDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_generator= tf.keras.preprocessing.image.ImageDataGenerator(validation_split=.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8e615dpgONf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ade30ecc-dd7a-424b-bc82-99f4c6885517"
      },
      "source": [
        "#Build training generator. \n",
        "train_generator = img_generator.flow_from_directory('train',\n",
        "                                                    target_size=(img_size, img_size),\n",
        "                                                    subset='training',\n",
        "                                                    batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3803 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAA0GB-0gXdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0f9344b8-5f77-4912-9943-834b9802c403"
      },
      "source": [
        "#Build test generator\n",
        "test_generator = img_generator.flow_from_directory('train',\n",
        "                                                   target_size=(img_size, img_size),                                                   \n",
        "                                                   subset='validation',\n",
        "                                                   batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 947 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADtFqlY0g2fH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c898bef5-769e-46d3-b370-8af4817b81b4"
      },
      "source": [
        "# Lets check the features (images) and Labels (flower class) returned by ImageDataGenerator\n",
        "X, y = next(train_generator)\n",
        "\n",
        "print('Input features shape', X.shape)\n",
        "print('Actual labels shape', y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input features shape (64, 60, 60, 3)\n",
            "Actual labels shape (64, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kqsv4inuSQGc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e547dacf-bce4-4bf7-d7d7-2b95140a4f00"
      },
      "source": [
        "type(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Om-OLZb9Sa_R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "68df924a-f30f-4e02-fa85-264186adecec"
      },
      "source": [
        "y[15]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bwbz3RdYSlEK",
        "colab_type": "text"
      },
      "source": [
        "Build CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjuFS2egSe_R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear any previous model from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Initialize model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization(input_shape=(img_size,img_size,3,)))\n",
        "\n",
        "#Add Conv Layer\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Conv Layer\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Max Pool layer\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Add Dense Layers after flattening the data\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "#Add Dropout\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "#Add Output Layer\n",
        "model.add(tf.keras.layers.Dense(12, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnbX-VyoSubC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify Loass and Optimizer\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni1eDkEFSyLX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "13d1f7a5-15b3-4a01-fe7a-33f2e42a828c"
      },
      "source": [
        "#Model Summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 60, 60, 3)         12        \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 58, 58, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 58, 58, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 56, 56, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               6422656   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12)                1548      \n",
            "=================================================================\n",
            "Total params: 6,443,992\n",
            "Trainable params: 6,443,794\n",
            "Non-trainable params: 198\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moy3AT7mTBBK",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Onu9VsoKio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the best model using model checkpoint callback\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('seedling.h5', \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_accuracy', \n",
        "                                                    mode='max', \n",
        "                                                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flO13WPkS0Uo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1b648755-dfa4-49b0-a7f3-368b110310af"
      },
      "source": [
        "# Running the model for 100 epochs\n",
        "model.fit(train_generator, \n",
        "          epochs=100,\n",
        "          steps_per_epoch= 3803//64,  #Number of training images//batch_size\n",
        "          validation_data=test_generator,\n",
        "          validation_steps = 947//64, #Number of test images//batch_size\n",
        "          callbacks = [model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 4.3115 - accuracy: 0.2428\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.10938, saving model to seedling.h5\n",
            "59/59 [==============================] - 53s 895ms/step - loss: 4.3115 - accuracy: 0.2428 - val_loss: 2.5314 - val_accuracy: 0.1094\n",
            "Epoch 2/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.7842 - accuracy: 0.3726\n",
            "Epoch 00002: val_accuracy improved from 0.10938 to 0.20424, saving model to seedling.h5\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 1.7842 - accuracy: 0.3726 - val_loss: 13.3650 - val_accuracy: 0.2042\n",
            "Epoch 3/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.5778 - accuracy: 0.4509\n",
            "Epoch 00003: val_accuracy improved from 0.20424 to 0.20647, saving model to seedling.h5\n",
            "59/59 [==============================] - 50s 846ms/step - loss: 1.5778 - accuracy: 0.4509 - val_loss: 21.1561 - val_accuracy: 0.2065\n",
            "Epoch 4/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.3965 - accuracy: 0.5127\n",
            "Epoch 00004: val_accuracy improved from 0.20647 to 0.27344, saving model to seedling.h5\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 1.3965 - accuracy: 0.5127 - val_loss: 17.7812 - val_accuracy: 0.2734\n",
            "Epoch 5/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.1758 - accuracy: 0.5667\n",
            "Epoch 00005: val_accuracy improved from 0.27344 to 0.47098, saving model to seedling.h5\n",
            "59/59 [==============================] - 50s 843ms/step - loss: 1.1758 - accuracy: 0.5667 - val_loss: 6.1360 - val_accuracy: 0.4710\n",
            "Epoch 6/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.1083 - accuracy: 0.5996\n",
            "Epoch 00006: val_accuracy improved from 0.47098 to 0.51228, saving model to seedling.h5\n",
            "59/59 [==============================] - 50s 843ms/step - loss: 1.1083 - accuracy: 0.5996 - val_loss: 3.2014 - val_accuracy: 0.5123\n",
            "Epoch 7/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9649 - accuracy: 0.6381\n",
            "Epoch 00007: val_accuracy improved from 0.51228 to 0.58371, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 828ms/step - loss: 0.9649 - accuracy: 0.6381 - val_loss: 1.7726 - val_accuracy: 0.5837\n",
            "Epoch 8/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9194 - accuracy: 0.6643\n",
            "Epoch 00008: val_accuracy improved from 0.58371 to 0.62723, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 823ms/step - loss: 0.9194 - accuracy: 0.6643 - val_loss: 1.7771 - val_accuracy: 0.6272\n",
            "Epoch 9/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7947 - accuracy: 0.7141\n",
            "Epoch 00009: val_accuracy improved from 0.62723 to 0.66295, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 824ms/step - loss: 0.7947 - accuracy: 0.7141 - val_loss: 1.4157 - val_accuracy: 0.6629\n",
            "Epoch 10/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7876 - accuracy: 0.7154\n",
            "Epoch 00010: val_accuracy did not improve from 0.66295\n",
            "59/59 [==============================] - 48s 813ms/step - loss: 0.7876 - accuracy: 0.7154 - val_loss: 1.3912 - val_accuracy: 0.6384\n",
            "Epoch 11/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7147 - accuracy: 0.7325\n",
            "Epoch 00011: val_accuracy did not improve from 0.66295\n",
            "59/59 [==============================] - 49s 826ms/step - loss: 0.7147 - accuracy: 0.7325 - val_loss: 1.4378 - val_accuracy: 0.6496\n",
            "Epoch 12/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6779 - accuracy: 0.7416\n",
            "Epoch 00012: val_accuracy did not improve from 0.66295\n",
            "59/59 [==============================] - 48s 818ms/step - loss: 0.6779 - accuracy: 0.7416 - val_loss: 1.2823 - val_accuracy: 0.6596\n",
            "Epoch 13/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.7791\n",
            "Epoch 00013: val_accuracy improved from 0.66295 to 0.66853, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 824ms/step - loss: 0.6126 - accuracy: 0.7791 - val_loss: 1.2594 - val_accuracy: 0.6685\n",
            "Epoch 14/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.7938\n",
            "Epoch 00014: val_accuracy did not improve from 0.66853\n",
            "59/59 [==============================] - 49s 834ms/step - loss: 0.5366 - accuracy: 0.7938 - val_loss: 1.6742 - val_accuracy: 0.6440\n",
            "Epoch 15/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.5486 - accuracy: 0.7895\n",
            "Epoch 00015: val_accuracy improved from 0.66853 to 0.66964, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 831ms/step - loss: 0.5486 - accuracy: 0.7895 - val_loss: 1.4183 - val_accuracy: 0.6696\n",
            "Epoch 16/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.4996 - accuracy: 0.8093\n",
            "Epoch 00016: val_accuracy improved from 0.66964 to 0.67634, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.4996 - accuracy: 0.8093 - val_loss: 1.2430 - val_accuracy: 0.6763\n",
            "Epoch 17/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.4814 - accuracy: 0.8235\n",
            "Epoch 00017: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.4814 - accuracy: 0.8235 - val_loss: 1.5315 - val_accuracy: 0.6752\n",
            "Epoch 18/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.8377\n",
            "Epoch 00018: val_accuracy improved from 0.67634 to 0.68080, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 836ms/step - loss: 0.4527 - accuracy: 0.8377 - val_loss: 1.4691 - val_accuracy: 0.6808\n",
            "Epoch 19/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.4456 - accuracy: 0.8401\n",
            "Epoch 00019: val_accuracy did not improve from 0.68080\n",
            "59/59 [==============================] - 48s 806ms/step - loss: 0.4456 - accuracy: 0.8401 - val_loss: 1.5937 - val_accuracy: 0.6719\n",
            "Epoch 20/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.4807 - accuracy: 0.8189\n",
            "Epoch 00020: val_accuracy improved from 0.68080 to 0.69196, saving model to seedling.h5\n",
            "59/59 [==============================] - 47s 798ms/step - loss: 0.4807 - accuracy: 0.8189 - val_loss: 1.6074 - val_accuracy: 0.6920\n",
            "Epoch 21/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.4061 - accuracy: 0.8462\n",
            "Epoch 00021: val_accuracy improved from 0.69196 to 0.69531, saving model to seedling.h5\n",
            "59/59 [==============================] - 47s 802ms/step - loss: 0.4061 - accuracy: 0.8462 - val_loss: 1.6144 - val_accuracy: 0.6953\n",
            "Epoch 22/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.8663\n",
            "Epoch 00022: val_accuracy did not improve from 0.69531\n",
            "59/59 [==============================] - 47s 792ms/step - loss: 0.3732 - accuracy: 0.8663 - val_loss: 2.0542 - val_accuracy: 0.6897\n",
            "Epoch 23/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3382 - accuracy: 0.8772\n",
            "Epoch 00023: val_accuracy improved from 0.69531 to 0.71094, saving model to seedling.h5\n",
            "59/59 [==============================] - 47s 799ms/step - loss: 0.3382 - accuracy: 0.8772 - val_loss: 1.6609 - val_accuracy: 0.7109\n",
            "Epoch 24/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3162 - accuracy: 0.8818\n",
            "Epoch 00024: val_accuracy improved from 0.71094 to 0.71987, saving model to seedling.h5\n",
            "59/59 [==============================] - 47s 794ms/step - loss: 0.3162 - accuracy: 0.8818 - val_loss: 1.7518 - val_accuracy: 0.7199\n",
            "Epoch 25/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.8738\n",
            "Epoch 00025: val_accuracy did not improve from 0.71987\n",
            "59/59 [==============================] - 47s 795ms/step - loss: 0.3519 - accuracy: 0.8738 - val_loss: 1.5789 - val_accuracy: 0.6975\n",
            "Epoch 26/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3195 - accuracy: 0.8839\n",
            "Epoch 00026: val_accuracy improved from 0.71987 to 0.72768, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.3195 - accuracy: 0.8839 - val_loss: 1.7703 - val_accuracy: 0.7277\n",
            "Epoch 27/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3008 - accuracy: 0.8925\n",
            "Epoch 00027: val_accuracy did not improve from 0.72768\n",
            "59/59 [==============================] - 47s 800ms/step - loss: 0.3008 - accuracy: 0.8925 - val_loss: 1.9008 - val_accuracy: 0.7221\n",
            "Epoch 28/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.8909\n",
            "Epoch 00028: val_accuracy improved from 0.72768 to 0.72991, saving model to seedling.h5\n",
            "59/59 [==============================] - 47s 804ms/step - loss: 0.2955 - accuracy: 0.8909 - val_loss: 1.5562 - val_accuracy: 0.7299\n",
            "Epoch 29/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2680 - accuracy: 0.9021\n",
            "Epoch 00029: val_accuracy improved from 0.72991 to 0.73438, saving model to seedling.h5\n",
            "59/59 [==============================] - 47s 801ms/step - loss: 0.2680 - accuracy: 0.9021 - val_loss: 1.6417 - val_accuracy: 0.7344\n",
            "Epoch 30/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2750 - accuracy: 0.9037\n",
            "Epoch 00030: val_accuracy did not improve from 0.73438\n",
            "59/59 [==============================] - 47s 805ms/step - loss: 0.2750 - accuracy: 0.9037 - val_loss: 1.5793 - val_accuracy: 0.7199\n",
            "Epoch 31/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2828 - accuracy: 0.9010\n",
            "Epoch 00031: val_accuracy did not improve from 0.73438\n",
            "59/59 [==============================] - 49s 828ms/step - loss: 0.2828 - accuracy: 0.9010 - val_loss: 1.6873 - val_accuracy: 0.6897\n",
            "Epoch 32/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.9112\n",
            "Epoch 00032: val_accuracy improved from 0.73438 to 0.74888, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 834ms/step - loss: 0.2788 - accuracy: 0.9112 - val_loss: 1.6872 - val_accuracy: 0.7489\n",
            "Epoch 33/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9099\n",
            "Epoch 00033: val_accuracy did not improve from 0.74888\n",
            "59/59 [==============================] - 49s 834ms/step - loss: 0.2647 - accuracy: 0.9099 - val_loss: 1.5919 - val_accuracy: 0.7321\n",
            "Epoch 34/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2313 - accuracy: 0.9179\n",
            "Epoch 00034: val_accuracy did not improve from 0.74888\n",
            "59/59 [==============================] - 49s 833ms/step - loss: 0.2313 - accuracy: 0.9179 - val_loss: 1.9226 - val_accuracy: 0.7188\n",
            "Epoch 35/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2396 - accuracy: 0.9139\n",
            "Epoch 00035: val_accuracy did not improve from 0.74888\n",
            "59/59 [==============================] - 49s 825ms/step - loss: 0.2396 - accuracy: 0.9139 - val_loss: 1.8721 - val_accuracy: 0.7422\n",
            "Epoch 36/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2402 - accuracy: 0.9174\n",
            "Epoch 00036: val_accuracy did not improve from 0.74888\n",
            "59/59 [==============================] - 48s 811ms/step - loss: 0.2402 - accuracy: 0.9174 - val_loss: 1.7894 - val_accuracy: 0.7411\n",
            "Epoch 37/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2087 - accuracy: 0.9216\n",
            "Epoch 00037: val_accuracy did not improve from 0.74888\n",
            "59/59 [==============================] - 48s 810ms/step - loss: 0.2087 - accuracy: 0.9216 - val_loss: 1.8218 - val_accuracy: 0.7355\n",
            "Epoch 38/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2378 - accuracy: 0.9184\n",
            "Epoch 00038: val_accuracy did not improve from 0.74888\n",
            "59/59 [==============================] - 48s 814ms/step - loss: 0.2378 - accuracy: 0.9184 - val_loss: 1.9019 - val_accuracy: 0.7422\n",
            "Epoch 39/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9315\n",
            "Epoch 00039: val_accuracy improved from 0.74888 to 0.76562, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.2117 - accuracy: 0.9315 - val_loss: 1.8945 - val_accuracy: 0.7656\n",
            "Epoch 40/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9323\n",
            "Epoch 00040: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 807ms/step - loss: 0.1915 - accuracy: 0.9323 - val_loss: 1.6947 - val_accuracy: 0.7578\n",
            "Epoch 41/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2081 - accuracy: 0.9264\n",
            "Epoch 00041: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 821ms/step - loss: 0.2081 - accuracy: 0.9264 - val_loss: 1.7052 - val_accuracy: 0.7388\n",
            "Epoch 42/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1796 - accuracy: 0.9371\n",
            "Epoch 00042: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 807ms/step - loss: 0.1796 - accuracy: 0.9371 - val_loss: 1.7400 - val_accuracy: 0.7333\n",
            "Epoch 43/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9326\n",
            "Epoch 00043: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 47s 804ms/step - loss: 0.1962 - accuracy: 0.9326 - val_loss: 1.7308 - val_accuracy: 0.7333\n",
            "Epoch 44/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1650 - accuracy: 0.9452\n",
            "Epoch 00044: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 806ms/step - loss: 0.1650 - accuracy: 0.9452 - val_loss: 1.8665 - val_accuracy: 0.7533\n",
            "Epoch 45/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1825 - accuracy: 0.9390\n",
            "Epoch 00045: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.1825 - accuracy: 0.9390 - val_loss: 1.7343 - val_accuracy: 0.7143\n",
            "Epoch 46/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2138 - accuracy: 0.9278\n",
            "Epoch 00046: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.2138 - accuracy: 0.9278 - val_loss: 1.9040 - val_accuracy: 0.7210\n",
            "Epoch 47/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1788 - accuracy: 0.9382\n",
            "Epoch 00047: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 815ms/step - loss: 0.1788 - accuracy: 0.9382 - val_loss: 1.8020 - val_accuracy: 0.7455\n",
            "Epoch 48/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9342\n",
            "Epoch 00048: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 49s 825ms/step - loss: 0.1986 - accuracy: 0.9342 - val_loss: 2.0117 - val_accuracy: 0.6931\n",
            "Epoch 49/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9353\n",
            "Epoch 00049: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 49s 824ms/step - loss: 0.2167 - accuracy: 0.9353 - val_loss: 1.9385 - val_accuracy: 0.7366\n",
            "Epoch 50/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9382\n",
            "Epoch 00050: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.1870 - accuracy: 0.9382 - val_loss: 1.9005 - val_accuracy: 0.7578\n",
            "Epoch 51/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9404\n",
            "Epoch 00051: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 811ms/step - loss: 0.1763 - accuracy: 0.9404 - val_loss: 2.0952 - val_accuracy: 0.7556\n",
            "Epoch 52/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1662 - accuracy: 0.9465\n",
            "Epoch 00052: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.1662 - accuracy: 0.9465 - val_loss: 2.1466 - val_accuracy: 0.7344\n",
            "Epoch 53/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1264 - accuracy: 0.9569\n",
            "Epoch 00053: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 47s 805ms/step - loss: 0.1264 - accuracy: 0.9569 - val_loss: 2.0430 - val_accuracy: 0.7578\n",
            "Epoch 54/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9511\n",
            "Epoch 00054: val_accuracy did not improve from 0.76562\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.1514 - accuracy: 0.9511 - val_loss: 2.3839 - val_accuracy: 0.7433\n",
            "Epoch 55/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9564\n",
            "Epoch 00055: val_accuracy improved from 0.76562 to 0.77009, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 813ms/step - loss: 0.1227 - accuracy: 0.9564 - val_loss: 2.0392 - val_accuracy: 0.7701\n",
            "Epoch 56/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1922 - accuracy: 0.9406\n",
            "Epoch 00056: val_accuracy did not improve from 0.77009\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.1922 - accuracy: 0.9406 - val_loss: 1.8164 - val_accuracy: 0.7288\n",
            "Epoch 57/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1427 - accuracy: 0.9553\n",
            "Epoch 00057: val_accuracy improved from 0.77009 to 0.77567, saving model to seedling.h5\n",
            "59/59 [==============================] - 47s 803ms/step - loss: 0.1427 - accuracy: 0.9553 - val_loss: 1.7001 - val_accuracy: 0.7757\n",
            "Epoch 58/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9577\n",
            "Epoch 00058: val_accuracy did not improve from 0.77567\n",
            "59/59 [==============================] - 47s 800ms/step - loss: 0.1334 - accuracy: 0.9577 - val_loss: 1.6759 - val_accuracy: 0.7623\n",
            "Epoch 59/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1355 - accuracy: 0.9548\n",
            "Epoch 00059: val_accuracy did not improve from 0.77567\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.1355 - accuracy: 0.9548 - val_loss: 1.6987 - val_accuracy: 0.7679\n",
            "Epoch 60/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1382 - accuracy: 0.9556\n",
            "Epoch 00060: val_accuracy did not improve from 0.77567\n",
            "59/59 [==============================] - 47s 801ms/step - loss: 0.1382 - accuracy: 0.9556 - val_loss: 1.9224 - val_accuracy: 0.7690\n",
            "Epoch 61/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1289 - accuracy: 0.9575\n",
            "Epoch 00061: val_accuracy did not improve from 0.77567\n",
            "59/59 [==============================] - 48s 810ms/step - loss: 0.1289 - accuracy: 0.9575 - val_loss: 2.0240 - val_accuracy: 0.7545\n",
            "Epoch 62/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0977 - accuracy: 0.9655\n",
            "Epoch 00062: val_accuracy improved from 0.77567 to 0.78125, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 823ms/step - loss: 0.0977 - accuracy: 0.9655 - val_loss: 1.8523 - val_accuracy: 0.7812\n",
            "Epoch 63/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1145 - accuracy: 0.9593\n",
            "Epoch 00063: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 47s 802ms/step - loss: 0.1145 - accuracy: 0.9593 - val_loss: 2.1217 - val_accuracy: 0.7667\n",
            "Epoch 64/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1518 - accuracy: 0.9508\n",
            "Epoch 00064: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 805ms/step - loss: 0.1518 - accuracy: 0.9508 - val_loss: 2.2213 - val_accuracy: 0.7567\n",
            "Epoch 65/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9572\n",
            "Epoch 00065: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 806ms/step - loss: 0.1258 - accuracy: 0.9572 - val_loss: 1.9516 - val_accuracy: 0.7701\n",
            "Epoch 66/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9631\n",
            "Epoch 00066: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 822ms/step - loss: 0.1013 - accuracy: 0.9631 - val_loss: 2.1119 - val_accuracy: 0.7533\n",
            "Epoch 67/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1173 - accuracy: 0.9580\n",
            "Epoch 00067: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.1173 - accuracy: 0.9580 - val_loss: 1.8226 - val_accuracy: 0.7567\n",
            "Epoch 68/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1189 - accuracy: 0.9585\n",
            "Epoch 00068: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 821ms/step - loss: 0.1189 - accuracy: 0.9585 - val_loss: 1.7705 - val_accuracy: 0.7567\n",
            "Epoch 69/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1399 - accuracy: 0.9572\n",
            "Epoch 00069: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 812ms/step - loss: 0.1399 - accuracy: 0.9572 - val_loss: 1.8727 - val_accuracy: 0.7388\n",
            "Epoch 70/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9620\n",
            "Epoch 00070: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 812ms/step - loss: 0.1153 - accuracy: 0.9620 - val_loss: 2.9951 - val_accuracy: 0.7210\n",
            "Epoch 71/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1227 - accuracy: 0.9607\n",
            "Epoch 00071: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.1227 - accuracy: 0.9607 - val_loss: 1.5831 - val_accuracy: 0.7768\n",
            "Epoch 72/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9652\n",
            "Epoch 00072: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.1121 - accuracy: 0.9652 - val_loss: 1.9253 - val_accuracy: 0.7400\n",
            "Epoch 73/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1222 - accuracy: 0.9631\n",
            "Epoch 00073: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 811ms/step - loss: 0.1222 - accuracy: 0.9631 - val_loss: 1.9138 - val_accuracy: 0.7667\n",
            "Epoch 74/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1184 - accuracy: 0.9674\n",
            "Epoch 00074: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 811ms/step - loss: 0.1184 - accuracy: 0.9674 - val_loss: 1.8315 - val_accuracy: 0.7567\n",
            "Epoch 75/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9684\n",
            "Epoch 00075: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 809ms/step - loss: 0.0964 - accuracy: 0.9684 - val_loss: 2.0498 - val_accuracy: 0.7757\n",
            "Epoch 76/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9660\n",
            "Epoch 00076: val_accuracy did not improve from 0.78125\n",
            "59/59 [==============================] - 48s 807ms/step - loss: 0.1026 - accuracy: 0.9660 - val_loss: 1.8091 - val_accuracy: 0.7812\n",
            "Epoch 77/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9650\n",
            "Epoch 00077: val_accuracy improved from 0.78125 to 0.78460, saving model to seedling.h5\n",
            "59/59 [==============================] - 47s 804ms/step - loss: 0.1075 - accuracy: 0.9650 - val_loss: 1.8532 - val_accuracy: 0.7846\n",
            "Epoch 78/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9650\n",
            "Epoch 00078: val_accuracy improved from 0.78460 to 0.79353, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 807ms/step - loss: 0.1160 - accuracy: 0.9650 - val_loss: 2.1538 - val_accuracy: 0.7935\n",
            "Epoch 79/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1236 - accuracy: 0.9687\n",
            "Epoch 00079: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 47s 801ms/step - loss: 0.1236 - accuracy: 0.9687 - val_loss: 2.3377 - val_accuracy: 0.7667\n",
            "Epoch 80/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9579\n",
            "Epoch 00080: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 47s 803ms/step - loss: 0.1246 - accuracy: 0.9579 - val_loss: 2.7588 - val_accuracy: 0.7500\n",
            "Epoch 81/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9642\n",
            "Epoch 00081: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 48s 813ms/step - loss: 0.1095 - accuracy: 0.9642 - val_loss: 2.3871 - val_accuracy: 0.7746\n",
            "Epoch 82/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9695\n",
            "Epoch 00082: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 47s 803ms/step - loss: 0.0969 - accuracy: 0.9695 - val_loss: 2.0490 - val_accuracy: 0.7723\n",
            "Epoch 83/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9695\n",
            "Epoch 00083: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 48s 812ms/step - loss: 0.0871 - accuracy: 0.9695 - val_loss: 2.4479 - val_accuracy: 0.7667\n",
            "Epoch 84/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1124 - accuracy: 0.9663\n",
            "Epoch 00084: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 47s 804ms/step - loss: 0.1124 - accuracy: 0.9663 - val_loss: 2.2436 - val_accuracy: 0.7567\n",
            "Epoch 85/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1038 - accuracy: 0.9663\n",
            "Epoch 00085: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 47s 801ms/step - loss: 0.1038 - accuracy: 0.9663 - val_loss: 2.7509 - val_accuracy: 0.7444\n",
            "Epoch 86/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1122 - accuracy: 0.9644\n",
            "Epoch 00086: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 48s 818ms/step - loss: 0.1122 - accuracy: 0.9644 - val_loss: 2.7373 - val_accuracy: 0.7556\n",
            "Epoch 87/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1016 - accuracy: 0.9703\n",
            "Epoch 00087: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 49s 829ms/step - loss: 0.1016 - accuracy: 0.9703 - val_loss: 2.2868 - val_accuracy: 0.7734\n",
            "Epoch 88/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9682\n",
            "Epoch 00088: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 48s 818ms/step - loss: 0.0934 - accuracy: 0.9682 - val_loss: 2.0981 - val_accuracy: 0.7612\n",
            "Epoch 89/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0958 - accuracy: 0.9684\n",
            "Epoch 00089: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 49s 829ms/step - loss: 0.0958 - accuracy: 0.9684 - val_loss: 2.2475 - val_accuracy: 0.7612\n",
            "Epoch 90/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9711\n",
            "Epoch 00090: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 49s 831ms/step - loss: 0.0800 - accuracy: 0.9711 - val_loss: 1.9869 - val_accuracy: 0.7768\n",
            "Epoch 91/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9674\n",
            "Epoch 00091: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 49s 838ms/step - loss: 0.0979 - accuracy: 0.9674 - val_loss: 2.2856 - val_accuracy: 0.7891\n",
            "Epoch 92/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1027 - accuracy: 0.9706\n",
            "Epoch 00092: val_accuracy did not improve from 0.79353\n",
            "59/59 [==============================] - 49s 826ms/step - loss: 0.1027 - accuracy: 0.9706 - val_loss: 2.6234 - val_accuracy: 0.7746\n",
            "Epoch 93/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9690\n",
            "Epoch 00093: val_accuracy improved from 0.79353 to 0.79911, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 819ms/step - loss: 0.1180 - accuracy: 0.9690 - val_loss: 2.2266 - val_accuracy: 0.7991\n",
            "Epoch 94/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0987 - accuracy: 0.9682\n",
            "Epoch 00094: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 49s 826ms/step - loss: 0.0987 - accuracy: 0.9682 - val_loss: 2.2258 - val_accuracy: 0.7757\n",
            "Epoch 95/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9733\n",
            "Epoch 00095: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0863 - accuracy: 0.9733 - val_loss: 3.1137 - val_accuracy: 0.7511\n",
            "Epoch 96/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1306 - accuracy: 0.9676\n",
            "Epoch 00096: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 48s 813ms/step - loss: 0.1306 - accuracy: 0.9676 - val_loss: 2.3535 - val_accuracy: 0.7812\n",
            "Epoch 97/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1108 - accuracy: 0.9666\n",
            "Epoch 00097: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 48s 821ms/step - loss: 0.1108 - accuracy: 0.9666 - val_loss: 2.1356 - val_accuracy: 0.7746\n",
            "Epoch 98/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9722\n",
            "Epoch 00098: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 48s 811ms/step - loss: 0.0898 - accuracy: 0.9722 - val_loss: 1.9656 - val_accuracy: 0.7902\n",
            "Epoch 99/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9626\n",
            "Epoch 00099: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 48s 817ms/step - loss: 0.1216 - accuracy: 0.9626 - val_loss: 1.9210 - val_accuracy: 0.7645\n",
            "Epoch 100/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1087 - accuracy: 0.9660\n",
            "Epoch 00100: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 48s 817ms/step - loss: 0.1087 - accuracy: 0.9660 - val_loss: 2.3382 - val_accuracy: 0.7355\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdd92ac6c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6KoJSkCxSXr",
        "colab_type": "text"
      },
      "source": [
        "The model still seems to be learning so running it for 100 more epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jg919i-QxZmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "550451c5-0208-4485-ba7b-6293eaa1ac25"
      },
      "source": [
        "model.fit(train_generator,\n",
        "                    epochs=200,\n",
        "                    initial_epoch=100,\n",
        "                    steps_per_epoch= 3803//64,  #Number of training images//batch_size\n",
        "                    validation_data=test_generator,\n",
        "                    validation_steps = 947//64, #Number of test images//batch_size\n",
        "                    callbacks = [model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 101/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0872 - accuracy: 0.9706\n",
            "Epoch 00101: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 51s 856ms/step - loss: 0.0872 - accuracy: 0.9706 - val_loss: 2.9058 - val_accuracy: 0.7533\n",
            "Epoch 102/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9733\n",
            "Epoch 00102: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 49s 824ms/step - loss: 0.0775 - accuracy: 0.9733 - val_loss: 2.2231 - val_accuracy: 0.7734\n",
            "Epoch 103/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0992 - accuracy: 0.9674\n",
            "Epoch 00103: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0992 - accuracy: 0.9674 - val_loss: 2.1972 - val_accuracy: 0.7667\n",
            "Epoch 104/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9773\n",
            "Epoch 00104: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 49s 837ms/step - loss: 0.0677 - accuracy: 0.9773 - val_loss: 2.1390 - val_accuracy: 0.7857\n",
            "Epoch 105/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9754\n",
            "Epoch 00105: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 51s 866ms/step - loss: 0.0744 - accuracy: 0.9754 - val_loss: 2.4347 - val_accuracy: 0.7701\n",
            "Epoch 106/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9751\n",
            "Epoch 00106: val_accuracy did not improve from 0.79911\n",
            "59/59 [==============================] - 49s 830ms/step - loss: 0.0830 - accuracy: 0.9751 - val_loss: 2.0749 - val_accuracy: 0.7846\n",
            "Epoch 107/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0632 - accuracy: 0.9802\n",
            "Epoch 00107: val_accuracy improved from 0.79911 to 0.80580, saving model to seedling.h5\n",
            "59/59 [==============================] - 49s 832ms/step - loss: 0.0632 - accuracy: 0.9802 - val_loss: 2.3199 - val_accuracy: 0.8058\n",
            "Epoch 108/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9826\n",
            "Epoch 00108: val_accuracy did not improve from 0.80580\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0522 - accuracy: 0.9826 - val_loss: 2.5064 - val_accuracy: 0.7857\n",
            "Epoch 109/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0909 - accuracy: 0.9743\n",
            "Epoch 00109: val_accuracy did not improve from 0.80580\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.0909 - accuracy: 0.9743 - val_loss: 2.3607 - val_accuracy: 0.7734\n",
            "Epoch 110/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9743\n",
            "Epoch 00110: val_accuracy did not improve from 0.80580\n",
            "59/59 [==============================] - 48s 822ms/step - loss: 0.0863 - accuracy: 0.9743 - val_loss: 2.1173 - val_accuracy: 0.7980\n",
            "Epoch 111/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9722\n",
            "Epoch 00111: val_accuracy did not improve from 0.80580\n",
            "59/59 [==============================] - 48s 819ms/step - loss: 0.0753 - accuracy: 0.9722 - val_loss: 2.0296 - val_accuracy: 0.7768\n",
            "Epoch 112/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9815\n",
            "Epoch 00112: val_accuracy improved from 0.80580 to 0.80804, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0541 - accuracy: 0.9815 - val_loss: 2.4960 - val_accuracy: 0.8080\n",
            "Epoch 113/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9789\n",
            "Epoch 00113: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 49s 822ms/step - loss: 0.0655 - accuracy: 0.9789 - val_loss: 2.8651 - val_accuracy: 0.7902\n",
            "Epoch 114/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9778\n",
            "Epoch 00114: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 805ms/step - loss: 0.0772 - accuracy: 0.9778 - val_loss: 2.9659 - val_accuracy: 0.7634\n",
            "Epoch 115/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1095 - accuracy: 0.9743\n",
            "Epoch 00115: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 821ms/step - loss: 0.1095 - accuracy: 0.9743 - val_loss: 2.7622 - val_accuracy: 0.7812\n",
            "Epoch 116/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1177 - accuracy: 0.9698\n",
            "Epoch 00116: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 49s 822ms/step - loss: 0.1177 - accuracy: 0.9698 - val_loss: 2.4442 - val_accuracy: 0.7946\n",
            "Epoch 117/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9749\n",
            "Epoch 00117: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 817ms/step - loss: 0.0948 - accuracy: 0.9749 - val_loss: 2.9837 - val_accuracy: 0.7679\n",
            "Epoch 118/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0770 - accuracy: 0.9757\n",
            "Epoch 00118: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.0770 - accuracy: 0.9757 - val_loss: 2.3516 - val_accuracy: 0.7902\n",
            "Epoch 119/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9810\n",
            "Epoch 00119: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0538 - accuracy: 0.9810 - val_loss: 3.1523 - val_accuracy: 0.7723\n",
            "Epoch 120/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9770\n",
            "Epoch 00120: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.0839 - accuracy: 0.9770 - val_loss: 2.6301 - val_accuracy: 0.7946\n",
            "Epoch 121/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9746\n",
            "Epoch 00121: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 818ms/step - loss: 0.0802 - accuracy: 0.9746 - val_loss: 2.4049 - val_accuracy: 0.7656\n",
            "Epoch 122/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9767\n",
            "Epoch 00122: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 819ms/step - loss: 0.0876 - accuracy: 0.9767 - val_loss: 2.4542 - val_accuracy: 0.7634\n",
            "Epoch 123/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9791\n",
            "Epoch 00123: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 49s 827ms/step - loss: 0.0822 - accuracy: 0.9791 - val_loss: 2.2120 - val_accuracy: 0.7958\n",
            "Epoch 124/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0807 - accuracy: 0.9781\n",
            "Epoch 00124: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0807 - accuracy: 0.9781 - val_loss: 2.1728 - val_accuracy: 0.7757\n",
            "Epoch 125/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0824 - accuracy: 0.9773\n",
            "Epoch 00125: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 819ms/step - loss: 0.0824 - accuracy: 0.9773 - val_loss: 2.2623 - val_accuracy: 0.7556\n",
            "Epoch 126/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9778\n",
            "Epoch 00126: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 49s 822ms/step - loss: 0.0734 - accuracy: 0.9778 - val_loss: 2.1731 - val_accuracy: 0.7946\n",
            "Epoch 127/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0834 - accuracy: 0.9754\n",
            "Epoch 00127: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 49s 824ms/step - loss: 0.0834 - accuracy: 0.9754 - val_loss: 2.9741 - val_accuracy: 0.7656\n",
            "Epoch 128/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9775\n",
            "Epoch 00128: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.0722 - accuracy: 0.9775 - val_loss: 2.1409 - val_accuracy: 0.7991\n",
            "Epoch 129/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0626 - accuracy: 0.9813\n",
            "Epoch 00129: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 807ms/step - loss: 0.0626 - accuracy: 0.9813 - val_loss: 2.7251 - val_accuracy: 0.7969\n",
            "Epoch 130/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9799\n",
            "Epoch 00130: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 815ms/step - loss: 0.0630 - accuracy: 0.9799 - val_loss: 2.8586 - val_accuracy: 0.7779\n",
            "Epoch 131/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9786\n",
            "Epoch 00131: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 49s 830ms/step - loss: 0.0725 - accuracy: 0.9786 - val_loss: 1.9588 - val_accuracy: 0.7846\n",
            "Epoch 132/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9791\n",
            "Epoch 00132: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.0652 - accuracy: 0.9791 - val_loss: 2.5957 - val_accuracy: 0.7868\n",
            "Epoch 133/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9826\n",
            "Epoch 00133: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 806ms/step - loss: 0.0601 - accuracy: 0.9826 - val_loss: 2.3098 - val_accuracy: 0.7812\n",
            "Epoch 134/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9805\n",
            "Epoch 00134: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 48s 811ms/step - loss: 0.0625 - accuracy: 0.9805 - val_loss: 2.4313 - val_accuracy: 0.8080\n",
            "Epoch 135/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0685 - accuracy: 0.9789\n",
            "Epoch 00135: val_accuracy improved from 0.80804 to 0.81696, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0685 - accuracy: 0.9789 - val_loss: 2.4132 - val_accuracy: 0.8170\n",
            "Epoch 136/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0744 - accuracy: 0.9789\n",
            "Epoch 00136: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 813ms/step - loss: 0.0744 - accuracy: 0.9789 - val_loss: 2.5361 - val_accuracy: 0.7946\n",
            "Epoch 137/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9799\n",
            "Epoch 00137: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 818ms/step - loss: 0.0658 - accuracy: 0.9799 - val_loss: 2.5157 - val_accuracy: 0.8114\n",
            "Epoch 138/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9840\n",
            "Epoch 00138: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 807ms/step - loss: 0.0510 - accuracy: 0.9840 - val_loss: 2.7893 - val_accuracy: 0.7868\n",
            "Epoch 139/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9805\n",
            "Epoch 00139: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 819ms/step - loss: 0.0866 - accuracy: 0.9805 - val_loss: 3.3446 - val_accuracy: 0.7500\n",
            "Epoch 140/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9759\n",
            "Epoch 00140: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 824ms/step - loss: 0.0865 - accuracy: 0.9759 - val_loss: 3.0908 - val_accuracy: 0.7857\n",
            "Epoch 141/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9832\n",
            "Epoch 00141: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 822ms/step - loss: 0.0847 - accuracy: 0.9832 - val_loss: 2.8651 - val_accuracy: 0.7913\n",
            "Epoch 142/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9823\n",
            "Epoch 00142: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 822ms/step - loss: 0.0541 - accuracy: 0.9823 - val_loss: 2.9379 - val_accuracy: 0.7857\n",
            "Epoch 143/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0534 - accuracy: 0.9832\n",
            "Epoch 00143: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 826ms/step - loss: 0.0534 - accuracy: 0.9832 - val_loss: 2.9704 - val_accuracy: 0.7812\n",
            "Epoch 144/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9807\n",
            "Epoch 00144: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 827ms/step - loss: 0.0728 - accuracy: 0.9807 - val_loss: 2.7468 - val_accuracy: 0.7891\n",
            "Epoch 145/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9818\n",
            "Epoch 00145: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0609 - accuracy: 0.9818 - val_loss: 3.0637 - val_accuracy: 0.7913\n",
            "Epoch 146/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0733 - accuracy: 0.9823\n",
            "Epoch 00146: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 827ms/step - loss: 0.0733 - accuracy: 0.9823 - val_loss: 2.9287 - val_accuracy: 0.7868\n",
            "Epoch 147/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9807\n",
            "Epoch 00147: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 821ms/step - loss: 0.0637 - accuracy: 0.9807 - val_loss: 2.4728 - val_accuracy: 0.7958\n",
            "Epoch 148/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0677 - accuracy: 0.9829\n",
            "Epoch 00148: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 825ms/step - loss: 0.0677 - accuracy: 0.9829 - val_loss: 2.6361 - val_accuracy: 0.7801\n",
            "Epoch 149/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9797\n",
            "Epoch 00149: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 823ms/step - loss: 0.0724 - accuracy: 0.9797 - val_loss: 2.6384 - val_accuracy: 0.8103\n",
            "Epoch 150/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9813\n",
            "Epoch 00150: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 815ms/step - loss: 0.0610 - accuracy: 0.9813 - val_loss: 2.8239 - val_accuracy: 0.8103\n",
            "Epoch 151/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9840\n",
            "Epoch 00151: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0686 - accuracy: 0.9840 - val_loss: 2.9998 - val_accuracy: 0.7835\n",
            "Epoch 152/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9802\n",
            "Epoch 00152: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 811ms/step - loss: 0.0648 - accuracy: 0.9802 - val_loss: 2.9252 - val_accuracy: 0.8013\n",
            "Epoch 153/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9807\n",
            "Epoch 00153: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 819ms/step - loss: 0.0734 - accuracy: 0.9807 - val_loss: 2.5913 - val_accuracy: 0.7734\n",
            "Epoch 154/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.9826\n",
            "Epoch 00154: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 809ms/step - loss: 0.0860 - accuracy: 0.9826 - val_loss: 3.5067 - val_accuracy: 0.7746\n",
            "Epoch 155/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9829\n",
            "Epoch 00155: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 809ms/step - loss: 0.0599 - accuracy: 0.9829 - val_loss: 3.5191 - val_accuracy: 0.7801\n",
            "Epoch 156/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9807\n",
            "Epoch 00156: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 829ms/step - loss: 0.0752 - accuracy: 0.9807 - val_loss: 2.7633 - val_accuracy: 0.7991\n",
            "Epoch 157/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9834\n",
            "Epoch 00157: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 818ms/step - loss: 0.0645 - accuracy: 0.9834 - val_loss: 3.1376 - val_accuracy: 0.8092\n",
            "Epoch 158/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9799\n",
            "Epoch 00158: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 47s 800ms/step - loss: 0.0644 - accuracy: 0.9799 - val_loss: 2.7432 - val_accuracy: 0.8080\n",
            "Epoch 159/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9853\n",
            "Epoch 00159: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 806ms/step - loss: 0.0522 - accuracy: 0.9853 - val_loss: 3.0527 - val_accuracy: 0.7879\n",
            "Epoch 160/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0881 - accuracy: 0.9791\n",
            "Epoch 00160: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 47s 801ms/step - loss: 0.0881 - accuracy: 0.9791 - val_loss: 2.8017 - val_accuracy: 0.7801\n",
            "Epoch 161/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9829\n",
            "Epoch 00161: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 809ms/step - loss: 0.0643 - accuracy: 0.9829 - val_loss: 2.6307 - val_accuracy: 0.7857\n",
            "Epoch 162/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9850\n",
            "Epoch 00162: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.0487 - accuracy: 0.9850 - val_loss: 2.6377 - val_accuracy: 0.7980\n",
            "Epoch 163/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9805\n",
            "Epoch 00163: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 810ms/step - loss: 0.0903 - accuracy: 0.9805 - val_loss: 2.5166 - val_accuracy: 0.7857\n",
            "Epoch 164/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9783\n",
            "Epoch 00164: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 816ms/step - loss: 0.0737 - accuracy: 0.9783 - val_loss: 2.7565 - val_accuracy: 0.7779\n",
            "Epoch 165/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0661 - accuracy: 0.9810\n",
            "Epoch 00165: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 47s 800ms/step - loss: 0.0661 - accuracy: 0.9810 - val_loss: 2.4179 - val_accuracy: 0.7757\n",
            "Epoch 166/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0520 - accuracy: 0.9848\n",
            "Epoch 00166: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 810ms/step - loss: 0.0520 - accuracy: 0.9848 - val_loss: 2.7256 - val_accuracy: 0.8002\n",
            "Epoch 167/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9872\n",
            "Epoch 00167: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 810ms/step - loss: 0.0450 - accuracy: 0.9872 - val_loss: 3.3589 - val_accuracy: 0.7746\n",
            "Epoch 168/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0476 - accuracy: 0.9864\n",
            "Epoch 00168: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 821ms/step - loss: 0.0476 - accuracy: 0.9864 - val_loss: 2.9730 - val_accuracy: 0.7935\n",
            "Epoch 169/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0640 - accuracy: 0.9805\n",
            "Epoch 00169: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 814ms/step - loss: 0.0640 - accuracy: 0.9805 - val_loss: 4.0788 - val_accuracy: 0.7444\n",
            "Epoch 170/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9829\n",
            "Epoch 00170: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 809ms/step - loss: 0.0575 - accuracy: 0.9829 - val_loss: 3.0793 - val_accuracy: 0.7857\n",
            "Epoch 171/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0511 - accuracy: 0.9874\n",
            "Epoch 00171: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 813ms/step - loss: 0.0511 - accuracy: 0.9874 - val_loss: 3.3287 - val_accuracy: 0.7589\n",
            "Epoch 172/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9869\n",
            "Epoch 00172: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 810ms/step - loss: 0.0453 - accuracy: 0.9869 - val_loss: 3.6037 - val_accuracy: 0.7734\n",
            "Epoch 173/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0546 - accuracy: 0.9850\n",
            "Epoch 00173: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 812ms/step - loss: 0.0546 - accuracy: 0.9850 - val_loss: 2.6887 - val_accuracy: 0.7812\n",
            "Epoch 174/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9861\n",
            "Epoch 00174: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 820ms/step - loss: 0.0473 - accuracy: 0.9861 - val_loss: 3.4404 - val_accuracy: 0.7812\n",
            "Epoch 175/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0580 - accuracy: 0.9858\n",
            "Epoch 00175: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 822ms/step - loss: 0.0580 - accuracy: 0.9858 - val_loss: 2.4120 - val_accuracy: 0.7891\n",
            "Epoch 176/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9856\n",
            "Epoch 00176: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 815ms/step - loss: 0.0590 - accuracy: 0.9856 - val_loss: 2.5126 - val_accuracy: 0.8092\n",
            "Epoch 177/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0539 - accuracy: 0.9845\n",
            "Epoch 00177: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 822ms/step - loss: 0.0539 - accuracy: 0.9845 - val_loss: 2.4243 - val_accuracy: 0.8114\n",
            "Epoch 178/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9837\n",
            "Epoch 00178: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 819ms/step - loss: 0.0411 - accuracy: 0.9837 - val_loss: 2.5070 - val_accuracy: 0.7958\n",
            "Epoch 179/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0531 - accuracy: 0.9858\n",
            "Epoch 00179: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 49s 822ms/step - loss: 0.0531 - accuracy: 0.9858 - val_loss: 2.7660 - val_accuracy: 0.7991\n",
            "Epoch 180/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9880\n",
            "Epoch 00180: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.0465 - accuracy: 0.9880 - val_loss: 2.6008 - val_accuracy: 0.8025\n",
            "Epoch 181/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9869\n",
            "Epoch 00181: val_accuracy did not improve from 0.81696\n",
            "59/59 [==============================] - 48s 809ms/step - loss: 0.0487 - accuracy: 0.9869 - val_loss: 2.6927 - val_accuracy: 0.8013\n",
            "Epoch 182/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9870\n",
            "Epoch 00182: val_accuracy improved from 0.81696 to 0.81920, saving model to seedling.h5\n",
            "59/59 [==============================] - 48s 810ms/step - loss: 0.0492 - accuracy: 0.9870 - val_loss: 2.0889 - val_accuracy: 0.8192\n",
            "Epoch 183/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0519 - accuracy: 0.9872\n",
            "Epoch 00183: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 795ms/step - loss: 0.0519 - accuracy: 0.9872 - val_loss: 2.9104 - val_accuracy: 0.7734\n",
            "Epoch 184/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9885\n",
            "Epoch 00184: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 792ms/step - loss: 0.0417 - accuracy: 0.9885 - val_loss: 2.7100 - val_accuracy: 0.7835\n",
            "Epoch 185/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9861\n",
            "Epoch 00185: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 790ms/step - loss: 0.0510 - accuracy: 0.9861 - val_loss: 2.8353 - val_accuracy: 0.7667\n",
            "Epoch 186/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9888\n",
            "Epoch 00186: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 790ms/step - loss: 0.0448 - accuracy: 0.9888 - val_loss: 2.6559 - val_accuracy: 0.7835\n",
            "Epoch 187/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9896\n",
            "Epoch 00187: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 798ms/step - loss: 0.0395 - accuracy: 0.9896 - val_loss: 2.5212 - val_accuracy: 0.8125\n",
            "Epoch 188/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0600 - accuracy: 0.9845\n",
            "Epoch 00188: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 46s 788ms/step - loss: 0.0600 - accuracy: 0.9845 - val_loss: 2.3870 - val_accuracy: 0.7857\n",
            "Epoch 189/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9869\n",
            "Epoch 00189: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 46s 788ms/step - loss: 0.0384 - accuracy: 0.9869 - val_loss: 2.8272 - val_accuracy: 0.8080\n",
            "Epoch 190/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9832\n",
            "Epoch 00190: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 46s 781ms/step - loss: 0.0591 - accuracy: 0.9832 - val_loss: 2.0226 - val_accuracy: 0.7935\n",
            "Epoch 191/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9914\n",
            "Epoch 00191: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 795ms/step - loss: 0.0321 - accuracy: 0.9914 - val_loss: 2.3072 - val_accuracy: 0.8092\n",
            "Epoch 192/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9885\n",
            "Epoch 00192: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 46s 785ms/step - loss: 0.0413 - accuracy: 0.9885 - val_loss: 2.3403 - val_accuracy: 0.8158\n",
            "Epoch 193/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9872\n",
            "Epoch 00193: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 789ms/step - loss: 0.0459 - accuracy: 0.9872 - val_loss: 2.4351 - val_accuracy: 0.7913\n",
            "Epoch 194/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9906\n",
            "Epoch 00194: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 788ms/step - loss: 0.0259 - accuracy: 0.9906 - val_loss: 2.6275 - val_accuracy: 0.8125\n",
            "Epoch 195/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9890\n",
            "Epoch 00195: val_accuracy did not improve from 0.81920\n",
            "59/59 [==============================] - 47s 790ms/step - loss: 0.0311 - accuracy: 0.9890 - val_loss: 2.4297 - val_accuracy: 0.8125\n",
            "Epoch 196/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0438 - accuracy: 0.9877\n",
            "Epoch 00196: val_accuracy improved from 0.81920 to 0.82031, saving model to seedling.h5\n",
            "59/59 [==============================] - 46s 787ms/step - loss: 0.0438 - accuracy: 0.9877 - val_loss: 2.4659 - val_accuracy: 0.8203\n",
            "Epoch 197/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9896\n",
            "Epoch 00197: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 786ms/step - loss: 0.0311 - accuracy: 0.9896 - val_loss: 3.0368 - val_accuracy: 0.7935\n",
            "Epoch 198/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9898\n",
            "Epoch 00198: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 784ms/step - loss: 0.0339 - accuracy: 0.9898 - val_loss: 2.8126 - val_accuracy: 0.7991\n",
            "Epoch 199/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9888\n",
            "Epoch 00199: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 785ms/step - loss: 0.0342 - accuracy: 0.9888 - val_loss: 3.0039 - val_accuracy: 0.8069\n",
            "Epoch 200/200\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0575 - accuracy: 0.9832\n",
            "Epoch 00200: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 775ms/step - loss: 0.0575 - accuracy: 0.9832 - val_loss: 2.9796 - val_accuracy: 0.7946\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdd92acdac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IuGvojyqtyL",
        "colab_type": "text"
      },
      "source": [
        "Train for 100 more epocs as model seems to still be learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjJ1_4y1xuBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "13472d0c-1b98-4512-fbaf-ce7f943e5d0e"
      },
      "source": [
        "model.fit(train_generator,\n",
        "          epochs=300,\n",
        "          initial_epoch=200,\n",
        "          steps_per_epoch= 3803//64,  #Number of training images//batch_size\n",
        "          validation_data=test_generator,\n",
        "          validation_steps = 947//64, #Number of test images//batch_size\n",
        "          callbacks = [model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 201/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9869\n",
            "Epoch 00201: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.0415 - accuracy: 0.9869 - val_loss: 3.5568 - val_accuracy: 0.7946\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9880\n",
            "Epoch 00202: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 48s 814ms/step - loss: 0.0563 - accuracy: 0.9880 - val_loss: 3.4955 - val_accuracy: 0.7645\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9815\n",
            "Epoch 00203: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 48s 811ms/step - loss: 0.0787 - accuracy: 0.9815 - val_loss: 2.4409 - val_accuracy: 0.7857\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9821\n",
            "Epoch 00204: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 49s 826ms/step - loss: 0.0623 - accuracy: 0.9821 - val_loss: 2.0777 - val_accuracy: 0.7589\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9874\n",
            "Epoch 00205: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 48s 819ms/step - loss: 0.0412 - accuracy: 0.9874 - val_loss: 1.9680 - val_accuracy: 0.7913\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9885\n",
            "Epoch 00206: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 802ms/step - loss: 0.0347 - accuracy: 0.9885 - val_loss: 2.3353 - val_accuracy: 0.7768\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9874\n",
            "Epoch 00207: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 800ms/step - loss: 0.0712 - accuracy: 0.9874 - val_loss: 2.7211 - val_accuracy: 0.7946\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9885\n",
            "Epoch 00208: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 797ms/step - loss: 0.0349 - accuracy: 0.9885 - val_loss: 2.6453 - val_accuracy: 0.7656\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9866\n",
            "Epoch 00209: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 796ms/step - loss: 0.0602 - accuracy: 0.9866 - val_loss: 2.5466 - val_accuracy: 0.7969\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9901\n",
            "Epoch 00210: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 791ms/step - loss: 0.0392 - accuracy: 0.9901 - val_loss: 2.5984 - val_accuracy: 0.7946\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9858\n",
            "Epoch 00211: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 803ms/step - loss: 0.0510 - accuracy: 0.9858 - val_loss: 3.4019 - val_accuracy: 0.7935\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9858\n",
            "Epoch 00212: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 787ms/step - loss: 0.0540 - accuracy: 0.9858 - val_loss: 2.4438 - val_accuracy: 0.7891\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9869\n",
            "Epoch 00213: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 802ms/step - loss: 0.0433 - accuracy: 0.9869 - val_loss: 2.6228 - val_accuracy: 0.7946\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9898\n",
            "Epoch 00214: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 799ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 2.5487 - val_accuracy: 0.8092\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9896\n",
            "Epoch 00215: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 794ms/step - loss: 0.0351 - accuracy: 0.9896 - val_loss: 3.3641 - val_accuracy: 0.7991\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9890\n",
            "Epoch 00216: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 799ms/step - loss: 0.0322 - accuracy: 0.9890 - val_loss: 3.2961 - val_accuracy: 0.7946\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9869\n",
            "Epoch 00217: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 795ms/step - loss: 0.0527 - accuracy: 0.9869 - val_loss: 2.9324 - val_accuracy: 0.7801\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0453 - accuracy: 0.9845\n",
            "Epoch 00218: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 797ms/step - loss: 0.0453 - accuracy: 0.9845 - val_loss: 2.8549 - val_accuracy: 0.8058\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9882\n",
            "Epoch 00219: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 788ms/step - loss: 0.0441 - accuracy: 0.9882 - val_loss: 3.1014 - val_accuracy: 0.7935\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0657 - accuracy: 0.9837\n",
            "Epoch 00220: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 803ms/step - loss: 0.0657 - accuracy: 0.9837 - val_loss: 2.8922 - val_accuracy: 0.7879\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9917\n",
            "Epoch 00221: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 791ms/step - loss: 0.0434 - accuracy: 0.9917 - val_loss: 2.3699 - val_accuracy: 0.8036\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9885\n",
            "Epoch 00222: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 790ms/step - loss: 0.0406 - accuracy: 0.9885 - val_loss: 2.3441 - val_accuracy: 0.8047\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9874\n",
            "Epoch 00223: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 793ms/step - loss: 0.0425 - accuracy: 0.9874 - val_loss: 3.9501 - val_accuracy: 0.7857\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9888\n",
            "Epoch 00224: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 790ms/step - loss: 0.0336 - accuracy: 0.9888 - val_loss: 3.3200 - val_accuracy: 0.7801\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0884 - accuracy: 0.9810\n",
            "Epoch 00225: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 786ms/step - loss: 0.0884 - accuracy: 0.9810 - val_loss: 2.8702 - val_accuracy: 0.7388\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0362 - accuracy: 0.9882\n",
            "Epoch 00226: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 782ms/step - loss: 0.0362 - accuracy: 0.9882 - val_loss: 2.2501 - val_accuracy: 0.7868\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0574 - accuracy: 0.9861\n",
            "Epoch 00227: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 794ms/step - loss: 0.0574 - accuracy: 0.9861 - val_loss: 2.8847 - val_accuracy: 0.7701\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9874\n",
            "Epoch 00228: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 790ms/step - loss: 0.0412 - accuracy: 0.9874 - val_loss: 3.4755 - val_accuracy: 0.7846\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0371 - accuracy: 0.9906\n",
            "Epoch 00229: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 784ms/step - loss: 0.0371 - accuracy: 0.9906 - val_loss: 3.1205 - val_accuracy: 0.7723\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0376 - accuracy: 0.9898\n",
            "Epoch 00230: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 795ms/step - loss: 0.0376 - accuracy: 0.9898 - val_loss: 3.3046 - val_accuracy: 0.7891\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9848\n",
            "Epoch 00231: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 786ms/step - loss: 0.0644 - accuracy: 0.9848 - val_loss: 3.3416 - val_accuracy: 0.7913\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9906\n",
            "Epoch 00232: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 786ms/step - loss: 0.0390 - accuracy: 0.9906 - val_loss: 3.3780 - val_accuracy: 0.7991\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9898\n",
            "Epoch 00233: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 789ms/step - loss: 0.0384 - accuracy: 0.9898 - val_loss: 2.9613 - val_accuracy: 0.7746\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9912\n",
            "Epoch 00234: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 784ms/step - loss: 0.0319 - accuracy: 0.9912 - val_loss: 2.7439 - val_accuracy: 0.8136\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9890\n",
            "Epoch 00235: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 780ms/step - loss: 0.0464 - accuracy: 0.9890 - val_loss: 3.1998 - val_accuracy: 0.8013\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9901\n",
            "Epoch 00236: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 791ms/step - loss: 0.0460 - accuracy: 0.9901 - val_loss: 4.9983 - val_accuracy: 0.7824\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9882\n",
            "Epoch 00237: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 785ms/step - loss: 0.0556 - accuracy: 0.9882 - val_loss: 3.3985 - val_accuracy: 0.8047\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9904\n",
            "Epoch 00238: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 786ms/step - loss: 0.0304 - accuracy: 0.9904 - val_loss: 3.4198 - val_accuracy: 0.8203\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0450 - accuracy: 0.9912\n",
            "Epoch 00239: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 784ms/step - loss: 0.0450 - accuracy: 0.9912 - val_loss: 2.9088 - val_accuracy: 0.8036\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0383 - accuracy: 0.9896\n",
            "Epoch 00240: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 791ms/step - loss: 0.0383 - accuracy: 0.9896 - val_loss: 3.3820 - val_accuracy: 0.8002\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0388 - accuracy: 0.9893\n",
            "Epoch 00241: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 785ms/step - loss: 0.0388 - accuracy: 0.9893 - val_loss: 3.0682 - val_accuracy: 0.7969\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9912\n",
            "Epoch 00242: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 789ms/step - loss: 0.0408 - accuracy: 0.9912 - val_loss: 2.9203 - val_accuracy: 0.8158\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9882\n",
            "Epoch 00243: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 781ms/step - loss: 0.0468 - accuracy: 0.9882 - val_loss: 3.2849 - val_accuracy: 0.8013\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9901\n",
            "Epoch 00244: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 779ms/step - loss: 0.0316 - accuracy: 0.9901 - val_loss: 3.4880 - val_accuracy: 0.8025\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0287 - accuracy: 0.9909\n",
            "Epoch 00245: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 786ms/step - loss: 0.0287 - accuracy: 0.9909 - val_loss: 2.8779 - val_accuracy: 0.8092\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9896\n",
            "Epoch 00246: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 785ms/step - loss: 0.0384 - accuracy: 0.9896 - val_loss: 2.7358 - val_accuracy: 0.8058\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0441 - accuracy: 0.9880\n",
            "Epoch 00247: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 791ms/step - loss: 0.0441 - accuracy: 0.9880 - val_loss: 2.5863 - val_accuracy: 0.7902\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9920\n",
            "Epoch 00248: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 779ms/step - loss: 0.0218 - accuracy: 0.9920 - val_loss: 2.3684 - val_accuracy: 0.8069\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0572 - accuracy: 0.9882\n",
            "Epoch 00249: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 47s 792ms/step - loss: 0.0572 - accuracy: 0.9882 - val_loss: 3.2584 - val_accuracy: 0.7790\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9893\n",
            "Epoch 00250: val_accuracy did not improve from 0.82031\n",
            "59/59 [==============================] - 46s 782ms/step - loss: 0.0442 - accuracy: 0.9893 - val_loss: 3.1066 - val_accuracy: 0.7958\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9920\n",
            "Epoch 00251: val_accuracy improved from 0.82031 to 0.83371, saving model to seedling.h5\n",
            "59/59 [==============================] - 46s 781ms/step - loss: 0.0260 - accuracy: 0.9920 - val_loss: 2.5657 - val_accuracy: 0.8337\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9898\n",
            "Epoch 00252: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 777ms/step - loss: 0.0392 - accuracy: 0.9898 - val_loss: 2.3450 - val_accuracy: 0.8058\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9896\n",
            "Epoch 00253: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 781ms/step - loss: 0.0317 - accuracy: 0.9896 - val_loss: 2.7636 - val_accuracy: 0.7824\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9901\n",
            "Epoch 00254: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 787ms/step - loss: 0.0497 - accuracy: 0.9901 - val_loss: 2.3423 - val_accuracy: 0.8259\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9898\n",
            "Epoch 00255: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 771ms/step - loss: 0.0325 - accuracy: 0.9898 - val_loss: 2.9413 - val_accuracy: 0.7868\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9896\n",
            "Epoch 00256: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 784ms/step - loss: 0.0340 - accuracy: 0.9896 - val_loss: 2.8035 - val_accuracy: 0.7969\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0326 - accuracy: 0.9909\n",
            "Epoch 00257: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 775ms/step - loss: 0.0326 - accuracy: 0.9909 - val_loss: 2.8107 - val_accuracy: 0.8225\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9917\n",
            "Epoch 00258: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 776ms/step - loss: 0.0341 - accuracy: 0.9917 - val_loss: 2.9434 - val_accuracy: 0.7690\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9898\n",
            "Epoch 00259: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 782ms/step - loss: 0.0422 - accuracy: 0.9898 - val_loss: 3.1775 - val_accuracy: 0.7924\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9912\n",
            "Epoch 00260: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 791ms/step - loss: 0.0275 - accuracy: 0.9912 - val_loss: 2.8320 - val_accuracy: 0.8259\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9914\n",
            "Epoch 00261: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 772ms/step - loss: 0.0385 - accuracy: 0.9914 - val_loss: 2.5046 - val_accuracy: 0.8103\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9896\n",
            "Epoch 00262: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 791ms/step - loss: 0.0367 - accuracy: 0.9896 - val_loss: 3.0857 - val_accuracy: 0.7846\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9901\n",
            "Epoch 00263: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 786ms/step - loss: 0.0318 - accuracy: 0.9901 - val_loss: 2.8125 - val_accuracy: 0.8192\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9922\n",
            "Epoch 00264: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 785ms/step - loss: 0.0236 - accuracy: 0.9922 - val_loss: 3.1137 - val_accuracy: 0.7846\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9904\n",
            "Epoch 00265: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 788ms/step - loss: 0.0247 - accuracy: 0.9904 - val_loss: 3.0313 - val_accuracy: 0.8080\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0398 - accuracy: 0.9898\n",
            "Epoch 00266: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 49s 822ms/step - loss: 0.0398 - accuracy: 0.9898 - val_loss: 3.3366 - val_accuracy: 0.7868\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0433 - accuracy: 0.9872\n",
            "Epoch 00267: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 49s 835ms/step - loss: 0.0433 - accuracy: 0.9872 - val_loss: 2.9297 - val_accuracy: 0.8103\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0230 - accuracy: 0.9917\n",
            "Epoch 00268: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 49s 827ms/step - loss: 0.0230 - accuracy: 0.9917 - val_loss: 3.1814 - val_accuracy: 0.8170\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9901\n",
            "Epoch 00269: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 48s 813ms/step - loss: 0.0306 - accuracy: 0.9901 - val_loss: 2.9256 - val_accuracy: 0.8025\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9901\n",
            "Epoch 00270: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 802ms/step - loss: 0.0308 - accuracy: 0.9901 - val_loss: 2.8770 - val_accuracy: 0.7790\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9914\n",
            "Epoch 00271: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 800ms/step - loss: 0.0342 - accuracy: 0.9914 - val_loss: 2.7018 - val_accuracy: 0.8103\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0360 - accuracy: 0.9904\n",
            "Epoch 00272: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 804ms/step - loss: 0.0360 - accuracy: 0.9904 - val_loss: 2.5113 - val_accuracy: 0.7991\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9920\n",
            "Epoch 00273: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 802ms/step - loss: 0.0227 - accuracy: 0.9920 - val_loss: 2.8273 - val_accuracy: 0.8013\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9898\n",
            "Epoch 00274: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.0307 - accuracy: 0.9898 - val_loss: 2.7911 - val_accuracy: 0.7690\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9922\n",
            "Epoch 00275: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 798ms/step - loss: 0.0308 - accuracy: 0.9922 - val_loss: 2.6723 - val_accuracy: 0.8013\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9936\n",
            "Epoch 00276: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 795ms/step - loss: 0.0275 - accuracy: 0.9936 - val_loss: 2.9222 - val_accuracy: 0.8002\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0260 - accuracy: 0.9933\n",
            "Epoch 00277: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 802ms/step - loss: 0.0260 - accuracy: 0.9933 - val_loss: 3.1227 - val_accuracy: 0.8203\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9904\n",
            "Epoch 00278: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 794ms/step - loss: 0.0254 - accuracy: 0.9904 - val_loss: 4.3203 - val_accuracy: 0.7835\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9922\n",
            "Epoch 00279: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 803ms/step - loss: 0.0275 - accuracy: 0.9922 - val_loss: 3.1869 - val_accuracy: 0.7991\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0307 - accuracy: 0.9914\n",
            "Epoch 00280: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 793ms/step - loss: 0.0307 - accuracy: 0.9914 - val_loss: 2.6200 - val_accuracy: 0.8080\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9896\n",
            "Epoch 00281: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 805ms/step - loss: 0.0416 - accuracy: 0.9896 - val_loss: 3.3750 - val_accuracy: 0.7969\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9896\n",
            "Epoch 00282: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 797ms/step - loss: 0.0321 - accuracy: 0.9896 - val_loss: 3.1428 - val_accuracy: 0.8080\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9877\n",
            "Epoch 00283: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 799ms/step - loss: 0.0422 - accuracy: 0.9877 - val_loss: 3.0809 - val_accuracy: 0.8237\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9928\n",
            "Epoch 00284: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 800ms/step - loss: 0.0264 - accuracy: 0.9928 - val_loss: 2.5328 - val_accuracy: 0.8225\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0592 - accuracy: 0.9874\n",
            "Epoch 00285: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 794ms/step - loss: 0.0592 - accuracy: 0.9874 - val_loss: 5.0480 - val_accuracy: 0.7065\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9914\n",
            "Epoch 00286: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 798ms/step - loss: 0.0298 - accuracy: 0.9914 - val_loss: 2.9345 - val_accuracy: 0.7935\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9933\n",
            "Epoch 00287: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 790ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 3.0569 - val_accuracy: 0.8025\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0211 - accuracy: 0.9925\n",
            "Epoch 00288: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 796ms/step - loss: 0.0211 - accuracy: 0.9925 - val_loss: 4.1245 - val_accuracy: 0.7801\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0563 - accuracy: 0.9885\n",
            "Epoch 00289: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 801ms/step - loss: 0.0563 - accuracy: 0.9885 - val_loss: 2.5956 - val_accuracy: 0.7701\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9922\n",
            "Epoch 00290: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 799ms/step - loss: 0.0231 - accuracy: 0.9922 - val_loss: 2.9255 - val_accuracy: 0.7958\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9928\n",
            "Epoch 00291: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.0286 - accuracy: 0.9928 - val_loss: 3.1623 - val_accuracy: 0.7958\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9920\n",
            "Epoch 00292: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 794ms/step - loss: 0.0239 - accuracy: 0.9920 - val_loss: 3.1045 - val_accuracy: 0.8114\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9930\n",
            "Epoch 00293: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 801ms/step - loss: 0.0286 - accuracy: 0.9930 - val_loss: 2.9685 - val_accuracy: 0.8248\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9941\n",
            "Epoch 00294: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 48s 808ms/step - loss: 0.0180 - accuracy: 0.9941 - val_loss: 2.7614 - val_accuracy: 0.8304\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0271 - accuracy: 0.9925\n",
            "Epoch 00295: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 46s 783ms/step - loss: 0.0271 - accuracy: 0.9925 - val_loss: 2.9836 - val_accuracy: 0.8114\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9920\n",
            "Epoch 00296: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 797ms/step - loss: 0.0244 - accuracy: 0.9920 - val_loss: 3.6126 - val_accuracy: 0.7913\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9914\n",
            "Epoch 00297: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 790ms/step - loss: 0.0329 - accuracy: 0.9914 - val_loss: 4.2054 - val_accuracy: 0.7879\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9880\n",
            "Epoch 00298: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 47s 797ms/step - loss: 0.0426 - accuracy: 0.9880 - val_loss: 2.9259 - val_accuracy: 0.8304\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9936\n",
            "Epoch 00299: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 48s 813ms/step - loss: 0.0185 - accuracy: 0.9936 - val_loss: 3.5141 - val_accuracy: 0.8158\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9938\n",
            "Epoch 00300: val_accuracy did not improve from 0.83371\n",
            "59/59 [==============================] - 48s 821ms/step - loss: 0.0239 - accuracy: 0.9938 - val_loss: 3.1513 - val_accuracy: 0.8058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdd926a8f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP3cvI3hIIg_",
        "colab_type": "text"
      },
      "source": [
        "We can see that the accuracy has not improved beyond 83.37% for many iterations so taking that as final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUP0WXGDI17g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e00fa327-5c0b-4201-aeec-cb7527ed2ea9"
      },
      "source": [
        "ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 75652\n",
            "drwx------  4 root root     4096 Aug 18 14:28 \u001b[0m\u001b[01;34mdrive\u001b[0m/\n",
            "drwxr-xr-x  1 root root     4096 Jul 30 16:30 \u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r--  1 root root    19863 Dec 11  2019 sample_submission.csv\n",
            "-rw-r--r--  1 root root 77396288 Aug 18 14:26 seedling.h5\n",
            "drwxr-xr-x  2 root root    36864 Aug 18 10:29 \u001b[01;34mtest\u001b[0m/\n",
            "drwxr-xr-x 14 root root     4096 Aug 18 10:29 \u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Pi3qfa0jHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9163205f-2c7b-4261-d09f-95618b8a6ea3"
      },
      "source": [
        "# Download the model\n",
        "files.download('seedling.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_9ec51646-c12f-4412-ab89-161e123f1019\", \"seedling.h5\", 77396288)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKweV5Z6bA2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testData = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwhPcS9Hbkyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8545ce22-b730-46a5-8ebc-eef08d694105"
      },
      "source": [
        "testData.file[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0021e90e4.png'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue8emis6brSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "for img in testData.file:\n",
        "  image = tf.keras.preprocessing.image.load_img(path='test/'+img, target_size=(60,60))\n",
        "  input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
        "  input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
        "  predictions.append(np.argmax((model.predict(input_arr))[0]))\n",
        "# input_arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZLL3bOJcGkq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fb195b77-f71d-4ef9-ec32-a1c2d955ed23"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10,\n",
              " 5,\n",
              " 2,\n",
              " 3,\n",
              " 11,\n",
              " 6,\n",
              " 3,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 5,\n",
              " 10,\n",
              " 9,\n",
              " 8,\n",
              " 11,\n",
              " 5,\n",
              " 8,\n",
              " 8,\n",
              " 3,\n",
              " 9,\n",
              " 3,\n",
              " 10,\n",
              " 9,\n",
              " 11,\n",
              " 11,\n",
              " 7,\n",
              " 8,\n",
              " 8,\n",
              " 3,\n",
              " 11,\n",
              " 3,\n",
              " 9,\n",
              " 0,\n",
              " 7,\n",
              " 6,\n",
              " 0,\n",
              " 3,\n",
              " 6,\n",
              " 3,\n",
              " 7,\n",
              " 3,\n",
              " 4,\n",
              " 3,\n",
              " 10,\n",
              " 11,\n",
              " 11,\n",
              " 7,\n",
              " 9,\n",
              " 7,\n",
              " 10,\n",
              " 6,\n",
              " 11,\n",
              " 2,\n",
              " 6,\n",
              " 7,\n",
              " 5,\n",
              " 1,\n",
              " 10,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 2,\n",
              " 1,\n",
              " 4,\n",
              " 7,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 9,\n",
              " 8,\n",
              " 10,\n",
              " 3,\n",
              " 8,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 6,\n",
              " 8,\n",
              " 8,\n",
              " 6,\n",
              " 11,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 9,\n",
              " 2,\n",
              " 2,\n",
              " 8,\n",
              " 5,\n",
              " 6,\n",
              " 1,\n",
              " 0,\n",
              " 5,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 3,\n",
              " 8,\n",
              " 7,\n",
              " 10,\n",
              " 5,\n",
              " 5,\n",
              " 8,\n",
              " 11,\n",
              " 5,\n",
              " 5,\n",
              " 3,\n",
              " 9,\n",
              " 2,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 3,\n",
              " 3,\n",
              " 3,\n",
              " 11,\n",
              " 2,\n",
              " 1,\n",
              " 11,\n",
              " 3,\n",
              " 3,\n",
              " 11,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 4,\n",
              " 1,\n",
              " 7,\n",
              " 5,\n",
              " 3,\n",
              " 11,\n",
              " 11,\n",
              " 3,\n",
              " 3,\n",
              " 5,\n",
              " 4,\n",
              " 5,\n",
              " 1,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 10,\n",
              " 9,\n",
              " 3,\n",
              " 5,\n",
              " 11,\n",
              " 4,\n",
              " 10,\n",
              " 11,\n",
              " 1,\n",
              " 8,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 6,\n",
              " 10,\n",
              " 3,\n",
              " 5,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 8,\n",
              " 8,\n",
              " 9,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 3,\n",
              " 3,\n",
              " 9,\n",
              " 7,\n",
              " 9,\n",
              " 3,\n",
              " 3,\n",
              " 10,\n",
              " 0,\n",
              " 2,\n",
              " 0,\n",
              " 0,\n",
              " 6,\n",
              " 10,\n",
              " 6,\n",
              " 4,\n",
              " 8,\n",
              " 6,\n",
              " 3,\n",
              " 5,\n",
              " 5,\n",
              " 10,\n",
              " 6,\n",
              " 1,\n",
              " 8,\n",
              " 4,\n",
              " 1,\n",
              " 6,\n",
              " 4,\n",
              " 3,\n",
              " 6,\n",
              " 3,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 1,\n",
              " 10,\n",
              " 2,\n",
              " 5,\n",
              " 3,\n",
              " 5,\n",
              " 9,\n",
              " 5,\n",
              " 1,\n",
              " 5,\n",
              " 6,\n",
              " 10,\n",
              " 2,\n",
              " 7,\n",
              " 6,\n",
              " 10,\n",
              " 3,\n",
              " 8,\n",
              " 6,\n",
              " 3,\n",
              " 10,\n",
              " 4,\n",
              " 5,\n",
              " 5,\n",
              " 3,\n",
              " 3,\n",
              " 7,\n",
              " 9,\n",
              " 2,\n",
              " 9,\n",
              " 2,\n",
              " 3,\n",
              " 6,\n",
              " 0,\n",
              " 2,\n",
              " 10,\n",
              " 1,\n",
              " 11,\n",
              " 3,\n",
              " 7,\n",
              " 10,\n",
              " 10,\n",
              " 9,\n",
              " 6,\n",
              " 5,\n",
              " 10,\n",
              " 5,\n",
              " 3,\n",
              " 6,\n",
              " 7,\n",
              " 3,\n",
              " 3,\n",
              " 4,\n",
              " 2,\n",
              " 1,\n",
              " 10,\n",
              " 3,\n",
              " 0,\n",
              " 7,\n",
              " 3,\n",
              " 7,\n",
              " 10,\n",
              " 2,\n",
              " 5,\n",
              " 9,\n",
              " 8,\n",
              " 9,\n",
              " 1,\n",
              " 0,\n",
              " 6,\n",
              " 10,\n",
              " 3,\n",
              " 10,\n",
              " 8,\n",
              " 3,\n",
              " 5,\n",
              " 2,\n",
              " 0,\n",
              " 6,\n",
              " 7,\n",
              " 11,\n",
              " 7,\n",
              " 6,\n",
              " 3,\n",
              " 11,\n",
              " 7,\n",
              " 3,\n",
              " 0,\n",
              " 8,\n",
              " 1,\n",
              " 7,\n",
              " 10,\n",
              " 11,\n",
              " 6,\n",
              " 6,\n",
              " 10,\n",
              " 5,\n",
              " 9,\n",
              " 3,\n",
              " 2,\n",
              " 10,\n",
              " 11,\n",
              " 2,\n",
              " 1,\n",
              " 6,\n",
              " 4,\n",
              " 2,\n",
              " 11,\n",
              " 6,\n",
              " 8,\n",
              " 4,\n",
              " 9,\n",
              " 6,\n",
              " 11,\n",
              " 1,\n",
              " 6,\n",
              " 10,\n",
              " 11,\n",
              " 1,\n",
              " 1,\n",
              " 10,\n",
              " 10,\n",
              " 10,\n",
              " 6,\n",
              " 11,\n",
              " 10,\n",
              " 11,\n",
              " 1,\n",
              " 8,\n",
              " 6,\n",
              " 0,\n",
              " 11,\n",
              " 2,\n",
              " 1,\n",
              " 9,\n",
              " 0,\n",
              " 9,\n",
              " 10,\n",
              " 8,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 5,\n",
              " 9,\n",
              " 7,\n",
              " 0,\n",
              " 11,\n",
              " 6,\n",
              " 5,\n",
              " 6,\n",
              " 1,\n",
              " 5,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 8,\n",
              " 10,\n",
              " 10,\n",
              " 3,\n",
              " 1,\n",
              " 2,\n",
              " 6,\n",
              " 1,\n",
              " 0,\n",
              " 8,\n",
              " 10,\n",
              " 10,\n",
              " 11,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 11,\n",
              " 11,\n",
              " 0,\n",
              " 6,\n",
              " 4,\n",
              " 3,\n",
              " 1,\n",
              " 11,\n",
              " 6,\n",
              " 8,\n",
              " 6,\n",
              " 6,\n",
              " 8,\n",
              " 11,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 11,\n",
              " 8,\n",
              " 8,\n",
              " 5,\n",
              " 1,\n",
              " 9,\n",
              " 1,\n",
              " 9,\n",
              " 3,\n",
              " 7,\n",
              " 11,\n",
              " 8,\n",
              " 9,\n",
              " 1,\n",
              " 6,\n",
              " 5,\n",
              " 10,\n",
              " 8,\n",
              " 4,\n",
              " 6,\n",
              " 6,\n",
              " 10,\n",
              " 10,\n",
              " 6,\n",
              " 11,\n",
              " 5,\n",
              " 3,\n",
              " 2,\n",
              " 5,\n",
              " 1,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 5,\n",
              " 10,\n",
              " 9,\n",
              " 2,\n",
              " 11,\n",
              " 10,\n",
              " 6,\n",
              " 2,\n",
              " 5,\n",
              " 10,\n",
              " 10,\n",
              " 10,\n",
              " 6,\n",
              " 6,\n",
              " 5,\n",
              " 6,\n",
              " 3,\n",
              " 1,\n",
              " 8,\n",
              " 0,\n",
              " 3,\n",
              " 3,\n",
              " 1,\n",
              " 9,\n",
              " 11,\n",
              " 10,\n",
              " 3,\n",
              " 10,\n",
              " 8,\n",
              " 9,\n",
              " 11,\n",
              " 3,\n",
              " 8,\n",
              " 1,\n",
              " 3,\n",
              " 11,\n",
              " 3,\n",
              " 10,\n",
              " 2,\n",
              " 0,\n",
              " 5,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 6,\n",
              " 8,\n",
              " 10,\n",
              " 11,\n",
              " 4,\n",
              " 3,\n",
              " 9,\n",
              " 10,\n",
              " 3,\n",
              " 9,\n",
              " 8,\n",
              " 6,\n",
              " 4,\n",
              " 6,\n",
              " 6,\n",
              " 0,\n",
              " 1,\n",
              " 8,\n",
              " 11,\n",
              " 6,\n",
              " 4,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 4,\n",
              " 10,\n",
              " 10,\n",
              " 5,\n",
              " 3,\n",
              " 7,\n",
              " 10,\n",
              " 10,\n",
              " 1,\n",
              " 1,\n",
              " 7,\n",
              " 2,\n",
              " 6,\n",
              " 6,\n",
              " 10,\n",
              " 1,\n",
              " 10,\n",
              " 3,\n",
              " 10,\n",
              " 6,\n",
              " 6,\n",
              " 6,\n",
              " 8,\n",
              " 3,\n",
              " 7,\n",
              " 9,\n",
              " 3,\n",
              " 5,\n",
              " 11,\n",
              " 11,\n",
              " 5,\n",
              " 8,\n",
              " 10,\n",
              " 1,\n",
              " 11,\n",
              " 9,\n",
              " 1,\n",
              " 6,\n",
              " 8,\n",
              " 8,\n",
              " 0,\n",
              " 3,\n",
              " 9,\n",
              " 11,\n",
              " 0,\n",
              " 2,\n",
              " 4,\n",
              " 8,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 6,\n",
              " 6,\n",
              " 3,\n",
              " 3,\n",
              " 6,\n",
              " 8,\n",
              " 10,\n",
              " 10,\n",
              " 6,\n",
              " 6,\n",
              " 0,\n",
              " 2,\n",
              " 8,\n",
              " 2,\n",
              " 2,\n",
              " 4,\n",
              " 0,\n",
              " 2,\n",
              " 3,\n",
              " 3,\n",
              " 7,\n",
              " 9,\n",
              " 5,\n",
              " 8,\n",
              " 7,\n",
              " 0,\n",
              " 6,\n",
              " 0,\n",
              " 5,\n",
              " 10,\n",
              " 0,\n",
              " 8,\n",
              " 4,\n",
              " 5,\n",
              " 9,\n",
              " 7,\n",
              " 11,\n",
              " 6,\n",
              " 8,\n",
              " 8,\n",
              " 6,\n",
              " 1,\n",
              " 9,\n",
              " 11,\n",
              " 5,\n",
              " 8,\n",
              " 3,\n",
              " 8,\n",
              " 6,\n",
              " 6,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 3,\n",
              " 7,\n",
              " 11,\n",
              " 8,\n",
              " 3,\n",
              " 5,\n",
              " 8,\n",
              " 8,\n",
              " 10,\n",
              " 10,\n",
              " 8,\n",
              " 7,\n",
              " 6,\n",
              " 3,\n",
              " 10,\n",
              " 8,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 8,\n",
              " 3,\n",
              " 6,\n",
              " 11,\n",
              " 11,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 9,\n",
              " 8,\n",
              " 8,\n",
              " 8,\n",
              " 11,\n",
              " 5,\n",
              " 3,\n",
              " 10,\n",
              " 4,\n",
              " 3,\n",
              " 9,\n",
              " 1,\n",
              " 6,\n",
              " 5,\n",
              " 7,\n",
              " 1,\n",
              " 11,\n",
              " 0,\n",
              " 10,\n",
              " 10,\n",
              " 1,\n",
              " 3,\n",
              " 6,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 7,\n",
              " 9,\n",
              " 1,\n",
              " 5,\n",
              " 0,\n",
              " 11,\n",
              " 6,\n",
              " 4,\n",
              " 2,\n",
              " 10,\n",
              " 8,\n",
              " 5,\n",
              " 3,\n",
              " 6,\n",
              " 5,\n",
              " 3,\n",
              " 8,\n",
              " 4,\n",
              " 6,\n",
              " 5,\n",
              " 8,\n",
              " 0,\n",
              " 2,\n",
              " 10,\n",
              " 8,\n",
              " 8,\n",
              " 11,\n",
              " 3,\n",
              " 10,\n",
              " 3,\n",
              " 6,\n",
              " 11,\n",
              " 9,\n",
              " 1,\n",
              " 3,\n",
              " 0,\n",
              " 1,\n",
              " 10,\n",
              " 0,\n",
              " 6,\n",
              " 11,\n",
              " 8,\n",
              " 3,\n",
              " 11,\n",
              " 10,\n",
              " 5,\n",
              " 11,\n",
              " 6,\n",
              " 3,\n",
              " 0,\n",
              " 11,\n",
              " 0,\n",
              " 3,\n",
              " 8,\n",
              " 8,\n",
              " 1,\n",
              " 1,\n",
              " 6,\n",
              " 11,\n",
              " 2,\n",
              " 1,\n",
              " 11,\n",
              " 7,\n",
              " 6,\n",
              " 11,\n",
              " 2,\n",
              " 6,\n",
              " 8,\n",
              " 5,\n",
              " 4,\n",
              " 11,\n",
              " 8,\n",
              " 0,\n",
              " 11,\n",
              " 11,\n",
              " 10,\n",
              " 6,\n",
              " 1,\n",
              " 3,\n",
              " 5,\n",
              " 5,\n",
              " 9,\n",
              " 1,\n",
              " 3,\n",
              " 9,\n",
              " 10,\n",
              " 10,\n",
              " 2,\n",
              " 8,\n",
              " 1,\n",
              " 6,\n",
              " 11,\n",
              " 3,\n",
              " 6,\n",
              " 8,\n",
              " 3,\n",
              " 1,\n",
              " 1,\n",
              " 11,\n",
              " 6,\n",
              " 9,\n",
              " 6,\n",
              " 10,\n",
              " 2,\n",
              " 8,\n",
              " 6,\n",
              " 2,\n",
              " 5,\n",
              " 5,\n",
              " 1,\n",
              " 8,\n",
              " 11,\n",
              " 1,\n",
              " 10,\n",
              " 11,\n",
              " 7,\n",
              " 3,\n",
              " 9,\n",
              " 9,\n",
              " 2,\n",
              " 5,\n",
              " 3,\n",
              " 9,\n",
              " 2,\n",
              " 1,\n",
              " 11,\n",
              " 6,\n",
              " 1,\n",
              " 8,\n",
              " 11,\n",
              " 2,\n",
              " 6,\n",
              " 3,\n",
              " 4,\n",
              " 6,\n",
              " 0,\n",
              " 8,\n",
              " 8,\n",
              " 9,\n",
              " 6,\n",
              " 11,\n",
              " 11,\n",
              " 1,\n",
              " 4]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O4cO8iPotEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6b702256-5347-43d3-f6d3-4b0b3eac2c5f"
      },
      "source": [
        "np.argmax(predictions[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_KBLhzUsXZ5",
        "colab_type": "text"
      },
      "source": [
        "### 2. Build the model using image augmentation techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PILK8Xx7rry3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_generator= tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
        "                                                               width_shift_range=0.2,\n",
        "                                                               height_shift_range=0.2,\n",
        "                                                               horizontal_flip=True,\n",
        "                                                               validation_split=.2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_T95hqtzQkF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a028a378-b3a7-40c8-da3a-544af7cdb07d"
      },
      "source": [
        "#Build training generator. \n",
        "train_generator = img_generator.flow_from_directory('train',\n",
        "                                                    target_size=(img_size, img_size),\n",
        "                                                    subset='training',\n",
        "                                                    batch_size=64)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3803 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi5xkZYXzWy2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9ce246dd-e364-4c31-9736-dee914781ea8"
      },
      "source": [
        "#Build training generator. \n",
        "test_generator = img_generator.flow_from_directory('train',\n",
        "                                                    target_size=(img_size, img_size),\n",
        "                                                    subset='validation',\n",
        "                                                    batch_size=64)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 947 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8T5EyBtKshqU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d901a43e-6bc3-4159-a216-d2cebd046b3a"
      },
      "source": [
        "#Lets check the features (images) and Labels (flower class) returned by ImageDataGenerator\n",
        "X, y = next(train_generator)\n",
        "\n",
        "print('Input features shape', X.shape)\n",
        "print('Actual labels shape', y.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input features shape (64, 60, 60, 3)\n",
            "Actual labels shape (64, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGfBijPTz8N9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear any previous model from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Initialize model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization(input_shape=(img_size,img_size,3,)))\n",
        "\n",
        "#Add Conv Layer\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Conv Layer\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Max Pool layer\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Add Dense Layers after flattening the data\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "#Add Dropout\n",
        "model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "#Add Output Layer\n",
        "model.add(tf.keras.layers.Dense(12, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tw6uQGDS0CJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify Loass and Optimizer\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QTSn4ly0HZU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "3a67bb32-ec74-4ff2-ab40-ff7ac2c22db7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 60, 60, 3)         12        \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 58, 58, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 58, 58, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 56, 56, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               6422656   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12)                1548      \n",
            "=================================================================\n",
            "Total params: 6,443,992\n",
            "Trainable params: 6,443,794\n",
            "Non-trainable params: 198\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7Ooj36Q0JY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the best model using model checkpoint callback\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('seedling1.h5', \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_accuracy', \n",
        "                                                    mode='max', \n",
        "                                                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-LG1usm0SPJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "601edfde-d4e7-441c-f51b-f19dfa6d1c58"
      },
      "source": [
        "# Running the model for 100 epochs\n",
        "model.fit(train_generator, \n",
        "          epochs=100,\n",
        "          steps_per_epoch= 3803/64,  #Number of training images//batch_size\n",
        "          validation_data=test_generator,\n",
        "          validation_steps = 947//64, #Number of test images//batch_size\n",
        "          callbacks = [model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 3.1000 - accuracy: 0.2480\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.14286, saving model to seedling1.h5\n",
            "60/59 [==============================] - 60s 999ms/step - loss: 3.1000 - accuracy: 0.2480 - val_loss: 5.8268 - val_accuracy: 0.1429\n",
            "Epoch 2/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 2.1319 - accuracy: 0.3050\n",
            "Epoch 00002: val_accuracy did not improve from 0.14286\n",
            "60/59 [==============================] - 60s 1s/step - loss: 2.1319 - accuracy: 0.3050 - val_loss: 32.2260 - val_accuracy: 0.0960\n",
            "Epoch 3/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 2.0266 - accuracy: 0.3250\n",
            "Epoch 00003: val_accuracy did not improve from 0.14286\n",
            "60/59 [==============================] - 59s 991ms/step - loss: 2.0266 - accuracy: 0.3250 - val_loss: 58.0811 - val_accuracy: 0.0558\n",
            "Epoch 4/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.9502 - accuracy: 0.3387\n",
            "Epoch 00004: val_accuracy did not improve from 0.14286\n",
            "60/59 [==============================] - 59s 983ms/step - loss: 1.9502 - accuracy: 0.3387 - val_loss: 41.7095 - val_accuracy: 0.0882\n",
            "Epoch 5/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.8921 - accuracy: 0.3476\n",
            "Epoch 00005: val_accuracy improved from 0.14286 to 0.19420, saving model to seedling1.h5\n",
            "60/59 [==============================] - 59s 983ms/step - loss: 1.8921 - accuracy: 0.3476 - val_loss: 19.9988 - val_accuracy: 0.1942\n",
            "Epoch 6/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.8625 - accuracy: 0.3471\n",
            "Epoch 00006: val_accuracy improved from 0.19420 to 0.29799, saving model to seedling1.h5\n",
            "60/59 [==============================] - 59s 980ms/step - loss: 1.8625 - accuracy: 0.3471 - val_loss: 4.2954 - val_accuracy: 0.2980\n",
            "Epoch 7/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.7979 - accuracy: 0.3726\n",
            "Epoch 00007: val_accuracy improved from 0.29799 to 0.38058, saving model to seedling1.h5\n",
            "60/59 [==============================] - 57s 949ms/step - loss: 1.7979 - accuracy: 0.3726 - val_loss: 1.9193 - val_accuracy: 0.3806\n",
            "Epoch 8/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.7342 - accuracy: 0.3865\n",
            "Epoch 00008: val_accuracy improved from 0.38058 to 0.46094, saving model to seedling1.h5\n",
            "60/59 [==============================] - 57s 956ms/step - loss: 1.7342 - accuracy: 0.3865 - val_loss: 1.7388 - val_accuracy: 0.4609\n",
            "Epoch 9/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.7389 - accuracy: 0.3855\n",
            "Epoch 00009: val_accuracy improved from 0.46094 to 0.49330, saving model to seedling1.h5\n",
            "60/59 [==============================] - 57s 953ms/step - loss: 1.7389 - accuracy: 0.3855 - val_loss: 1.5702 - val_accuracy: 0.4933\n",
            "Epoch 10/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.6792 - accuracy: 0.4226\n",
            "Epoch 00010: val_accuracy did not improve from 0.49330\n",
            "60/59 [==============================] - 57s 948ms/step - loss: 1.6792 - accuracy: 0.4226 - val_loss: 1.5605 - val_accuracy: 0.4866\n",
            "Epoch 11/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.6185 - accuracy: 0.4241\n",
            "Epoch 00011: val_accuracy improved from 0.49330 to 0.49442, saving model to seedling1.h5\n",
            "60/59 [==============================] - 57s 957ms/step - loss: 1.6185 - accuracy: 0.4241 - val_loss: 1.4524 - val_accuracy: 0.4944\n",
            "Epoch 12/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.5459 - accuracy: 0.4686\n",
            "Epoch 00012: val_accuracy improved from 0.49442 to 0.55804, saving model to seedling1.h5\n",
            "60/59 [==============================] - 57s 951ms/step - loss: 1.5459 - accuracy: 0.4686 - val_loss: 1.4316 - val_accuracy: 0.5580\n",
            "Epoch 13/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.5321 - accuracy: 0.4646\n",
            "Epoch 00013: val_accuracy improved from 0.55804 to 0.57143, saving model to seedling1.h5\n",
            "60/59 [==============================] - 57s 949ms/step - loss: 1.5321 - accuracy: 0.4646 - val_loss: 1.3417 - val_accuracy: 0.5714\n",
            "Epoch 14/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.4524 - accuracy: 0.4915\n",
            "Epoch 00014: val_accuracy did not improve from 0.57143\n",
            "60/59 [==============================] - 56s 937ms/step - loss: 1.4524 - accuracy: 0.4915 - val_loss: 1.3424 - val_accuracy: 0.5592\n",
            "Epoch 15/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.4100 - accuracy: 0.5120\n",
            "Epoch 00015: val_accuracy improved from 0.57143 to 0.58259, saving model to seedling1.h5\n",
            "60/59 [==============================] - 57s 949ms/step - loss: 1.4100 - accuracy: 0.5120 - val_loss: 1.3706 - val_accuracy: 0.5826\n",
            "Epoch 16/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.4004 - accuracy: 0.5138\n",
            "Epoch 00016: val_accuracy did not improve from 0.58259\n",
            "60/59 [==============================] - 57s 949ms/step - loss: 1.4004 - accuracy: 0.5138 - val_loss: 1.4968 - val_accuracy: 0.5625\n",
            "Epoch 17/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.3813 - accuracy: 0.5291\n",
            "Epoch 00017: val_accuracy improved from 0.58259 to 0.59263, saving model to seedling1.h5\n",
            "60/59 [==============================] - 57s 943ms/step - loss: 1.3813 - accuracy: 0.5291 - val_loss: 1.2902 - val_accuracy: 0.5926\n",
            "Epoch 18/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.3244 - accuracy: 0.5393\n",
            "Epoch 00018: val_accuracy improved from 0.59263 to 0.64174, saving model to seedling1.h5\n",
            "60/59 [==============================] - 56s 940ms/step - loss: 1.3244 - accuracy: 0.5393 - val_loss: 1.1984 - val_accuracy: 0.6417\n",
            "Epoch 19/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.2829 - accuracy: 0.5383\n",
            "Epoch 00019: val_accuracy did not improve from 0.64174\n",
            "60/59 [==============================] - 57s 946ms/step - loss: 1.2829 - accuracy: 0.5383 - val_loss: 1.2042 - val_accuracy: 0.6105\n",
            "Epoch 20/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.3006 - accuracy: 0.5501\n",
            "Epoch 00020: val_accuracy improved from 0.64174 to 0.64621, saving model to seedling1.h5\n",
            "60/59 [==============================] - 56s 930ms/step - loss: 1.3006 - accuracy: 0.5501 - val_loss: 1.1432 - val_accuracy: 0.6462\n",
            "Epoch 21/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.2378 - accuracy: 0.5701\n",
            "Epoch 00021: val_accuracy improved from 0.64621 to 0.65848, saving model to seedling1.h5\n",
            "60/59 [==============================] - 55s 925ms/step - loss: 1.2378 - accuracy: 0.5701 - val_loss: 1.1274 - val_accuracy: 0.6585\n",
            "Epoch 22/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.2760 - accuracy: 0.5551\n",
            "Epoch 00022: val_accuracy improved from 0.65848 to 0.67076, saving model to seedling1.h5\n",
            "60/59 [==============================] - 56s 931ms/step - loss: 1.2760 - accuracy: 0.5551 - val_loss: 1.0629 - val_accuracy: 0.6708\n",
            "Epoch 23/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.1929 - accuracy: 0.5837\n",
            "Epoch 00023: val_accuracy improved from 0.67076 to 0.67299, saving model to seedling1.h5\n",
            "60/59 [==============================] - 55s 919ms/step - loss: 1.1929 - accuracy: 0.5837 - val_loss: 1.1452 - val_accuracy: 0.6730\n",
            "Epoch 24/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.1851 - accuracy: 0.5866\n",
            "Epoch 00024: val_accuracy did not improve from 0.67299\n",
            "60/59 [==============================] - 55s 917ms/step - loss: 1.1851 - accuracy: 0.5866 - val_loss: 1.0687 - val_accuracy: 0.6618\n",
            "Epoch 25/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.1534 - accuracy: 0.5985\n",
            "Epoch 00025: val_accuracy did not improve from 0.67299\n",
            "60/59 [==============================] - 56s 926ms/step - loss: 1.1534 - accuracy: 0.5985 - val_loss: 1.2566 - val_accuracy: 0.6373\n",
            "Epoch 26/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.1201 - accuracy: 0.6027\n",
            "Epoch 00026: val_accuracy improved from 0.67299 to 0.70089, saving model to seedling1.h5\n",
            "60/59 [==============================] - 55s 916ms/step - loss: 1.1201 - accuracy: 0.6027 - val_loss: 0.9879 - val_accuracy: 0.7009\n",
            "Epoch 27/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.1099 - accuracy: 0.6164\n",
            "Epoch 00027: val_accuracy did not improve from 0.70089\n",
            "60/59 [==============================] - 55s 924ms/step - loss: 1.1099 - accuracy: 0.6164 - val_loss: 1.1153 - val_accuracy: 0.6696\n",
            "Epoch 28/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.0837 - accuracy: 0.6216\n",
            "Epoch 00028: val_accuracy did not improve from 0.70089\n",
            "60/59 [==============================] - 55s 918ms/step - loss: 1.0837 - accuracy: 0.6216 - val_loss: 1.0753 - val_accuracy: 0.6975\n",
            "Epoch 29/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.0788 - accuracy: 0.6298\n",
            "Epoch 00029: val_accuracy improved from 0.70089 to 0.73549, saving model to seedling1.h5\n",
            "60/59 [==============================] - 56s 928ms/step - loss: 1.0788 - accuracy: 0.6298 - val_loss: 0.8837 - val_accuracy: 0.7355\n",
            "Epoch 30/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.0039 - accuracy: 0.6484\n",
            "Epoch 00030: val_accuracy did not improve from 0.73549\n",
            "60/59 [==============================] - 55s 921ms/step - loss: 1.0039 - accuracy: 0.6484 - val_loss: 0.9642 - val_accuracy: 0.7344\n",
            "Epoch 31/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9878 - accuracy: 0.6471\n",
            "Epoch 00031: val_accuracy did not improve from 0.73549\n",
            "60/59 [==============================] - 56s 926ms/step - loss: 0.9878 - accuracy: 0.6471 - val_loss: 1.1252 - val_accuracy: 0.6897\n",
            "Epoch 32/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9720 - accuracy: 0.6550\n",
            "Epoch 00032: val_accuracy improved from 0.73549 to 0.75335, saving model to seedling1.h5\n",
            "60/59 [==============================] - 56s 937ms/step - loss: 0.9720 - accuracy: 0.6550 - val_loss: 0.9844 - val_accuracy: 0.7533\n",
            "Epoch 33/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9911 - accuracy: 0.6529\n",
            "Epoch 00033: val_accuracy did not improve from 0.75335\n",
            "60/59 [==============================] - 56s 927ms/step - loss: 0.9911 - accuracy: 0.6529 - val_loss: 0.9167 - val_accuracy: 0.7132\n",
            "Epoch 34/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9515 - accuracy: 0.6682\n",
            "Epoch 00034: val_accuracy did not improve from 0.75335\n",
            "60/59 [==============================] - 56s 927ms/step - loss: 0.9515 - accuracy: 0.6682 - val_loss: 0.9471 - val_accuracy: 0.7388\n",
            "Epoch 35/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9377 - accuracy: 0.6674\n",
            "Epoch 00035: val_accuracy did not improve from 0.75335\n",
            "60/59 [==============================] - 56s 929ms/step - loss: 0.9377 - accuracy: 0.6674 - val_loss: 0.8281 - val_accuracy: 0.7377\n",
            "Epoch 36/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9367 - accuracy: 0.6679\n",
            "Epoch 00036: val_accuracy did not improve from 0.75335\n",
            "60/59 [==============================] - 56s 926ms/step - loss: 0.9367 - accuracy: 0.6679 - val_loss: 0.8943 - val_accuracy: 0.7400\n",
            "Epoch 37/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8965 - accuracy: 0.6860\n",
            "Epoch 00037: val_accuracy did not improve from 0.75335\n",
            "60/59 [==============================] - 55s 925ms/step - loss: 0.8965 - accuracy: 0.6860 - val_loss: 0.9079 - val_accuracy: 0.7288\n",
            "Epoch 38/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8475 - accuracy: 0.6968\n",
            "Epoch 00038: val_accuracy did not improve from 0.75335\n",
            "60/59 [==============================] - 56s 932ms/step - loss: 0.8475 - accuracy: 0.6968 - val_loss: 0.9396 - val_accuracy: 0.7489\n",
            "Epoch 39/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9213 - accuracy: 0.6879\n",
            "Epoch 00039: val_accuracy did not improve from 0.75335\n",
            "60/59 [==============================] - 56s 935ms/step - loss: 0.9213 - accuracy: 0.6879 - val_loss: 0.9140 - val_accuracy: 0.7221\n",
            "Epoch 40/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8730 - accuracy: 0.6987\n",
            "Epoch 00040: val_accuracy improved from 0.75335 to 0.78683, saving model to seedling1.h5\n",
            "60/59 [==============================] - 56s 938ms/step - loss: 0.8730 - accuracy: 0.6987 - val_loss: 0.7898 - val_accuracy: 0.7868\n",
            "Epoch 41/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8547 - accuracy: 0.6947\n",
            "Epoch 00041: val_accuracy did not improve from 0.78683\n",
            "60/59 [==============================] - 56s 936ms/step - loss: 0.8547 - accuracy: 0.6947 - val_loss: 0.8292 - val_accuracy: 0.7533\n",
            "Epoch 42/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8371 - accuracy: 0.7081\n",
            "Epoch 00042: val_accuracy did not improve from 0.78683\n",
            "60/59 [==============================] - 56s 929ms/step - loss: 0.8371 - accuracy: 0.7081 - val_loss: 0.8093 - val_accuracy: 0.7723\n",
            "Epoch 43/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8270 - accuracy: 0.7076\n",
            "Epoch 00043: val_accuracy did not improve from 0.78683\n",
            "60/59 [==============================] - 55s 921ms/step - loss: 0.8270 - accuracy: 0.7076 - val_loss: 0.8904 - val_accuracy: 0.7801\n",
            "Epoch 44/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8059 - accuracy: 0.7105\n",
            "Epoch 00044: val_accuracy did not improve from 0.78683\n",
            "60/59 [==============================] - 55s 924ms/step - loss: 0.8059 - accuracy: 0.7105 - val_loss: 1.2954 - val_accuracy: 0.6886\n",
            "Epoch 45/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8206 - accuracy: 0.7029\n",
            "Epoch 00045: val_accuracy did not improve from 0.78683\n",
            "60/59 [==============================] - 55s 918ms/step - loss: 0.8206 - accuracy: 0.7029 - val_loss: 1.0571 - val_accuracy: 0.7277\n",
            "Epoch 46/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7686 - accuracy: 0.7210\n",
            "Epoch 00046: val_accuracy improved from 0.78683 to 0.79911, saving model to seedling1.h5\n",
            "60/59 [==============================] - 56s 926ms/step - loss: 0.7686 - accuracy: 0.7210 - val_loss: 0.8595 - val_accuracy: 0.7991\n",
            "Epoch 47/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7556 - accuracy: 0.7252\n",
            "Epoch 00047: val_accuracy did not improve from 0.79911\n",
            "60/59 [==============================] - 55s 920ms/step - loss: 0.7556 - accuracy: 0.7252 - val_loss: 0.7674 - val_accuracy: 0.7958\n",
            "Epoch 48/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7615 - accuracy: 0.7297\n",
            "Epoch 00048: val_accuracy did not improve from 0.79911\n",
            "60/59 [==============================] - 55s 922ms/step - loss: 0.7615 - accuracy: 0.7297 - val_loss: 0.6804 - val_accuracy: 0.7946\n",
            "Epoch 49/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7062 - accuracy: 0.7410\n",
            "Epoch 00049: val_accuracy did not improve from 0.79911\n",
            "60/59 [==============================] - 55s 921ms/step - loss: 0.7062 - accuracy: 0.7410 - val_loss: 0.7815 - val_accuracy: 0.7645\n",
            "Epoch 50/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7460 - accuracy: 0.7286\n",
            "Epoch 00050: val_accuracy did not improve from 0.79911\n",
            "60/59 [==============================] - 55s 920ms/step - loss: 0.7460 - accuracy: 0.7286 - val_loss: 0.7564 - val_accuracy: 0.7924\n",
            "Epoch 51/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7165 - accuracy: 0.7447\n",
            "Epoch 00051: val_accuracy did not improve from 0.79911\n",
            "60/59 [==============================] - 56s 928ms/step - loss: 0.7165 - accuracy: 0.7447 - val_loss: 0.7352 - val_accuracy: 0.7958\n",
            "Epoch 52/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7060 - accuracy: 0.7384\n",
            "Epoch 00052: val_accuracy improved from 0.79911 to 0.80915, saving model to seedling1.h5\n",
            "60/59 [==============================] - 56s 931ms/step - loss: 0.7060 - accuracy: 0.7384 - val_loss: 0.7537 - val_accuracy: 0.8092\n",
            "Epoch 53/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6968 - accuracy: 0.7473\n",
            "Epoch 00053: val_accuracy did not improve from 0.80915\n",
            "60/59 [==============================] - 56s 930ms/step - loss: 0.6968 - accuracy: 0.7473 - val_loss: 0.7965 - val_accuracy: 0.7980\n",
            "Epoch 54/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7311 - accuracy: 0.7392\n",
            "Epoch 00054: val_accuracy did not improve from 0.80915\n",
            "60/59 [==============================] - 57s 942ms/step - loss: 0.7311 - accuracy: 0.7392 - val_loss: 0.7980 - val_accuracy: 0.8080\n",
            "Epoch 55/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7209 - accuracy: 0.7534\n",
            "Epoch 00055: val_accuracy did not improve from 0.80915\n",
            "60/59 [==============================] - 56s 928ms/step - loss: 0.7209 - accuracy: 0.7534 - val_loss: 0.8501 - val_accuracy: 0.7835\n",
            "Epoch 56/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6877 - accuracy: 0.7415\n",
            "Epoch 00056: val_accuracy did not improve from 0.80915\n",
            "60/59 [==============================] - 55s 915ms/step - loss: 0.6877 - accuracy: 0.7415 - val_loss: 0.7441 - val_accuracy: 0.7924\n",
            "Epoch 57/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6818 - accuracy: 0.7602\n",
            "Epoch 00057: val_accuracy improved from 0.80915 to 0.81362, saving model to seedling1.h5\n",
            "60/59 [==============================] - 55s 912ms/step - loss: 0.6818 - accuracy: 0.7602 - val_loss: 0.6995 - val_accuracy: 0.8136\n",
            "Epoch 58/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6409 - accuracy: 0.7560\n",
            "Epoch 00058: val_accuracy did not improve from 0.81362\n",
            "60/59 [==============================] - 55s 918ms/step - loss: 0.6409 - accuracy: 0.7560 - val_loss: 0.7920 - val_accuracy: 0.8114\n",
            "Epoch 59/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6375 - accuracy: 0.7676\n",
            "Epoch 00059: val_accuracy improved from 0.81362 to 0.83817, saving model to seedling1.h5\n",
            "60/59 [==============================] - 54s 906ms/step - loss: 0.6375 - accuracy: 0.7676 - val_loss: 0.6930 - val_accuracy: 0.8382\n",
            "Epoch 60/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6367 - accuracy: 0.7694\n",
            "Epoch 00060: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 55s 909ms/step - loss: 0.6367 - accuracy: 0.7694 - val_loss: 0.8044 - val_accuracy: 0.8058\n",
            "Epoch 61/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6040 - accuracy: 0.7781\n",
            "Epoch 00061: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 54s 908ms/step - loss: 0.6040 - accuracy: 0.7781 - val_loss: 0.9477 - val_accuracy: 0.7768\n",
            "Epoch 62/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6089 - accuracy: 0.7762\n",
            "Epoch 00062: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 54s 895ms/step - loss: 0.6089 - accuracy: 0.7762 - val_loss: 0.8118 - val_accuracy: 0.8315\n",
            "Epoch 63/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5937 - accuracy: 0.7796\n",
            "Epoch 00063: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 53s 877ms/step - loss: 0.5937 - accuracy: 0.7796 - val_loss: 0.7706 - val_accuracy: 0.8270\n",
            "Epoch 64/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5839 - accuracy: 0.7831\n",
            "Epoch 00064: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 53s 885ms/step - loss: 0.5839 - accuracy: 0.7831 - val_loss: 0.7483 - val_accuracy: 0.8259\n",
            "Epoch 65/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6231 - accuracy: 0.7710\n",
            "Epoch 00065: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 53s 885ms/step - loss: 0.6231 - accuracy: 0.7710 - val_loss: 0.7891 - val_accuracy: 0.7969\n",
            "Epoch 66/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5929 - accuracy: 0.7796\n",
            "Epoch 00066: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 53s 891ms/step - loss: 0.5929 - accuracy: 0.7796 - val_loss: 0.8560 - val_accuracy: 0.8237\n",
            "Epoch 67/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5984 - accuracy: 0.7852\n",
            "Epoch 00067: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 54s 892ms/step - loss: 0.5984 - accuracy: 0.7852 - val_loss: 0.6127 - val_accuracy: 0.8326\n",
            "Epoch 68/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6058 - accuracy: 0.7854\n",
            "Epoch 00068: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 54s 897ms/step - loss: 0.6058 - accuracy: 0.7854 - val_loss: 0.6869 - val_accuracy: 0.8382\n",
            "Epoch 69/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5552 - accuracy: 0.7970\n",
            "Epoch 00069: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 54s 895ms/step - loss: 0.5552 - accuracy: 0.7970 - val_loss: 0.7773 - val_accuracy: 0.8292\n",
            "Epoch 70/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5503 - accuracy: 0.8033\n",
            "Epoch 00070: val_accuracy did not improve from 0.83817\n",
            "60/59 [==============================] - 53s 886ms/step - loss: 0.5503 - accuracy: 0.8033 - val_loss: 0.7321 - val_accuracy: 0.8337\n",
            "Epoch 71/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5646 - accuracy: 0.7936\n",
            "Epoch 00071: val_accuracy improved from 0.83817 to 0.83929, saving model to seedling1.h5\n",
            "60/59 [==============================] - 54s 898ms/step - loss: 0.5646 - accuracy: 0.7936 - val_loss: 0.6153 - val_accuracy: 0.8393\n",
            "Epoch 72/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5354 - accuracy: 0.7991\n",
            "Epoch 00072: val_accuracy did not improve from 0.83929\n",
            "60/59 [==============================] - 52s 861ms/step - loss: 0.5354 - accuracy: 0.7991 - val_loss: 0.5710 - val_accuracy: 0.8348\n",
            "Epoch 73/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5428 - accuracy: 0.8049\n",
            "Epoch 00073: val_accuracy did not improve from 0.83929\n",
            "60/59 [==============================] - 51s 856ms/step - loss: 0.5428 - accuracy: 0.8049 - val_loss: 0.7244 - val_accuracy: 0.8371\n",
            "Epoch 74/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5771 - accuracy: 0.7936\n",
            "Epoch 00074: val_accuracy did not improve from 0.83929\n",
            "60/59 [==============================] - 52s 862ms/step - loss: 0.5771 - accuracy: 0.7936 - val_loss: 0.7304 - val_accuracy: 0.8158\n",
            "Epoch 75/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5450 - accuracy: 0.7981\n",
            "Epoch 00075: val_accuracy improved from 0.83929 to 0.86384, saving model to seedling1.h5\n",
            "60/59 [==============================] - 52s 861ms/step - loss: 0.5450 - accuracy: 0.7981 - val_loss: 0.5919 - val_accuracy: 0.8638\n",
            "Epoch 76/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.8059\n",
            "Epoch 00076: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 52s 861ms/step - loss: 0.5259 - accuracy: 0.8059 - val_loss: 0.6671 - val_accuracy: 0.8482\n",
            "Epoch 77/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4914 - accuracy: 0.8225\n",
            "Epoch 00077: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 52s 870ms/step - loss: 0.4914 - accuracy: 0.8225 - val_loss: 0.7313 - val_accuracy: 0.8415\n",
            "Epoch 78/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5174 - accuracy: 0.8117\n",
            "Epoch 00078: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 51s 857ms/step - loss: 0.5174 - accuracy: 0.8117 - val_loss: 0.5924 - val_accuracy: 0.8627\n",
            "Epoch 79/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5134 - accuracy: 0.8057\n",
            "Epoch 00079: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 52s 864ms/step - loss: 0.5134 - accuracy: 0.8057 - val_loss: 0.5921 - val_accuracy: 0.8616\n",
            "Epoch 80/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4947 - accuracy: 0.8257\n",
            "Epoch 00080: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 52s 859ms/step - loss: 0.4947 - accuracy: 0.8257 - val_loss: 0.6836 - val_accuracy: 0.8371\n",
            "Epoch 81/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5133 - accuracy: 0.8073\n",
            "Epoch 00081: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 51s 844ms/step - loss: 0.5133 - accuracy: 0.8073 - val_loss: 0.6656 - val_accuracy: 0.8571\n",
            "Epoch 82/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4939 - accuracy: 0.8280\n",
            "Epoch 00082: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 50s 833ms/step - loss: 0.4939 - accuracy: 0.8280 - val_loss: 0.6195 - val_accuracy: 0.8404\n",
            "Epoch 83/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4956 - accuracy: 0.8141\n",
            "Epoch 00083: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 50s 832ms/step - loss: 0.4956 - accuracy: 0.8141 - val_loss: 0.6787 - val_accuracy: 0.8493\n",
            "Epoch 84/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4841 - accuracy: 0.8309\n",
            "Epoch 00084: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 50s 833ms/step - loss: 0.4841 - accuracy: 0.8309 - val_loss: 0.7186 - val_accuracy: 0.8538\n",
            "Epoch 85/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4812 - accuracy: 0.8275\n",
            "Epoch 00085: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 50s 830ms/step - loss: 0.4812 - accuracy: 0.8275 - val_loss: 0.6840 - val_accuracy: 0.8504\n",
            "Epoch 86/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4853 - accuracy: 0.8186\n",
            "Epoch 00086: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 50s 838ms/step - loss: 0.4853 - accuracy: 0.8186 - val_loss: 0.6484 - val_accuracy: 0.8404\n",
            "Epoch 87/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4834 - accuracy: 0.8251\n",
            "Epoch 00087: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 50s 831ms/step - loss: 0.4834 - accuracy: 0.8251 - val_loss: 0.6694 - val_accuracy: 0.8583\n",
            "Epoch 88/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4562 - accuracy: 0.8283\n",
            "Epoch 00088: val_accuracy did not improve from 0.86384\n",
            "60/59 [==============================] - 50s 841ms/step - loss: 0.4562 - accuracy: 0.8283 - val_loss: 0.6354 - val_accuracy: 0.8460\n",
            "Epoch 89/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4532 - accuracy: 0.8309\n",
            "Epoch 00089: val_accuracy improved from 0.86384 to 0.88616, saving model to seedling1.h5\n",
            "60/59 [==============================] - 51s 844ms/step - loss: 0.4532 - accuracy: 0.8309 - val_loss: 0.6443 - val_accuracy: 0.8862\n",
            "Epoch 90/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4571 - accuracy: 0.8399\n",
            "Epoch 00090: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 51s 848ms/step - loss: 0.4571 - accuracy: 0.8399 - val_loss: 0.6777 - val_accuracy: 0.8627\n",
            "Epoch 91/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4631 - accuracy: 0.8246\n",
            "Epoch 00091: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 50s 837ms/step - loss: 0.4631 - accuracy: 0.8246 - val_loss: 0.7999 - val_accuracy: 0.8471\n",
            "Epoch 92/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4574 - accuracy: 0.8304\n",
            "Epoch 00092: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 50s 828ms/step - loss: 0.4574 - accuracy: 0.8304 - val_loss: 0.6950 - val_accuracy: 0.8661\n",
            "Epoch 93/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4633 - accuracy: 0.8288\n",
            "Epoch 00093: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 50s 830ms/step - loss: 0.4633 - accuracy: 0.8288 - val_loss: 0.9054 - val_accuracy: 0.8281\n",
            "Epoch 94/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4446 - accuracy: 0.8367\n",
            "Epoch 00094: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 50s 831ms/step - loss: 0.4446 - accuracy: 0.8367 - val_loss: 0.6659 - val_accuracy: 0.8605\n",
            "Epoch 95/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.8435\n",
            "Epoch 00095: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 50s 842ms/step - loss: 0.4386 - accuracy: 0.8435 - val_loss: 0.6670 - val_accuracy: 0.8739\n",
            "Epoch 96/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8425\n",
            "Epoch 00096: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 51s 846ms/step - loss: 0.4440 - accuracy: 0.8425 - val_loss: 0.5141 - val_accuracy: 0.8850\n",
            "Epoch 97/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4209 - accuracy: 0.8488\n",
            "Epoch 00097: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 51s 847ms/step - loss: 0.4209 - accuracy: 0.8488 - val_loss: 0.4546 - val_accuracy: 0.8627\n",
            "Epoch 98/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4527 - accuracy: 0.8351\n",
            "Epoch 00098: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 50s 841ms/step - loss: 0.4527 - accuracy: 0.8351 - val_loss: 0.4946 - val_accuracy: 0.8627\n",
            "Epoch 99/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4459 - accuracy: 0.8430\n",
            "Epoch 00099: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 51s 845ms/step - loss: 0.4459 - accuracy: 0.8430 - val_loss: 0.5917 - val_accuracy: 0.8471\n",
            "Epoch 100/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4233 - accuracy: 0.8483\n",
            "Epoch 00100: val_accuracy did not improve from 0.88616\n",
            "60/59 [==============================] - 51s 846ms/step - loss: 0.4233 - accuracy: 0.8483 - val_loss: 0.8018 - val_accuracy: 0.8393\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f05e2e26048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDTlBlKY5-2o",
        "colab_type": "text"
      },
      "source": [
        "We have an observation that the validation accuracy is better than test which raises suspection in model.\n",
        "\n",
        "Changing the model by reducing the dropouts so that the model can learn more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdIsLrAk6jKu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear any previous model from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Initialize model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization(input_shape=(img_size,img_size,3,)))\n",
        "\n",
        "#Add Conv Layer\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Conv Layer\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "#normalize data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Max Pool layer\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "#Add Dense Layers after flattening the data\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "\n",
        "#Add Dropout\n",
        "model.add(tf.keras.layers.Dropout(0.10)) #Reducing the dropouts to .1\n",
        "\n",
        "#Add Output Layer\n",
        "model.add(tf.keras.layers.Dense(12, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZX7QsS36wXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Specify Loass and Optimizer\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51hqVMMm64QF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "01a4ce54-1ece-479b-bbe5-22920b98b669"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 60, 60, 3)         12        \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 58, 58, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 58, 58, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 56, 56, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               6422656   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12)                1548      \n",
            "=================================================================\n",
            "Total params: 6,443,992\n",
            "Trainable params: 6,443,794\n",
            "Non-trainable params: 198\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHKfa3a365vB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the best model using model checkpoint callback\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('seedling2.h5', \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_accuracy', \n",
        "                                                    mode='max', \n",
        "                                                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpxKNEAk6_-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e94712ec-1d02-4046-9be8-6fcca1c4e4eb"
      },
      "source": [
        "# Running the model for 100 epochs\n",
        "model.fit(train_generator, \n",
        "          epochs=100,\n",
        "          steps_per_epoch= 3803/64,  #Number of training images//batch_size\n",
        "          validation_data=test_generator,\n",
        "          validation_steps = 947//64, #Number of test images//batch_size\n",
        "          callbacks = [model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 4.5607 - accuracy: 0.2369\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.05580, saving model to seedling2.h5\n",
            "60/59 [==============================] - 54s 904ms/step - loss: 4.5607 - accuracy: 0.2369 - val_loss: 7.1932 - val_accuracy: 0.0558\n",
            "Epoch 2/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 2.1996 - accuracy: 0.2848\n",
            "Epoch 00002: val_accuracy improved from 0.05580 to 0.14062, saving model to seedling2.h5\n",
            "60/59 [==============================] - 53s 887ms/step - loss: 2.1996 - accuracy: 0.2848 - val_loss: 24.3122 - val_accuracy: 0.1406\n",
            "Epoch 3/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 2.0185 - accuracy: 0.3350\n",
            "Epoch 00003: val_accuracy did not improve from 0.14062\n",
            "60/59 [==============================] - 53s 886ms/step - loss: 2.0185 - accuracy: 0.3350 - val_loss: 80.7389 - val_accuracy: 0.0971\n",
            "Epoch 4/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.8321 - accuracy: 0.3915\n",
            "Epoch 00004: val_accuracy did not improve from 0.14062\n",
            "60/59 [==============================] - 53s 884ms/step - loss: 1.8321 - accuracy: 0.3915 - val_loss: 80.5526 - val_accuracy: 0.0971\n",
            "Epoch 5/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.7326 - accuracy: 0.4289\n",
            "Epoch 00005: val_accuracy improved from 0.14062 to 0.16853, saving model to seedling2.h5\n",
            "60/59 [==============================] - 53s 877ms/step - loss: 1.7326 - accuracy: 0.4289 - val_loss: 45.2647 - val_accuracy: 0.1685\n",
            "Epoch 6/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.6406 - accuracy: 0.4528\n",
            "Epoch 00006: val_accuracy improved from 0.16853 to 0.29018, saving model to seedling2.h5\n",
            "60/59 [==============================] - 54s 893ms/step - loss: 1.6406 - accuracy: 0.4528 - val_loss: 12.2441 - val_accuracy: 0.2902\n",
            "Epoch 7/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.5264 - accuracy: 0.4930\n",
            "Epoch 00007: val_accuracy improved from 0.29018 to 0.41741, saving model to seedling2.h5\n",
            "60/59 [==============================] - 53s 889ms/step - loss: 1.5264 - accuracy: 0.4930 - val_loss: 4.4837 - val_accuracy: 0.4174\n",
            "Epoch 8/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.4212 - accuracy: 0.5306\n",
            "Epoch 00008: val_accuracy improved from 0.41741 to 0.54576, saving model to seedling2.h5\n",
            "60/59 [==============================] - 54s 896ms/step - loss: 1.4212 - accuracy: 0.5306 - val_loss: 1.6400 - val_accuracy: 0.5458\n",
            "Epoch 9/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.3730 - accuracy: 0.5475\n",
            "Epoch 00009: val_accuracy did not improve from 0.54576\n",
            "60/59 [==============================] - 52s 873ms/step - loss: 1.3730 - accuracy: 0.5475 - val_loss: 1.6916 - val_accuracy: 0.5223\n",
            "Epoch 10/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.3260 - accuracy: 0.5493\n",
            "Epoch 00010: val_accuracy improved from 0.54576 to 0.57924, saving model to seedling2.h5\n",
            "60/59 [==============================] - 53s 883ms/step - loss: 1.3260 - accuracy: 0.5493 - val_loss: 1.3993 - val_accuracy: 0.5792\n",
            "Epoch 11/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.2935 - accuracy: 0.5596\n",
            "Epoch 00011: val_accuracy did not improve from 0.57924\n",
            "60/59 [==============================] - 53s 879ms/step - loss: 1.2935 - accuracy: 0.5596 - val_loss: 1.8633 - val_accuracy: 0.5145\n",
            "Epoch 12/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.2157 - accuracy: 0.5822\n",
            "Epoch 00012: val_accuracy improved from 0.57924 to 0.58482, saving model to seedling2.h5\n",
            "60/59 [==============================] - 52s 874ms/step - loss: 1.2157 - accuracy: 0.5822 - val_loss: 1.4668 - val_accuracy: 0.5848\n",
            "Epoch 13/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.1595 - accuracy: 0.6043\n",
            "Epoch 00013: val_accuracy improved from 0.58482 to 0.62165, saving model to seedling2.h5\n",
            "60/59 [==============================] - 52s 870ms/step - loss: 1.1595 - accuracy: 0.6043 - val_loss: 1.3972 - val_accuracy: 0.6217\n",
            "Epoch 14/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.1151 - accuracy: 0.6227\n",
            "Epoch 00014: val_accuracy improved from 0.62165 to 0.65737, saving model to seedling2.h5\n",
            "60/59 [==============================] - 52s 865ms/step - loss: 1.1151 - accuracy: 0.6227 - val_loss: 1.0946 - val_accuracy: 0.6574\n",
            "Epoch 15/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.0648 - accuracy: 0.6358\n",
            "Epoch 00015: val_accuracy improved from 0.65737 to 0.66518, saving model to seedling2.h5\n",
            "60/59 [==============================] - 52s 866ms/step - loss: 1.0648 - accuracy: 0.6358 - val_loss: 1.2397 - val_accuracy: 0.6652\n",
            "Epoch 16/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.0567 - accuracy: 0.6429\n",
            "Epoch 00016: val_accuracy improved from 0.66518 to 0.72098, saving model to seedling2.h5\n",
            "60/59 [==============================] - 52s 873ms/step - loss: 1.0567 - accuracy: 0.6429 - val_loss: 0.9178 - val_accuracy: 0.7210\n",
            "Epoch 17/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.0024 - accuracy: 0.6605\n",
            "Epoch 00017: val_accuracy did not improve from 0.72098\n",
            "60/59 [==============================] - 52s 865ms/step - loss: 1.0024 - accuracy: 0.6605 - val_loss: 1.0488 - val_accuracy: 0.6808\n",
            "Epoch 18/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 1.0248 - accuracy: 0.6487\n",
            "Epoch 00018: val_accuracy did not improve from 0.72098\n",
            "60/59 [==============================] - 52s 859ms/step - loss: 1.0248 - accuracy: 0.6487 - val_loss: 1.0130 - val_accuracy: 0.7109\n",
            "Epoch 19/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9416 - accuracy: 0.6682\n",
            "Epoch 00019: val_accuracy did not improve from 0.72098\n",
            "60/59 [==============================] - 51s 847ms/step - loss: 0.9416 - accuracy: 0.6682 - val_loss: 1.1504 - val_accuracy: 0.6819\n",
            "Epoch 20/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9049 - accuracy: 0.6837\n",
            "Epoch 00020: val_accuracy did not improve from 0.72098\n",
            "60/59 [==============================] - 51s 850ms/step - loss: 0.9049 - accuracy: 0.6837 - val_loss: 0.9554 - val_accuracy: 0.7143\n",
            "Epoch 21/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9295 - accuracy: 0.6763\n",
            "Epoch 00021: val_accuracy improved from 0.72098 to 0.72210, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 845ms/step - loss: 0.9295 - accuracy: 0.6763 - val_loss: 0.9412 - val_accuracy: 0.7221\n",
            "Epoch 22/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.9566 - accuracy: 0.6800\n",
            "Epoch 00022: val_accuracy improved from 0.72210 to 0.72879, saving model to seedling2.h5\n",
            "60/59 [==============================] - 52s 867ms/step - loss: 0.9566 - accuracy: 0.6800 - val_loss: 0.8815 - val_accuracy: 0.7288\n",
            "Epoch 23/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8765 - accuracy: 0.6884\n",
            "Epoch 00023: val_accuracy improved from 0.72879 to 0.73996, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 852ms/step - loss: 0.8765 - accuracy: 0.6884 - val_loss: 0.8799 - val_accuracy: 0.7400\n",
            "Epoch 24/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8715 - accuracy: 0.7055\n",
            "Epoch 00024: val_accuracy did not improve from 0.73996\n",
            "60/59 [==============================] - 51s 846ms/step - loss: 0.8715 - accuracy: 0.7055 - val_loss: 0.9949 - val_accuracy: 0.7210\n",
            "Epoch 25/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.8275 - accuracy: 0.7100\n",
            "Epoch 00025: val_accuracy did not improve from 0.73996\n",
            "60/59 [==============================] - 51s 855ms/step - loss: 0.8275 - accuracy: 0.7100 - val_loss: 0.9863 - val_accuracy: 0.7266\n",
            "Epoch 26/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7686 - accuracy: 0.7326\n",
            "Epoch 00026: val_accuracy improved from 0.73996 to 0.74442, saving model to seedling2.h5\n",
            "60/59 [==============================] - 52s 867ms/step - loss: 0.7686 - accuracy: 0.7326 - val_loss: 0.9638 - val_accuracy: 0.7444\n",
            "Epoch 27/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7574 - accuracy: 0.7355\n",
            "Epoch 00027: val_accuracy did not improve from 0.74442\n",
            "60/59 [==============================] - 51s 846ms/step - loss: 0.7574 - accuracy: 0.7355 - val_loss: 0.9599 - val_accuracy: 0.7388\n",
            "Epoch 28/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7392 - accuracy: 0.7389\n",
            "Epoch 00028: val_accuracy did not improve from 0.74442\n",
            "60/59 [==============================] - 50s 839ms/step - loss: 0.7392 - accuracy: 0.7389 - val_loss: 1.0977 - val_accuracy: 0.7321\n",
            "Epoch 29/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7468 - accuracy: 0.7331\n",
            "Epoch 00029: val_accuracy improved from 0.74442 to 0.76897, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 848ms/step - loss: 0.7468 - accuracy: 0.7331 - val_loss: 0.8369 - val_accuracy: 0.7690\n",
            "Epoch 30/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6924 - accuracy: 0.7502\n",
            "Epoch 00030: val_accuracy improved from 0.76897 to 0.78460, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 845ms/step - loss: 0.6924 - accuracy: 0.7502 - val_loss: 0.7578 - val_accuracy: 0.7846\n",
            "Epoch 31/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.7070 - accuracy: 0.7486\n",
            "Epoch 00031: val_accuracy did not improve from 0.78460\n",
            "60/59 [==============================] - 51s 845ms/step - loss: 0.7070 - accuracy: 0.7486 - val_loss: 0.8319 - val_accuracy: 0.7679\n",
            "Epoch 32/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6672 - accuracy: 0.7610\n",
            "Epoch 00032: val_accuracy did not improve from 0.78460\n",
            "60/59 [==============================] - 50s 839ms/step - loss: 0.6672 - accuracy: 0.7610 - val_loss: 0.8262 - val_accuracy: 0.7746\n",
            "Epoch 33/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6611 - accuracy: 0.7599\n",
            "Epoch 00033: val_accuracy improved from 0.78460 to 0.78906, saving model to seedling2.h5\n",
            "60/59 [==============================] - 50s 836ms/step - loss: 0.6611 - accuracy: 0.7599 - val_loss: 0.8352 - val_accuracy: 0.7891\n",
            "Epoch 34/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6294 - accuracy: 0.7744\n",
            "Epoch 00034: val_accuracy did not improve from 0.78906\n",
            "60/59 [==============================] - 51s 845ms/step - loss: 0.6294 - accuracy: 0.7744 - val_loss: 0.7774 - val_accuracy: 0.7868\n",
            "Epoch 35/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6456 - accuracy: 0.7733\n",
            "Epoch 00035: val_accuracy improved from 0.78906 to 0.80580, saving model to seedling2.h5\n",
            "60/59 [==============================] - 50s 835ms/step - loss: 0.6456 - accuracy: 0.7733 - val_loss: 0.6873 - val_accuracy: 0.8058\n",
            "Epoch 36/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6153 - accuracy: 0.7744\n",
            "Epoch 00036: val_accuracy improved from 0.80580 to 0.81696, saving model to seedling2.h5\n",
            "60/59 [==============================] - 50s 839ms/step - loss: 0.6153 - accuracy: 0.7744 - val_loss: 0.8000 - val_accuracy: 0.8170\n",
            "Epoch 37/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6078 - accuracy: 0.7865\n",
            "Epoch 00037: val_accuracy improved from 0.81696 to 0.81808, saving model to seedling2.h5\n",
            "60/59 [==============================] - 50s 838ms/step - loss: 0.6078 - accuracy: 0.7865 - val_loss: 0.6663 - val_accuracy: 0.8181\n",
            "Epoch 38/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.6131 - accuracy: 0.7744\n",
            "Epoch 00038: val_accuracy did not improve from 0.81808\n",
            "60/59 [==============================] - 50s 828ms/step - loss: 0.6131 - accuracy: 0.7744 - val_loss: 0.6940 - val_accuracy: 0.8069\n",
            "Epoch 39/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5871 - accuracy: 0.7894\n",
            "Epoch 00039: val_accuracy improved from 0.81808 to 0.82254, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 843ms/step - loss: 0.5871 - accuracy: 0.7894 - val_loss: 0.5942 - val_accuracy: 0.8225\n",
            "Epoch 40/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5527 - accuracy: 0.7999\n",
            "Epoch 00040: val_accuracy improved from 0.82254 to 0.83036, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 856ms/step - loss: 0.5527 - accuracy: 0.7999 - val_loss: 0.6153 - val_accuracy: 0.8304\n",
            "Epoch 41/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5901 - accuracy: 0.7923\n",
            "Epoch 00041: val_accuracy did not improve from 0.83036\n",
            "60/59 [==============================] - 50s 840ms/step - loss: 0.5901 - accuracy: 0.7923 - val_loss: 0.7971 - val_accuracy: 0.7969\n",
            "Epoch 42/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5507 - accuracy: 0.7996\n",
            "Epoch 00042: val_accuracy did not improve from 0.83036\n",
            "60/59 [==============================] - 51s 845ms/step - loss: 0.5507 - accuracy: 0.7996 - val_loss: 0.8439 - val_accuracy: 0.7779\n",
            "Epoch 43/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5682 - accuracy: 0.7970\n",
            "Epoch 00043: val_accuracy did not improve from 0.83036\n",
            "60/59 [==============================] - 50s 832ms/step - loss: 0.5682 - accuracy: 0.7970 - val_loss: 0.6929 - val_accuracy: 0.7935\n",
            "Epoch 44/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5294 - accuracy: 0.8057\n",
            "Epoch 00044: val_accuracy did not improve from 0.83036\n",
            "60/59 [==============================] - 50s 840ms/step - loss: 0.5294 - accuracy: 0.8057 - val_loss: 0.6719 - val_accuracy: 0.8281\n",
            "Epoch 45/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5095 - accuracy: 0.8138\n",
            "Epoch 00045: val_accuracy improved from 0.83036 to 0.83147, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 847ms/step - loss: 0.5095 - accuracy: 0.8138 - val_loss: 0.5984 - val_accuracy: 0.8315\n",
            "Epoch 46/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.8136\n",
            "Epoch 00046: val_accuracy did not improve from 0.83147\n",
            "60/59 [==============================] - 51s 852ms/step - loss: 0.5226 - accuracy: 0.8136 - val_loss: 0.8020 - val_accuracy: 0.7801\n",
            "Epoch 47/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5297 - accuracy: 0.8091\n",
            "Epoch 00047: val_accuracy did not improve from 0.83147\n",
            "60/59 [==============================] - 50s 842ms/step - loss: 0.5297 - accuracy: 0.8091 - val_loss: 0.6214 - val_accuracy: 0.8192\n",
            "Epoch 48/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5477 - accuracy: 0.8054\n",
            "Epoch 00048: val_accuracy did not improve from 0.83147\n",
            "60/59 [==============================] - 50s 836ms/step - loss: 0.5477 - accuracy: 0.8054 - val_loss: 0.6730 - val_accuracy: 0.8248\n",
            "Epoch 49/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.8180\n",
            "Epoch 00049: val_accuracy improved from 0.83147 to 0.83371, saving model to seedling2.h5\n",
            "60/59 [==============================] - 50s 835ms/step - loss: 0.5098 - accuracy: 0.8180 - val_loss: 0.6194 - val_accuracy: 0.8337\n",
            "Epoch 50/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5130 - accuracy: 0.8175\n",
            "Epoch 00050: val_accuracy did not improve from 0.83371\n",
            "60/59 [==============================] - 50s 842ms/step - loss: 0.5130 - accuracy: 0.8175 - val_loss: 0.6391 - val_accuracy: 0.8237\n",
            "Epoch 51/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.5007 - accuracy: 0.8175\n",
            "Epoch 00051: val_accuracy improved from 0.83371 to 0.84040, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 851ms/step - loss: 0.5007 - accuracy: 0.8175 - val_loss: 0.6441 - val_accuracy: 0.8404\n",
            "Epoch 52/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4618 - accuracy: 0.8320\n",
            "Epoch 00052: val_accuracy did not improve from 0.84040\n",
            "60/59 [==============================] - 51s 857ms/step - loss: 0.4618 - accuracy: 0.8320 - val_loss: 0.6374 - val_accuracy: 0.8326\n",
            "Epoch 53/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.8364\n",
            "Epoch 00053: val_accuracy did not improve from 0.84040\n",
            "60/59 [==============================] - 51s 855ms/step - loss: 0.4612 - accuracy: 0.8364 - val_loss: 0.6724 - val_accuracy: 0.8214\n",
            "Epoch 54/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4913 - accuracy: 0.8238\n",
            "Epoch 00054: val_accuracy did not improve from 0.84040\n",
            "60/59 [==============================] - 51s 851ms/step - loss: 0.4913 - accuracy: 0.8238 - val_loss: 0.6017 - val_accuracy: 0.8304\n",
            "Epoch 55/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4784 - accuracy: 0.8254\n",
            "Epoch 00055: val_accuracy improved from 0.84040 to 0.84263, saving model to seedling2.h5\n",
            "60/59 [==============================] - 52s 861ms/step - loss: 0.4784 - accuracy: 0.8254 - val_loss: 0.5806 - val_accuracy: 0.8426\n",
            "Epoch 56/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4790 - accuracy: 0.8257\n",
            "Epoch 00056: val_accuracy did not improve from 0.84263\n",
            "60/59 [==============================] - 52s 863ms/step - loss: 0.4790 - accuracy: 0.8257 - val_loss: 0.6477 - val_accuracy: 0.8348\n",
            "Epoch 57/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4667 - accuracy: 0.8259\n",
            "Epoch 00057: val_accuracy did not improve from 0.84263\n",
            "60/59 [==============================] - 51s 855ms/step - loss: 0.4667 - accuracy: 0.8259 - val_loss: 0.6901 - val_accuracy: 0.8326\n",
            "Epoch 58/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4467 - accuracy: 0.8364\n",
            "Epoch 00058: val_accuracy improved from 0.84263 to 0.85379, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 850ms/step - loss: 0.4467 - accuracy: 0.8364 - val_loss: 0.6931 - val_accuracy: 0.8538\n",
            "Epoch 59/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4215 - accuracy: 0.8464\n",
            "Epoch 00059: val_accuracy did not improve from 0.85379\n",
            "60/59 [==============================] - 50s 825ms/step - loss: 0.4215 - accuracy: 0.8464 - val_loss: 0.6388 - val_accuracy: 0.8527\n",
            "Epoch 60/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4453 - accuracy: 0.8380\n",
            "Epoch 00060: val_accuracy did not improve from 0.85379\n",
            "60/59 [==============================] - 50s 831ms/step - loss: 0.4453 - accuracy: 0.8380 - val_loss: 0.7580 - val_accuracy: 0.8203\n",
            "Epoch 61/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4613 - accuracy: 0.8317\n",
            "Epoch 00061: val_accuracy did not improve from 0.85379\n",
            "60/59 [==============================] - 50s 828ms/step - loss: 0.4613 - accuracy: 0.8317 - val_loss: 0.6624 - val_accuracy: 0.8438\n",
            "Epoch 62/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4591 - accuracy: 0.8325\n",
            "Epoch 00062: val_accuracy improved from 0.85379 to 0.85826, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 844ms/step - loss: 0.4591 - accuracy: 0.8325 - val_loss: 0.6281 - val_accuracy: 0.8583\n",
            "Epoch 63/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4032 - accuracy: 0.8527\n",
            "Epoch 00063: val_accuracy did not improve from 0.85826\n",
            "60/59 [==============================] - 51s 843ms/step - loss: 0.4032 - accuracy: 0.8527 - val_loss: 0.6101 - val_accuracy: 0.8538\n",
            "Epoch 64/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.8378\n",
            "Epoch 00064: val_accuracy did not improve from 0.85826\n",
            "60/59 [==============================] - 51s 852ms/step - loss: 0.4447 - accuracy: 0.8378 - val_loss: 0.6418 - val_accuracy: 0.8382\n",
            "Epoch 65/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4305 - accuracy: 0.8491\n",
            "Epoch 00065: val_accuracy did not improve from 0.85826\n",
            "60/59 [==============================] - 50s 835ms/step - loss: 0.4305 - accuracy: 0.8491 - val_loss: 0.5927 - val_accuracy: 0.8482\n",
            "Epoch 66/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.8409\n",
            "Epoch 00066: val_accuracy did not improve from 0.85826\n",
            "60/59 [==============================] - 51s 856ms/step - loss: 0.4404 - accuracy: 0.8409 - val_loss: 0.7249 - val_accuracy: 0.8393\n",
            "Epoch 67/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8480\n",
            "Epoch 00067: val_accuracy did not improve from 0.85826\n",
            "60/59 [==============================] - 51s 845ms/step - loss: 0.4202 - accuracy: 0.8480 - val_loss: 0.5701 - val_accuracy: 0.8426\n",
            "Epoch 68/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3913 - accuracy: 0.8591\n",
            "Epoch 00068: val_accuracy improved from 0.85826 to 0.87612, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 850ms/step - loss: 0.3913 - accuracy: 0.8591 - val_loss: 0.5537 - val_accuracy: 0.8761\n",
            "Epoch 69/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4208 - accuracy: 0.8462\n",
            "Epoch 00069: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 51s 852ms/step - loss: 0.4208 - accuracy: 0.8462 - val_loss: 0.6435 - val_accuracy: 0.8560\n",
            "Epoch 70/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8506\n",
            "Epoch 00070: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 51s 847ms/step - loss: 0.4213 - accuracy: 0.8506 - val_loss: 0.5264 - val_accuracy: 0.8717\n",
            "Epoch 71/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4164 - accuracy: 0.8559\n",
            "Epoch 00071: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 833ms/step - loss: 0.4164 - accuracy: 0.8559 - val_loss: 0.4891 - val_accuracy: 0.8661\n",
            "Epoch 72/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.4310 - accuracy: 0.8509\n",
            "Epoch 00072: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 836ms/step - loss: 0.4310 - accuracy: 0.8509 - val_loss: 0.6449 - val_accuracy: 0.8359\n",
            "Epoch 73/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3956 - accuracy: 0.8572\n",
            "Epoch 00073: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 839ms/step - loss: 0.3956 - accuracy: 0.8572 - val_loss: 0.5329 - val_accuracy: 0.8638\n",
            "Epoch 74/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3938 - accuracy: 0.8588\n",
            "Epoch 00074: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 830ms/step - loss: 0.3938 - accuracy: 0.8588 - val_loss: 0.5685 - val_accuracy: 0.8728\n",
            "Epoch 75/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3907 - accuracy: 0.8601\n",
            "Epoch 00075: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 830ms/step - loss: 0.3907 - accuracy: 0.8601 - val_loss: 0.4987 - val_accuracy: 0.8638\n",
            "Epoch 76/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3663 - accuracy: 0.8714\n",
            "Epoch 00076: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 834ms/step - loss: 0.3663 - accuracy: 0.8714 - val_loss: 0.5758 - val_accuracy: 0.8560\n",
            "Epoch 77/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3691 - accuracy: 0.8651\n",
            "Epoch 00077: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 835ms/step - loss: 0.3691 - accuracy: 0.8651 - val_loss: 0.6272 - val_accuracy: 0.8549\n",
            "Epoch 78/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3698 - accuracy: 0.8635\n",
            "Epoch 00078: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 49s 825ms/step - loss: 0.3698 - accuracy: 0.8635 - val_loss: 0.6520 - val_accuracy: 0.8650\n",
            "Epoch 79/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3877 - accuracy: 0.8638\n",
            "Epoch 00079: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 826ms/step - loss: 0.3877 - accuracy: 0.8638 - val_loss: 0.6787 - val_accuracy: 0.8705\n",
            "Epoch 80/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3669 - accuracy: 0.8727\n",
            "Epoch 00080: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 49s 821ms/step - loss: 0.3669 - accuracy: 0.8727 - val_loss: 0.6732 - val_accuracy: 0.8605\n",
            "Epoch 81/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3860 - accuracy: 0.8606\n",
            "Epoch 00081: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 49s 814ms/step - loss: 0.3860 - accuracy: 0.8606 - val_loss: 0.6359 - val_accuracy: 0.8571\n",
            "Epoch 82/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.8706\n",
            "Epoch 00082: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 51s 857ms/step - loss: 0.3654 - accuracy: 0.8706 - val_loss: 0.6087 - val_accuracy: 0.8694\n",
            "Epoch 83/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3721 - accuracy: 0.8651\n",
            "Epoch 00083: val_accuracy did not improve from 0.87612\n",
            "60/59 [==============================] - 50s 834ms/step - loss: 0.3721 - accuracy: 0.8651 - val_loss: 0.5459 - val_accuracy: 0.8728\n",
            "Epoch 84/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3989 - accuracy: 0.8570\n",
            "Epoch 00084: val_accuracy improved from 0.87612 to 0.88170, saving model to seedling2.h5\n",
            "60/59 [==============================] - 50s 833ms/step - loss: 0.3989 - accuracy: 0.8570 - val_loss: 0.5372 - val_accuracy: 0.8817\n",
            "Epoch 85/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3549 - accuracy: 0.8746\n",
            "Epoch 00085: val_accuracy did not improve from 0.88170\n",
            "60/59 [==============================] - 51s 844ms/step - loss: 0.3549 - accuracy: 0.8746 - val_loss: 0.6239 - val_accuracy: 0.8739\n",
            "Epoch 86/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8643\n",
            "Epoch 00086: val_accuracy did not improve from 0.88170\n",
            "60/59 [==============================] - 51s 847ms/step - loss: 0.3775 - accuracy: 0.8643 - val_loss: 0.6246 - val_accuracy: 0.8683\n",
            "Epoch 87/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3724 - accuracy: 0.8696\n",
            "Epoch 00087: val_accuracy did not improve from 0.88170\n",
            "60/59 [==============================] - 52s 861ms/step - loss: 0.3724 - accuracy: 0.8696 - val_loss: 0.5224 - val_accuracy: 0.8638\n",
            "Epoch 88/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3345 - accuracy: 0.8798\n",
            "Epoch 00088: val_accuracy did not improve from 0.88170\n",
            "60/59 [==============================] - 52s 863ms/step - loss: 0.3345 - accuracy: 0.8798 - val_loss: 0.6613 - val_accuracy: 0.8594\n",
            "Epoch 89/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8704\n",
            "Epoch 00089: val_accuracy improved from 0.88170 to 0.88504, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 843ms/step - loss: 0.3639 - accuracy: 0.8704 - val_loss: 0.6450 - val_accuracy: 0.8850\n",
            "Epoch 90/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3434 - accuracy: 0.8769\n",
            "Epoch 00090: val_accuracy did not improve from 0.88504\n",
            "60/59 [==============================] - 51s 846ms/step - loss: 0.3434 - accuracy: 0.8769 - val_loss: 0.6315 - val_accuracy: 0.8359\n",
            "Epoch 91/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3583 - accuracy: 0.8762\n",
            "Epoch 00091: val_accuracy improved from 0.88504 to 0.88728, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 849ms/step - loss: 0.3583 - accuracy: 0.8762 - val_loss: 0.4954 - val_accuracy: 0.8873\n",
            "Epoch 92/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3237 - accuracy: 0.8809\n",
            "Epoch 00092: val_accuracy did not improve from 0.88728\n",
            "60/59 [==============================] - 51s 844ms/step - loss: 0.3237 - accuracy: 0.8809 - val_loss: 0.5409 - val_accuracy: 0.8683\n",
            "Epoch 93/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3751 - accuracy: 0.8698\n",
            "Epoch 00093: val_accuracy did not improve from 0.88728\n",
            "60/59 [==============================] - 51s 849ms/step - loss: 0.3751 - accuracy: 0.8698 - val_loss: 0.5503 - val_accuracy: 0.8750\n",
            "Epoch 94/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3703 - accuracy: 0.8696\n",
            "Epoch 00094: val_accuracy did not improve from 0.88728\n",
            "60/59 [==============================] - 51s 848ms/step - loss: 0.3703 - accuracy: 0.8696 - val_loss: 0.5350 - val_accuracy: 0.8683\n",
            "Epoch 95/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8727\n",
            "Epoch 00095: val_accuracy did not improve from 0.88728\n",
            "60/59 [==============================] - 51s 854ms/step - loss: 0.3602 - accuracy: 0.8727 - val_loss: 0.5075 - val_accuracy: 0.8862\n",
            "Epoch 96/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.8825\n",
            "Epoch 00096: val_accuracy did not improve from 0.88728\n",
            "60/59 [==============================] - 51s 855ms/step - loss: 0.3378 - accuracy: 0.8825 - val_loss: 0.6101 - val_accuracy: 0.8750\n",
            "Epoch 97/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3482 - accuracy: 0.8785\n",
            "Epoch 00097: val_accuracy did not improve from 0.88728\n",
            "60/59 [==============================] - 51s 857ms/step - loss: 0.3482 - accuracy: 0.8785 - val_loss: 0.5567 - val_accuracy: 0.8594\n",
            "Epoch 98/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3329 - accuracy: 0.8814\n",
            "Epoch 00098: val_accuracy improved from 0.88728 to 0.88951, saving model to seedling2.h5\n",
            "60/59 [==============================] - 51s 845ms/step - loss: 0.3329 - accuracy: 0.8814 - val_loss: 0.4883 - val_accuracy: 0.8895\n",
            "Epoch 99/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.8798\n",
            "Epoch 00099: val_accuracy did not improve from 0.88951\n",
            "60/59 [==============================] - 51s 852ms/step - loss: 0.3370 - accuracy: 0.8798 - val_loss: 0.6475 - val_accuracy: 0.8359\n",
            "Epoch 100/100\n",
            "60/59 [==============================] - ETA: 0s - loss: 0.3448 - accuracy: 0.8767\n",
            "Epoch 00100: val_accuracy did not improve from 0.88951\n",
            "60/59 [==============================] - 52s 859ms/step - loss: 0.3448 - accuracy: 0.8767 - val_loss: 0.5018 - val_accuracy: 0.8795\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f05e269d550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3GgrTkFO3Yp",
        "colab_type": "text"
      },
      "source": [
        "Few observations from the model above.\n",
        "1. The test and train accuracies are quite close now which is good suggesting model is not  overfitting\n",
        "2. The testing accuracy is still a bit higher than train accuracy and that might be because of the fact that the train data is skewed due to augmentation and the model might be having difficulties in memorising."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bInYQ5EEQ5Xf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4d68de8-693e-4564-a105-86b349d0c826"
      },
      "source": [
        "model.fit(train_generator,\n",
        "          epochs=300,\n",
        "          initial_epoch=100,\n",
        "          steps_per_epoch= 3803//64,  #Number of training images//batch_size\n",
        "          validation_data=test_generator,\n",
        "          validation_steps = 947//64, #Number of test images//batch_size\n",
        "          callbacks = [model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 101/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2560 - accuracy: 0.9047\n",
            "Epoch 00101: val_accuracy did not improve from 0.90067\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.2560 - accuracy: 0.9047 - val_loss: 0.5663 - val_accuracy: 0.8783\n",
            "Epoch 102/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2627 - accuracy: 0.9018\n",
            "Epoch 00102: val_accuracy did not improve from 0.90067\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.2627 - accuracy: 0.9018 - val_loss: 0.6521 - val_accuracy: 0.8806\n",
            "Epoch 103/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2866 - accuracy: 0.8976\n",
            "Epoch 00103: val_accuracy did not improve from 0.90067\n",
            "59/59 [==============================] - 59s 997ms/step - loss: 0.2866 - accuracy: 0.8976 - val_loss: 0.5310 - val_accuracy: 0.8873\n",
            "Epoch 104/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2569 - accuracy: 0.9061\n",
            "Epoch 00104: val_accuracy did not improve from 0.90067\n",
            "59/59 [==============================] - 59s 997ms/step - loss: 0.2569 - accuracy: 0.9061 - val_loss: 0.6154 - val_accuracy: 0.8761\n",
            "Epoch 105/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2842 - accuracy: 0.9002\n",
            "Epoch 00105: val_accuracy did not improve from 0.90067\n",
            "59/59 [==============================] - 58s 988ms/step - loss: 0.2842 - accuracy: 0.9002 - val_loss: 0.5758 - val_accuracy: 0.8884\n",
            "Epoch 106/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2555 - accuracy: 0.9064\n",
            "Epoch 00106: val_accuracy did not improve from 0.90067\n",
            "59/59 [==============================] - 58s 977ms/step - loss: 0.2555 - accuracy: 0.9064 - val_loss: 0.5878 - val_accuracy: 0.8817\n",
            "Epoch 107/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2894 - accuracy: 0.8986\n",
            "Epoch 00107: val_accuracy improved from 0.90067 to 0.90290, saving model to seedling2.h5\n",
            "59/59 [==============================] - 58s 977ms/step - loss: 0.2894 - accuracy: 0.8986 - val_loss: 0.5048 - val_accuracy: 0.9029\n",
            "Epoch 108/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2622 - accuracy: 0.9040\n",
            "Epoch 00108: val_accuracy did not improve from 0.90290\n",
            "59/59 [==============================] - 57s 961ms/step - loss: 0.2622 - accuracy: 0.9040 - val_loss: 0.6999 - val_accuracy: 0.8906\n",
            "Epoch 109/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2941 - accuracy: 0.9024\n",
            "Epoch 00109: val_accuracy did not improve from 0.90290\n",
            "59/59 [==============================] - 57s 966ms/step - loss: 0.2941 - accuracy: 0.9024 - val_loss: 0.4913 - val_accuracy: 0.9029\n",
            "Epoch 110/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2655 - accuracy: 0.9032\n",
            "Epoch 00110: val_accuracy did not improve from 0.90290\n",
            "59/59 [==============================] - 56s 950ms/step - loss: 0.2655 - accuracy: 0.9032 - val_loss: 0.6573 - val_accuracy: 0.8717\n",
            "Epoch 111/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2485 - accuracy: 0.9115\n",
            "Epoch 00111: val_accuracy did not improve from 0.90290\n",
            "59/59 [==============================] - 56s 946ms/step - loss: 0.2485 - accuracy: 0.9115 - val_loss: 0.5550 - val_accuracy: 0.8962\n",
            "Epoch 112/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9139\n",
            "Epoch 00112: val_accuracy did not improve from 0.90290\n",
            "59/59 [==============================] - 56s 952ms/step - loss: 0.2455 - accuracy: 0.9139 - val_loss: 0.5327 - val_accuracy: 0.8962\n",
            "Epoch 113/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2626 - accuracy: 0.9026\n",
            "Epoch 00113: val_accuracy did not improve from 0.90290\n",
            "59/59 [==============================] - 56s 942ms/step - loss: 0.2626 - accuracy: 0.9026 - val_loss: 0.5332 - val_accuracy: 0.8996\n",
            "Epoch 114/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2327 - accuracy: 0.9254\n",
            "Epoch 00114: val_accuracy did not improve from 0.90290\n",
            "59/59 [==============================] - 56s 948ms/step - loss: 0.2327 - accuracy: 0.9254 - val_loss: 0.5405 - val_accuracy: 0.8839\n",
            "Epoch 115/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2684 - accuracy: 0.9075\n",
            "Epoch 00115: val_accuracy improved from 0.90290 to 0.90513, saving model to seedling2.h5\n",
            "59/59 [==============================] - 56s 953ms/step - loss: 0.2684 - accuracy: 0.9075 - val_loss: 0.5657 - val_accuracy: 0.9051\n",
            "Epoch 116/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9072\n",
            "Epoch 00116: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 56s 947ms/step - loss: 0.2618 - accuracy: 0.9072 - val_loss: 0.6138 - val_accuracy: 0.8750\n",
            "Epoch 117/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2570 - accuracy: 0.9075\n",
            "Epoch 00117: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 56s 947ms/step - loss: 0.2570 - accuracy: 0.9075 - val_loss: 0.6635 - val_accuracy: 0.8772\n",
            "Epoch 118/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2491 - accuracy: 0.9136\n",
            "Epoch 00118: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 56s 945ms/step - loss: 0.2491 - accuracy: 0.9136 - val_loss: 0.5706 - val_accuracy: 0.8694\n",
            "Epoch 119/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2683 - accuracy: 0.9026\n",
            "Epoch 00119: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 56s 954ms/step - loss: 0.2683 - accuracy: 0.9026 - val_loss: 0.6323 - val_accuracy: 0.8772\n",
            "Epoch 120/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2779 - accuracy: 0.9045\n",
            "Epoch 00120: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 55s 937ms/step - loss: 0.2779 - accuracy: 0.9045 - val_loss: 0.6544 - val_accuracy: 0.8783\n",
            "Epoch 121/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2620 - accuracy: 0.9099\n",
            "Epoch 00121: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 55s 926ms/step - loss: 0.2620 - accuracy: 0.9099 - val_loss: 0.4727 - val_accuracy: 0.8862\n",
            "Epoch 122/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2455 - accuracy: 0.9152\n",
            "Epoch 00122: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 54s 922ms/step - loss: 0.2455 - accuracy: 0.9152 - val_loss: 0.5394 - val_accuracy: 0.8917\n",
            "Epoch 123/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2576 - accuracy: 0.9083\n",
            "Epoch 00123: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 54s 922ms/step - loss: 0.2576 - accuracy: 0.9083 - val_loss: 0.6957 - val_accuracy: 0.8717\n",
            "Epoch 124/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2364 - accuracy: 0.9200\n",
            "Epoch 00124: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 55s 925ms/step - loss: 0.2364 - accuracy: 0.9200 - val_loss: 0.5666 - val_accuracy: 0.8917\n",
            "Epoch 125/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2524 - accuracy: 0.9109\n",
            "Epoch 00125: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 55s 926ms/step - loss: 0.2524 - accuracy: 0.9109 - val_loss: 0.6022 - val_accuracy: 0.8962\n",
            "Epoch 126/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9174\n",
            "Epoch 00126: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 55s 928ms/step - loss: 0.2426 - accuracy: 0.9174 - val_loss: 0.5750 - val_accuracy: 0.9029\n",
            "Epoch 127/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2618 - accuracy: 0.9067\n",
            "Epoch 00127: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 55s 924ms/step - loss: 0.2618 - accuracy: 0.9067 - val_loss: 0.7544 - val_accuracy: 0.8493\n",
            "Epoch 128/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2323 - accuracy: 0.9150\n",
            "Epoch 00128: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 54s 910ms/step - loss: 0.2323 - accuracy: 0.9150 - val_loss: 0.5538 - val_accuracy: 0.8929\n",
            "Epoch 129/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2630 - accuracy: 0.9163\n",
            "Epoch 00129: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 54s 922ms/step - loss: 0.2630 - accuracy: 0.9163 - val_loss: 0.7503 - val_accuracy: 0.8917\n",
            "Epoch 130/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2255 - accuracy: 0.9187\n",
            "Epoch 00130: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 52s 889ms/step - loss: 0.2255 - accuracy: 0.9187 - val_loss: 0.6211 - val_accuracy: 0.8862\n",
            "Epoch 131/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2466 - accuracy: 0.9168\n",
            "Epoch 00131: val_accuracy did not improve from 0.90513\n",
            "59/59 [==============================] - 52s 877ms/step - loss: 0.2466 - accuracy: 0.9168 - val_loss: 0.5840 - val_accuracy: 0.8817\n",
            "Epoch 132/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2488 - accuracy: 0.9216\n",
            "Epoch 00132: val_accuracy improved from 0.90513 to 0.90625, saving model to seedling2.h5\n",
            "59/59 [==============================] - 52s 885ms/step - loss: 0.2488 - accuracy: 0.9216 - val_loss: 0.4893 - val_accuracy: 0.9062\n",
            "Epoch 133/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2617 - accuracy: 0.9107\n",
            "Epoch 00133: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 52s 879ms/step - loss: 0.2617 - accuracy: 0.9107 - val_loss: 0.6685 - val_accuracy: 0.8962\n",
            "Epoch 134/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2451 - accuracy: 0.9096\n",
            "Epoch 00134: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 51s 870ms/step - loss: 0.2451 - accuracy: 0.9096 - val_loss: 0.5376 - val_accuracy: 0.9029\n",
            "Epoch 135/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2168 - accuracy: 0.9248\n",
            "Epoch 00135: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 50s 850ms/step - loss: 0.2168 - accuracy: 0.9248 - val_loss: 0.6870 - val_accuracy: 0.8906\n",
            "Epoch 136/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2639 - accuracy: 0.9139\n",
            "Epoch 00136: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.2639 - accuracy: 0.9139 - val_loss: 0.7274 - val_accuracy: 0.8750\n",
            "Epoch 137/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9158\n",
            "Epoch 00137: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 50s 842ms/step - loss: 0.2384 - accuracy: 0.9158 - val_loss: 0.6207 - val_accuracy: 0.8917\n",
            "Epoch 138/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2311 - accuracy: 0.9224\n",
            "Epoch 00138: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.2311 - accuracy: 0.9224 - val_loss: 0.6589 - val_accuracy: 0.8638\n",
            "Epoch 139/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2426 - accuracy: 0.9147\n",
            "Epoch 00139: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 51s 859ms/step - loss: 0.2426 - accuracy: 0.9147 - val_loss: 0.6168 - val_accuracy: 0.8828\n",
            "Epoch 140/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2448 - accuracy: 0.9117\n",
            "Epoch 00140: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 50s 846ms/step - loss: 0.2448 - accuracy: 0.9117 - val_loss: 0.5975 - val_accuracy: 0.9051\n",
            "Epoch 141/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2387 - accuracy: 0.9174\n",
            "Epoch 00141: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.2387 - accuracy: 0.9174 - val_loss: 0.5368 - val_accuracy: 0.8973\n",
            "Epoch 142/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9174\n",
            "Epoch 00142: val_accuracy did not improve from 0.90625\n",
            "59/59 [==============================] - 50s 843ms/step - loss: 0.2347 - accuracy: 0.9174 - val_loss: 0.6432 - val_accuracy: 0.8850\n",
            "Epoch 143/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2186 - accuracy: 0.9240\n",
            "Epoch 00143: val_accuracy improved from 0.90625 to 0.92188, saving model to seedling2.h5\n",
            "59/59 [==============================] - 49s 834ms/step - loss: 0.2186 - accuracy: 0.9240 - val_loss: 0.4913 - val_accuracy: 0.9219\n",
            "Epoch 144/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2206 - accuracy: 0.9208\n",
            "Epoch 00144: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 49s 837ms/step - loss: 0.2206 - accuracy: 0.9208 - val_loss: 0.6993 - val_accuracy: 0.8839\n",
            "Epoch 145/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2218 - accuracy: 0.9219\n",
            "Epoch 00145: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 51s 864ms/step - loss: 0.2218 - accuracy: 0.9219 - val_loss: 0.5686 - val_accuracy: 0.9018\n",
            "Epoch 146/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2023 - accuracy: 0.9278\n",
            "Epoch 00146: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 50s 853ms/step - loss: 0.2023 - accuracy: 0.9278 - val_loss: 0.5265 - val_accuracy: 0.9029\n",
            "Epoch 147/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2430 - accuracy: 0.9182\n",
            "Epoch 00147: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 50s 854ms/step - loss: 0.2430 - accuracy: 0.9182 - val_loss: 0.5888 - val_accuracy: 0.8929\n",
            "Epoch 148/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9230\n",
            "Epoch 00148: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 51s 866ms/step - loss: 0.2196 - accuracy: 0.9230 - val_loss: 0.5727 - val_accuracy: 0.8862\n",
            "Epoch 149/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2613 - accuracy: 0.9117\n",
            "Epoch 00149: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 50s 845ms/step - loss: 0.2613 - accuracy: 0.9117 - val_loss: 0.6097 - val_accuracy: 0.8828\n",
            "Epoch 150/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2273 - accuracy: 0.9206\n",
            "Epoch 00150: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.2273 - accuracy: 0.9206 - val_loss: 0.6672 - val_accuracy: 0.9007\n",
            "Epoch 151/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 0.9237\n",
            "Epoch 00151: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.2212 - accuracy: 0.9237 - val_loss: 0.5791 - val_accuracy: 0.9074\n",
            "Epoch 152/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9297\n",
            "Epoch 00152: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 50s 843ms/step - loss: 0.2183 - accuracy: 0.9297 - val_loss: 0.5117 - val_accuracy: 0.9007\n",
            "Epoch 153/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9227\n",
            "Epoch 00153: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 49s 836ms/step - loss: 0.2053 - accuracy: 0.9227 - val_loss: 0.6122 - val_accuracy: 0.9085\n",
            "Epoch 154/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2081 - accuracy: 0.9262\n",
            "Epoch 00154: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 50s 854ms/step - loss: 0.2081 - accuracy: 0.9262 - val_loss: 0.5825 - val_accuracy: 0.9040\n",
            "Epoch 155/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2091 - accuracy: 0.9283\n",
            "Epoch 00155: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 962ms/step - loss: 0.2091 - accuracy: 0.9283 - val_loss: 0.6343 - val_accuracy: 0.8895\n",
            "Epoch 156/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2343 - accuracy: 0.9184\n",
            "Epoch 00156: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 984ms/step - loss: 0.2343 - accuracy: 0.9184 - val_loss: 0.6810 - val_accuracy: 0.8895\n",
            "Epoch 157/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2019 - accuracy: 0.9299\n",
            "Epoch 00157: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 970ms/step - loss: 0.2019 - accuracy: 0.9299 - val_loss: 0.6580 - val_accuracy: 0.8705\n",
            "Epoch 158/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9235\n",
            "Epoch 00158: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 971ms/step - loss: 0.2222 - accuracy: 0.9235 - val_loss: 0.6657 - val_accuracy: 0.8917\n",
            "Epoch 159/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2231 - accuracy: 0.9200\n",
            "Epoch 00159: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 972ms/step - loss: 0.2231 - accuracy: 0.9200 - val_loss: 0.5967 - val_accuracy: 0.9007\n",
            "Epoch 160/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.9283\n",
            "Epoch 00160: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 979ms/step - loss: 0.1954 - accuracy: 0.9283 - val_loss: 0.5685 - val_accuracy: 0.8940\n",
            "Epoch 161/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2089 - accuracy: 0.9224\n",
            "Epoch 00161: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 985ms/step - loss: 0.2089 - accuracy: 0.9224 - val_loss: 0.4847 - val_accuracy: 0.8917\n",
            "Epoch 162/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9297\n",
            "Epoch 00162: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 975ms/step - loss: 0.2020 - accuracy: 0.9297 - val_loss: 0.5174 - val_accuracy: 0.9129\n",
            "Epoch 163/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1977 - accuracy: 0.9310\n",
            "Epoch 00163: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 979ms/step - loss: 0.1977 - accuracy: 0.9310 - val_loss: 0.6682 - val_accuracy: 0.9141\n",
            "Epoch 164/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2145 - accuracy: 0.9216\n",
            "Epoch 00164: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 976ms/step - loss: 0.2145 - accuracy: 0.9216 - val_loss: 0.6334 - val_accuracy: 0.8951\n",
            "Epoch 165/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2001 - accuracy: 0.9299\n",
            "Epoch 00165: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 983ms/step - loss: 0.2001 - accuracy: 0.9299 - val_loss: 0.6186 - val_accuracy: 0.9007\n",
            "Epoch 166/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.9297\n",
            "Epoch 00166: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 965ms/step - loss: 0.2045 - accuracy: 0.9297 - val_loss: 0.7147 - val_accuracy: 0.8750\n",
            "Epoch 167/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2174 - accuracy: 0.9286\n",
            "Epoch 00167: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 982ms/step - loss: 0.2174 - accuracy: 0.9286 - val_loss: 0.6514 - val_accuracy: 0.9085\n",
            "Epoch 168/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2062 - accuracy: 0.9259\n",
            "Epoch 00168: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 979ms/step - loss: 0.2062 - accuracy: 0.9259 - val_loss: 0.6193 - val_accuracy: 0.9007\n",
            "Epoch 169/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9339\n",
            "Epoch 00169: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 978ms/step - loss: 0.1811 - accuracy: 0.9339 - val_loss: 0.5752 - val_accuracy: 0.9129\n",
            "Epoch 170/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1933 - accuracy: 0.9329\n",
            "Epoch 00170: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 971ms/step - loss: 0.1933 - accuracy: 0.9329 - val_loss: 0.7954 - val_accuracy: 0.8973\n",
            "Epoch 171/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2057 - accuracy: 0.9291\n",
            "Epoch 00171: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 973ms/step - loss: 0.2057 - accuracy: 0.9291 - val_loss: 0.6244 - val_accuracy: 0.9029\n",
            "Epoch 172/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9321\n",
            "Epoch 00172: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 975ms/step - loss: 0.1916 - accuracy: 0.9321 - val_loss: 0.6325 - val_accuracy: 0.9219\n",
            "Epoch 173/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9262\n",
            "Epoch 00173: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 965ms/step - loss: 0.2064 - accuracy: 0.9262 - val_loss: 0.5464 - val_accuracy: 0.9074\n",
            "Epoch 174/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2202 - accuracy: 0.9251\n",
            "Epoch 00174: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 978ms/step - loss: 0.2202 - accuracy: 0.9251 - val_loss: 0.7174 - val_accuracy: 0.8984\n",
            "Epoch 175/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.9337\n",
            "Epoch 00175: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 977ms/step - loss: 0.1835 - accuracy: 0.9337 - val_loss: 0.5706 - val_accuracy: 0.9074\n",
            "Epoch 176/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2050 - accuracy: 0.9355\n",
            "Epoch 00176: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 975ms/step - loss: 0.2050 - accuracy: 0.9355 - val_loss: 0.6190 - val_accuracy: 0.9007\n",
            "Epoch 177/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9275\n",
            "Epoch 00177: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 980ms/step - loss: 0.2024 - accuracy: 0.9275 - val_loss: 0.5657 - val_accuracy: 0.9141\n",
            "Epoch 178/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2031 - accuracy: 0.9270\n",
            "Epoch 00178: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 56s 955ms/step - loss: 0.2031 - accuracy: 0.9270 - val_loss: 0.6511 - val_accuracy: 0.8984\n",
            "Epoch 179/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1725 - accuracy: 0.9377\n",
            "Epoch 00179: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 964ms/step - loss: 0.1725 - accuracy: 0.9377 - val_loss: 0.5150 - val_accuracy: 0.9152\n",
            "Epoch 180/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2083 - accuracy: 0.9310\n",
            "Epoch 00180: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 969ms/step - loss: 0.2083 - accuracy: 0.9310 - val_loss: 0.6683 - val_accuracy: 0.8895\n",
            "Epoch 181/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2017 - accuracy: 0.9302\n",
            "Epoch 00181: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 981ms/step - loss: 0.2017 - accuracy: 0.9302 - val_loss: 0.6697 - val_accuracy: 0.8917\n",
            "Epoch 182/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1949 - accuracy: 0.9329\n",
            "Epoch 00182: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 970ms/step - loss: 0.1949 - accuracy: 0.9329 - val_loss: 0.6072 - val_accuracy: 0.9051\n",
            "Epoch 183/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1958 - accuracy: 0.9331\n",
            "Epoch 00183: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 966ms/step - loss: 0.1958 - accuracy: 0.9331 - val_loss: 0.5568 - val_accuracy: 0.9040\n",
            "Epoch 184/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1924 - accuracy: 0.9331\n",
            "Epoch 00184: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 56s 942ms/step - loss: 0.1924 - accuracy: 0.9331 - val_loss: 0.6217 - val_accuracy: 0.8940\n",
            "Epoch 185/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9345\n",
            "Epoch 00185: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 54s 921ms/step - loss: 0.1936 - accuracy: 0.9345 - val_loss: 0.5815 - val_accuracy: 0.9085\n",
            "Epoch 186/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1880 - accuracy: 0.9329\n",
            "Epoch 00186: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 54s 921ms/step - loss: 0.1880 - accuracy: 0.9329 - val_loss: 0.6745 - val_accuracy: 0.8594\n",
            "Epoch 187/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2038 - accuracy: 0.9329\n",
            "Epoch 00187: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 54s 916ms/step - loss: 0.2038 - accuracy: 0.9329 - val_loss: 0.6756 - val_accuracy: 0.8817\n",
            "Epoch 188/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2080 - accuracy: 0.9323\n",
            "Epoch 00188: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 900ms/step - loss: 0.2080 - accuracy: 0.9323 - val_loss: 0.6078 - val_accuracy: 0.8951\n",
            "Epoch 189/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9289\n",
            "Epoch 00189: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 898ms/step - loss: 0.2065 - accuracy: 0.9289 - val_loss: 0.5280 - val_accuracy: 0.9074\n",
            "Epoch 190/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1955 - accuracy: 0.9353\n",
            "Epoch 00190: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 54s 908ms/step - loss: 0.1955 - accuracy: 0.9353 - val_loss: 0.7455 - val_accuracy: 0.8828\n",
            "Epoch 191/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1994 - accuracy: 0.9278\n",
            "Epoch 00191: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 905ms/step - loss: 0.1994 - accuracy: 0.9278 - val_loss: 0.6911 - val_accuracy: 0.8996\n",
            "Epoch 192/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2032 - accuracy: 0.9339\n",
            "Epoch 00192: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 897ms/step - loss: 0.2032 - accuracy: 0.9339 - val_loss: 0.6559 - val_accuracy: 0.8917\n",
            "Epoch 193/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1969 - accuracy: 0.9329\n",
            "Epoch 00193: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 897ms/step - loss: 0.1969 - accuracy: 0.9329 - val_loss: 0.4425 - val_accuracy: 0.9040\n",
            "Epoch 194/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1881 - accuracy: 0.9369\n",
            "Epoch 00194: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 904ms/step - loss: 0.1881 - accuracy: 0.9369 - val_loss: 0.7697 - val_accuracy: 0.8817\n",
            "Epoch 195/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9347\n",
            "Epoch 00195: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 897ms/step - loss: 0.1815 - accuracy: 0.9347 - val_loss: 0.5301 - val_accuracy: 0.9152\n",
            "Epoch 196/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2001 - accuracy: 0.9313\n",
            "Epoch 00196: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 904ms/step - loss: 0.2001 - accuracy: 0.9313 - val_loss: 0.5736 - val_accuracy: 0.9051\n",
            "Epoch 197/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9345\n",
            "Epoch 00197: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 53s 897ms/step - loss: 0.1918 - accuracy: 0.9345 - val_loss: 0.6398 - val_accuracy: 0.8761\n",
            "Epoch 198/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9307\n",
            "Epoch 00198: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 969ms/step - loss: 0.1875 - accuracy: 0.9307 - val_loss: 0.5752 - val_accuracy: 0.8996\n",
            "Epoch 199/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2109 - accuracy: 0.9216\n",
            "Epoch 00199: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 971ms/step - loss: 0.2109 - accuracy: 0.9216 - val_loss: 0.6590 - val_accuracy: 0.9029\n",
            "Epoch 200/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1986 - accuracy: 0.9334\n",
            "Epoch 00200: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 971ms/step - loss: 0.1986 - accuracy: 0.9334 - val_loss: 0.6741 - val_accuracy: 0.9007\n",
            "Epoch 201/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9355\n",
            "Epoch 00201: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 57s 969ms/step - loss: 0.1859 - accuracy: 0.9355 - val_loss: 0.7569 - val_accuracy: 0.8739\n",
            "Epoch 202/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1942 - accuracy: 0.9339\n",
            "Epoch 00202: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 985ms/step - loss: 0.1942 - accuracy: 0.9339 - val_loss: 0.7088 - val_accuracy: 0.9074\n",
            "Epoch 203/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1891 - accuracy: 0.9366\n",
            "Epoch 00203: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 58s 982ms/step - loss: 0.1891 - accuracy: 0.9366 - val_loss: 0.7013 - val_accuracy: 0.8761\n",
            "Epoch 204/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1980 - accuracy: 0.9307\n",
            "Epoch 00204: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1980 - accuracy: 0.9307 - val_loss: 0.7254 - val_accuracy: 0.8929\n",
            "Epoch 205/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2036 - accuracy: 0.9314\n",
            "Epoch 00205: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.2036 - accuracy: 0.9314 - val_loss: 0.5834 - val_accuracy: 0.9129\n",
            "Epoch 206/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1835 - accuracy: 0.9398\n",
            "Epoch 00206: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1835 - accuracy: 0.9398 - val_loss: 0.4746 - val_accuracy: 0.9152\n",
            "Epoch 207/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1840 - accuracy: 0.9363\n",
            "Epoch 00207: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1840 - accuracy: 0.9363 - val_loss: 0.6666 - val_accuracy: 0.8717\n",
            "Epoch 208/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9331\n",
            "Epoch 00208: val_accuracy did not improve from 0.92188\n",
            "59/59 [==============================] - 59s 999ms/step - loss: 0.1911 - accuracy: 0.9331 - val_loss: 0.7182 - val_accuracy: 0.8951\n",
            "Epoch 209/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1952 - accuracy: 0.9342\n",
            "Epoch 00209: val_accuracy improved from 0.92188 to 0.92299, saving model to seedling2.h5\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1952 - accuracy: 0.9342 - val_loss: 0.5098 - val_accuracy: 0.9230\n",
            "Epoch 210/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9318\n",
            "Epoch 00210: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1797 - accuracy: 0.9318 - val_loss: 0.6178 - val_accuracy: 0.8962\n",
            "Epoch 211/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9353\n",
            "Epoch 00211: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 58s 981ms/step - loss: 0.1904 - accuracy: 0.9353 - val_loss: 0.6403 - val_accuracy: 0.8951\n",
            "Epoch 212/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9326\n",
            "Epoch 00212: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 970ms/step - loss: 0.1790 - accuracy: 0.9326 - val_loss: 0.5643 - val_accuracy: 0.8873\n",
            "Epoch 213/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1831 - accuracy: 0.9326\n",
            "Epoch 00213: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 963ms/step - loss: 0.1831 - accuracy: 0.9326 - val_loss: 0.6594 - val_accuracy: 0.9107\n",
            "Epoch 214/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9358\n",
            "Epoch 00214: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 961ms/step - loss: 0.1875 - accuracy: 0.9358 - val_loss: 0.5628 - val_accuracy: 0.9141\n",
            "Epoch 215/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1831 - accuracy: 0.9329\n",
            "Epoch 00215: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 964ms/step - loss: 0.1831 - accuracy: 0.9329 - val_loss: 0.6391 - val_accuracy: 0.9062\n",
            "Epoch 216/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1566 - accuracy: 0.9404\n",
            "Epoch 00216: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 56s 951ms/step - loss: 0.1566 - accuracy: 0.9404 - val_loss: 0.6318 - val_accuracy: 0.8839\n",
            "Epoch 217/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1906 - accuracy: 0.9355\n",
            "Epoch 00217: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 56s 954ms/step - loss: 0.1906 - accuracy: 0.9355 - val_loss: 0.5590 - val_accuracy: 0.8973\n",
            "Epoch 218/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1646 - accuracy: 0.9425\n",
            "Epoch 00218: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 964ms/step - loss: 0.1646 - accuracy: 0.9425 - val_loss: 0.5971 - val_accuracy: 0.8884\n",
            "Epoch 219/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.9396\n",
            "Epoch 00219: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 961ms/step - loss: 0.1668 - accuracy: 0.9396 - val_loss: 0.7033 - val_accuracy: 0.8984\n",
            "Epoch 220/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.9382\n",
            "Epoch 00220: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 56s 956ms/step - loss: 0.1897 - accuracy: 0.9382 - val_loss: 0.7171 - val_accuracy: 0.8917\n",
            "Epoch 221/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1902 - accuracy: 0.9299\n",
            "Epoch 00221: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 56s 955ms/step - loss: 0.1902 - accuracy: 0.9299 - val_loss: 0.7009 - val_accuracy: 0.9018\n",
            "Epoch 222/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1794 - accuracy: 0.9339\n",
            "Epoch 00222: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 56s 958ms/step - loss: 0.1794 - accuracy: 0.9339 - val_loss: 0.6233 - val_accuracy: 0.9051\n",
            "Epoch 223/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1802 - accuracy: 0.9355\n",
            "Epoch 00223: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 56s 952ms/step - loss: 0.1802 - accuracy: 0.9355 - val_loss: 0.7017 - val_accuracy: 0.9118\n",
            "Epoch 224/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9382\n",
            "Epoch 00224: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 56s 954ms/step - loss: 0.1763 - accuracy: 0.9382 - val_loss: 0.6774 - val_accuracy: 0.9062\n",
            "Epoch 225/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9380\n",
            "Epoch 00225: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 961ms/step - loss: 0.1804 - accuracy: 0.9380 - val_loss: 1.0152 - val_accuracy: 0.8728\n",
            "Epoch 226/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1953 - accuracy: 0.9337\n",
            "Epoch 00226: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 959ms/step - loss: 0.1953 - accuracy: 0.9337 - val_loss: 0.7138 - val_accuracy: 0.8817\n",
            "Epoch 227/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1973 - accuracy: 0.9310\n",
            "Epoch 00227: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 56s 956ms/step - loss: 0.1973 - accuracy: 0.9310 - val_loss: 0.6235 - val_accuracy: 0.8940\n",
            "Epoch 228/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9390\n",
            "Epoch 00228: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 963ms/step - loss: 0.1623 - accuracy: 0.9390 - val_loss: 0.5726 - val_accuracy: 0.9107\n",
            "Epoch 229/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1772 - accuracy: 0.9377\n",
            "Epoch 00229: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 58s 979ms/step - loss: 0.1772 - accuracy: 0.9377 - val_loss: 0.5518 - val_accuracy: 0.9208\n",
            "Epoch 230/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1563 - accuracy: 0.9476\n",
            "Epoch 00230: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 972ms/step - loss: 0.1563 - accuracy: 0.9476 - val_loss: 0.6157 - val_accuracy: 0.9129\n",
            "Epoch 231/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9452\n",
            "Epoch 00231: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 58s 977ms/step - loss: 0.1606 - accuracy: 0.9452 - val_loss: 0.7138 - val_accuracy: 0.8795\n",
            "Epoch 232/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1790 - accuracy: 0.9361\n",
            "Epoch 00232: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 992ms/step - loss: 0.1790 - accuracy: 0.9361 - val_loss: 0.5838 - val_accuracy: 0.9118\n",
            "Epoch 233/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1872 - accuracy: 0.9355\n",
            "Epoch 00233: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1872 - accuracy: 0.9355 - val_loss: 0.7156 - val_accuracy: 0.8850\n",
            "Epoch 234/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9438\n",
            "Epoch 00234: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1811 - accuracy: 0.9438 - val_loss: 0.8155 - val_accuracy: 0.9085\n",
            "Epoch 235/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1916 - accuracy: 0.9366\n",
            "Epoch 00235: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1916 - accuracy: 0.9366 - val_loss: 0.6562 - val_accuracy: 0.9085\n",
            "Epoch 236/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1760 - accuracy: 0.9404\n",
            "Epoch 00236: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1760 - accuracy: 0.9404 - val_loss: 0.5790 - val_accuracy: 0.9219\n",
            "Epoch 237/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9454\n",
            "Epoch 00237: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 999ms/step - loss: 0.1554 - accuracy: 0.9454 - val_loss: 0.5832 - val_accuracy: 0.9152\n",
            "Epoch 238/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9425\n",
            "Epoch 00238: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1577 - accuracy: 0.9425 - val_loss: 0.4041 - val_accuracy: 0.9196\n",
            "Epoch 239/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1805 - accuracy: 0.9404\n",
            "Epoch 00239: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 993ms/step - loss: 0.1805 - accuracy: 0.9404 - val_loss: 0.5208 - val_accuracy: 0.9029\n",
            "Epoch 240/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1627 - accuracy: 0.9446\n",
            "Epoch 00240: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 997ms/step - loss: 0.1627 - accuracy: 0.9446 - val_loss: 0.5838 - val_accuracy: 0.9074\n",
            "Epoch 241/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1591 - accuracy: 0.9465\n",
            "Epoch 00241: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1591 - accuracy: 0.9465 - val_loss: 0.6255 - val_accuracy: 0.8895\n",
            "Epoch 242/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9457\n",
            "Epoch 00242: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1596 - accuracy: 0.9457 - val_loss: 0.7053 - val_accuracy: 0.8873\n",
            "Epoch 243/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9404\n",
            "Epoch 00243: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1759 - accuracy: 0.9404 - val_loss: 0.4971 - val_accuracy: 0.9051\n",
            "Epoch 244/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9489\n",
            "Epoch 00244: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 58s 991ms/step - loss: 0.1498 - accuracy: 0.9489 - val_loss: 0.5427 - val_accuracy: 0.9074\n",
            "Epoch 245/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1875 - accuracy: 0.9334\n",
            "Epoch 00245: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1875 - accuracy: 0.9334 - val_loss: 0.6529 - val_accuracy: 0.9185\n",
            "Epoch 246/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9425\n",
            "Epoch 00246: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1759 - accuracy: 0.9425 - val_loss: 0.8357 - val_accuracy: 0.8917\n",
            "Epoch 247/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1745 - accuracy: 0.9398\n",
            "Epoch 00247: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 59s 995ms/step - loss: 0.1745 - accuracy: 0.9398 - val_loss: 0.7194 - val_accuracy: 0.9085\n",
            "Epoch 248/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1564 - accuracy: 0.9425\n",
            "Epoch 00248: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 58s 981ms/step - loss: 0.1564 - accuracy: 0.9425 - val_loss: 0.4334 - val_accuracy: 0.9208\n",
            "Epoch 249/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1962 - accuracy: 0.9390\n",
            "Epoch 00249: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 58s 978ms/step - loss: 0.1962 - accuracy: 0.9390 - val_loss: 0.5826 - val_accuracy: 0.9007\n",
            "Epoch 250/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9406\n",
            "Epoch 00250: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 58s 981ms/step - loss: 0.1674 - accuracy: 0.9406 - val_loss: 0.6142 - val_accuracy: 0.9018\n",
            "Epoch 251/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1426 - accuracy: 0.9462\n",
            "Epoch 00251: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 57s 962ms/step - loss: 0.1426 - accuracy: 0.9462 - val_loss: 0.7402 - val_accuracy: 0.8951\n",
            "Epoch 252/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1598 - accuracy: 0.9486\n",
            "Epoch 00252: val_accuracy did not improve from 0.92299\n",
            "59/59 [==============================] - 58s 983ms/step - loss: 0.1598 - accuracy: 0.9486 - val_loss: 0.5828 - val_accuracy: 0.9096\n",
            "Epoch 253/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9462\n",
            "Epoch 00253: val_accuracy improved from 0.92299 to 0.92411, saving model to seedling2.h5\n",
            "59/59 [==============================] - 58s 988ms/step - loss: 0.1571 - accuracy: 0.9462 - val_loss: 0.6252 - val_accuracy: 0.9241\n",
            "Epoch 254/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9417\n",
            "Epoch 00254: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 974ms/step - loss: 0.1656 - accuracy: 0.9417 - val_loss: 0.5133 - val_accuracy: 0.9118\n",
            "Epoch 255/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1763 - accuracy: 0.9382\n",
            "Epoch 00255: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 973ms/step - loss: 0.1763 - accuracy: 0.9382 - val_loss: 0.7253 - val_accuracy: 0.9230\n",
            "Epoch 256/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9503\n",
            "Epoch 00256: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 970ms/step - loss: 0.1575 - accuracy: 0.9503 - val_loss: 0.6947 - val_accuracy: 0.8917\n",
            "Epoch 257/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1610 - accuracy: 0.9430\n",
            "Epoch 00257: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 968ms/step - loss: 0.1610 - accuracy: 0.9430 - val_loss: 0.6890 - val_accuracy: 0.9029\n",
            "Epoch 258/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1703 - accuracy: 0.9406\n",
            "Epoch 00258: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 58s 975ms/step - loss: 0.1703 - accuracy: 0.9406 - val_loss: 0.7801 - val_accuracy: 0.9085\n",
            "Epoch 259/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1755 - accuracy: 0.9393\n",
            "Epoch 00259: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 993ms/step - loss: 0.1755 - accuracy: 0.9393 - val_loss: 0.6740 - val_accuracy: 0.9085\n",
            "Epoch 260/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1652 - accuracy: 0.9425\n",
            "Epoch 00260: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 1000ms/step - loss: 0.1652 - accuracy: 0.9425 - val_loss: 0.5569 - val_accuracy: 0.9107\n",
            "Epoch 261/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9433\n",
            "Epoch 00261: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1638 - accuracy: 0.9433 - val_loss: 0.6954 - val_accuracy: 0.9174\n",
            "Epoch 262/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1418 - accuracy: 0.9486\n",
            "Epoch 00262: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1418 - accuracy: 0.9486 - val_loss: 0.6298 - val_accuracy: 0.9107\n",
            "Epoch 263/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9489\n",
            "Epoch 00263: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1476 - accuracy: 0.9489 - val_loss: 0.7281 - val_accuracy: 0.9051\n",
            "Epoch 264/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1577 - accuracy: 0.9454\n",
            "Epoch 00264: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 998ms/step - loss: 0.1577 - accuracy: 0.9454 - val_loss: 0.5890 - val_accuracy: 0.9163\n",
            "Epoch 265/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1828 - accuracy: 0.9414\n",
            "Epoch 00265: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 1000ms/step - loss: 0.1828 - accuracy: 0.9414 - val_loss: 0.6676 - val_accuracy: 0.9141\n",
            "Epoch 266/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1717 - accuracy: 0.9465\n",
            "Epoch 00266: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1717 - accuracy: 0.9465 - val_loss: 0.7220 - val_accuracy: 0.8984\n",
            "Epoch 267/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1590 - accuracy: 0.9478\n",
            "Epoch 00267: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 58s 983ms/step - loss: 0.1590 - accuracy: 0.9478 - val_loss: 0.6255 - val_accuracy: 0.9196\n",
            "Epoch 268/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1714 - accuracy: 0.9430\n",
            "Epoch 00268: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 58s 980ms/step - loss: 0.1714 - accuracy: 0.9430 - val_loss: 0.5554 - val_accuracy: 0.9230\n",
            "Epoch 269/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1774 - accuracy: 0.9412\n",
            "Epoch 00269: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 968ms/step - loss: 0.1774 - accuracy: 0.9412 - val_loss: 0.4750 - val_accuracy: 0.9196\n",
            "Epoch 270/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1667 - accuracy: 0.9454\n",
            "Epoch 00270: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 969ms/step - loss: 0.1667 - accuracy: 0.9454 - val_loss: 0.6371 - val_accuracy: 0.9185\n",
            "Epoch 271/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9465\n",
            "Epoch 00271: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 963ms/step - loss: 0.1623 - accuracy: 0.9465 - val_loss: 0.6649 - val_accuracy: 0.8940\n",
            "Epoch 272/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9492\n",
            "Epoch 00272: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 961ms/step - loss: 0.1514 - accuracy: 0.9492 - val_loss: 0.6473 - val_accuracy: 0.9196\n",
            "Epoch 273/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9468\n",
            "Epoch 00273: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 964ms/step - loss: 0.1483 - accuracy: 0.9468 - val_loss: 0.5750 - val_accuracy: 0.8940\n",
            "Epoch 274/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1596 - accuracy: 0.9462\n",
            "Epoch 00274: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 963ms/step - loss: 0.1596 - accuracy: 0.9462 - val_loss: 0.6412 - val_accuracy: 0.9185\n",
            "Epoch 275/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9519\n",
            "Epoch 00275: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 964ms/step - loss: 0.1340 - accuracy: 0.9519 - val_loss: 0.6206 - val_accuracy: 0.9185\n",
            "Epoch 276/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9508\n",
            "Epoch 00276: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 960ms/step - loss: 0.1367 - accuracy: 0.9508 - val_loss: 0.6093 - val_accuracy: 0.9141\n",
            "Epoch 277/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9524\n",
            "Epoch 00277: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 59s 996ms/step - loss: 0.1326 - accuracy: 0.9524 - val_loss: 0.5806 - val_accuracy: 0.9185\n",
            "Epoch 278/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1554 - accuracy: 0.9486\n",
            "Epoch 00278: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 963ms/step - loss: 0.1554 - accuracy: 0.9486 - val_loss: 0.5999 - val_accuracy: 0.9141\n",
            "Epoch 279/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9511\n",
            "Epoch 00279: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 965ms/step - loss: 0.1452 - accuracy: 0.9511 - val_loss: 0.5582 - val_accuracy: 0.9196\n",
            "Epoch 280/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1439 - accuracy: 0.9503\n",
            "Epoch 00280: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 966ms/step - loss: 0.1439 - accuracy: 0.9503 - val_loss: 0.6187 - val_accuracy: 0.9152\n",
            "Epoch 281/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1493 - accuracy: 0.9527\n",
            "Epoch 00281: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 966ms/step - loss: 0.1493 - accuracy: 0.9527 - val_loss: 0.6722 - val_accuracy: 0.8884\n",
            "Epoch 282/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9385\n",
            "Epoch 00282: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 56s 954ms/step - loss: 0.1691 - accuracy: 0.9385 - val_loss: 0.6291 - val_accuracy: 0.8984\n",
            "Epoch 283/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1801 - accuracy: 0.9446\n",
            "Epoch 00283: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 56s 957ms/step - loss: 0.1801 - accuracy: 0.9446 - val_loss: 0.6915 - val_accuracy: 0.8917\n",
            "Epoch 284/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1499 - accuracy: 0.9508\n",
            "Epoch 00284: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 971ms/step - loss: 0.1499 - accuracy: 0.9508 - val_loss: 0.6242 - val_accuracy: 0.9007\n",
            "Epoch 285/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1203 - accuracy: 0.9577\n",
            "Epoch 00285: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 56s 955ms/step - loss: 0.1203 - accuracy: 0.9577 - val_loss: 0.5936 - val_accuracy: 0.9174\n",
            "Epoch 286/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1599 - accuracy: 0.9492\n",
            "Epoch 00286: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 56s 957ms/step - loss: 0.1599 - accuracy: 0.9492 - val_loss: 0.6173 - val_accuracy: 0.9219\n",
            "Epoch 287/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1405 - accuracy: 0.9476\n",
            "Epoch 00287: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 56s 956ms/step - loss: 0.1405 - accuracy: 0.9476 - val_loss: 0.7569 - val_accuracy: 0.9018\n",
            "Epoch 288/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.9484\n",
            "Epoch 00288: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 56s 956ms/step - loss: 0.1635 - accuracy: 0.9484 - val_loss: 0.7017 - val_accuracy: 0.9007\n",
            "Epoch 289/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9428\n",
            "Epoch 00289: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 58s 978ms/step - loss: 0.1633 - accuracy: 0.9428 - val_loss: 0.6931 - val_accuracy: 0.8984\n",
            "Epoch 290/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1582 - accuracy: 0.9489\n",
            "Epoch 00290: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 57s 969ms/step - loss: 0.1582 - accuracy: 0.9489 - val_loss: 0.6014 - val_accuracy: 0.9074\n",
            "Epoch 291/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1824 - accuracy: 0.9422\n",
            "Epoch 00291: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 56s 956ms/step - loss: 0.1824 - accuracy: 0.9422 - val_loss: 0.5915 - val_accuracy: 0.9129\n",
            "Epoch 292/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1503 - accuracy: 0.9497\n",
            "Epoch 00292: val_accuracy did not improve from 0.92411\n",
            "59/59 [==============================] - 56s 954ms/step - loss: 0.1503 - accuracy: 0.9497 - val_loss: 0.6569 - val_accuracy: 0.8929\n",
            "Epoch 293/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1529 - accuracy: 0.9484\n",
            "Epoch 00293: val_accuracy improved from 0.92411 to 0.93415, saving model to seedling2.h5\n",
            "59/59 [==============================] - 57s 961ms/step - loss: 0.1529 - accuracy: 0.9484 - val_loss: 0.4605 - val_accuracy: 0.9342\n",
            "Epoch 294/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1351 - accuracy: 0.9500\n",
            "Epoch 00294: val_accuracy did not improve from 0.93415\n",
            "59/59 [==============================] - 56s 954ms/step - loss: 0.1351 - accuracy: 0.9500 - val_loss: 0.6246 - val_accuracy: 0.9141\n",
            "Epoch 295/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1337 - accuracy: 0.9553\n",
            "Epoch 00295: val_accuracy did not improve from 0.93415\n",
            "59/59 [==============================] - 57s 960ms/step - loss: 0.1337 - accuracy: 0.9553 - val_loss: 0.7501 - val_accuracy: 0.9029\n",
            "Epoch 296/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1779 - accuracy: 0.9425\n",
            "Epoch 00296: val_accuracy did not improve from 0.93415\n",
            "59/59 [==============================] - 57s 960ms/step - loss: 0.1779 - accuracy: 0.9425 - val_loss: 0.4833 - val_accuracy: 0.9062\n",
            "Epoch 297/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1571 - accuracy: 0.9492\n",
            "Epoch 00297: val_accuracy did not improve from 0.93415\n",
            "59/59 [==============================] - 57s 964ms/step - loss: 0.1571 - accuracy: 0.9492 - val_loss: 0.7079 - val_accuracy: 0.9152\n",
            "Epoch 298/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1296 - accuracy: 0.9527\n",
            "Epoch 00298: val_accuracy did not improve from 0.93415\n",
            "59/59 [==============================] - 57s 967ms/step - loss: 0.1296 - accuracy: 0.9527 - val_loss: 0.6778 - val_accuracy: 0.9230\n",
            "Epoch 299/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1514 - accuracy: 0.9511\n",
            "Epoch 00299: val_accuracy did not improve from 0.93415\n",
            "59/59 [==============================] - 57s 958ms/step - loss: 0.1514 - accuracy: 0.9511 - val_loss: 0.6243 - val_accuracy: 0.9096\n",
            "Epoch 300/300\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1246 - accuracy: 0.9569\n",
            "Epoch 00300: val_accuracy did not improve from 0.93415\n",
            "59/59 [==============================] - 56s 954ms/step - loss: 0.1246 - accuracy: 0.9569 - val_loss: 0.6065 - val_accuracy: 0.9297\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f05e118a080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQxipytKC-z7",
        "colab_type": "text"
      },
      "source": [
        "We can see that the validation accuracy of the model is 93.42%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrnhi45m7EwU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "30c7db6e-ed83-4ba0-dcea-e295003e3940"
      },
      "source": [
        "# Download the model on local machine.\n",
        "files.download('seedling2.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_d44b3ab5-3c1d-4e3d-a7c1-c74ea40b48b9\", \"seedling2.h5\", 77396288)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUstAj8sxF3d",
        "colab_type": "text"
      },
      "source": [
        "### 3. Build the model using Resnet50 prebuild model on keras using imagenet dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNOaK-ZBX4nh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_data(img):\n",
        "    \n",
        "    #Normalize for ResNet50\n",
        "    return tf.keras.applications.resnet50.preprocess_input(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BXRh_WnX9wj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
        "                                                                width_shift_range=0.2,\n",
        "                                                                height_shift_range=0.2,\n",
        "                                                                horizontal_flip=True,\n",
        "                                                                validation_split=.2,\n",
        "                                                                preprocessing_function=normalize_data) #Normalize the data accordingly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlybrS9VYnfF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "633fa6c0-bca9-49da-c299-5c42c7532bc1"
      },
      "source": [
        "#Build training generator. \n",
        "train_generator = image_generator.flow_from_directory('train',\n",
        "                                                    target_size=(img_size, img_size),\n",
        "                                                    subset='training',\n",
        "                                                    batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3803 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyYHH4e6YxiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "149cdc69-2575-44c7-ea7a-e495bdfd0d3d"
      },
      "source": [
        "#Build testing generator. \n",
        "test_generator = image_generator.flow_from_directory('train',\n",
        "                                                    target_size=(img_size, img_size),\n",
        "                                                    subset='validation',\n",
        "                                                    batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 947 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JC5eH4_Z2W84",
        "colab_type": "text"
      },
      "source": [
        "#### 3.1 Resnet50 all layers frozen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwFMK035ZdXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e434b385-6fd4-433f-e5e0-58a63c6e9411"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.applications.ResNet50(include_top=False, #Do not include classification layer for imagenet\n",
        "                                       input_shape=(img_size,img_size,3),\n",
        "                                       weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssYMB47aat62",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b1254ff7-c64f-43b5-b720-dd0e33799727"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 60, 60, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 66, 66, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 30, 30, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 30, 30, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 30, 30, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 32, 32, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 15, 15, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 15, 15, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 15, 15, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 15, 15, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 15, 15, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 15, 15, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 15, 15, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 15, 15, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 15, 15, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 15, 15, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 15, 15, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 15, 15, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 15, 15, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 15, 15, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 15, 15, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 15, 15, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 15, 15, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLfAoKk3c0T5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f668b5b6-6619-4f44-b97c-48419cb324c9"
      },
      "source": [
        "model.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'conv5_block3_out/Relu:0' shape=(None, 2, 2, 2048) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tSj1Yh3dzrk",
        "colab_type": "text"
      },
      "source": [
        "Freeze the layers in Pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz2OaqmOdp6N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7ac7364d-14c5-48a6-9417-8c1106bd0b2b"
      },
      "source": [
        "len(model.layers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aj62lDO9d1-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set pre-trained model layers to not trainable\n",
        "for layer in model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFgLtgB8d7_d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4838520d-34a3-46ef-a4b3-d4cf3ec3df9b"
      },
      "source": [
        "#Check if layers frozen\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 60, 60, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 66, 66, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 30, 30, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 30, 30, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 30, 30, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 32, 32, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 15, 15, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 15, 15, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 15, 15, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 15, 15, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 15, 15, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 15, 15, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 15, 15, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 15, 15, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 15, 15, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 15, 15, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 15, 15, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 15, 15, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 15, 15, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 15, 15, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 15, 15, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 15, 15, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 15, 15, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 0\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpCHKsDdefdJ",
        "colab_type": "text"
      },
      "source": [
        "As seen above all the layers are frozen.\n",
        "\n",
        "Add FC layer for new classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5uSelORd-B5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7a983240-ae99-404f-927e-3bba91ab912a"
      },
      "source": [
        "model.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'conv5_block3_out/Relu:0' shape=(None, 2, 2, 2048) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkgoC4XWejIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get Output layer of Pre0trained model\n",
        "x = model.output\n",
        "\n",
        "#Global average pool to reduce number of features and Flatten the output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olMSv37Aeq68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cebf473d-26f3-4461-913a-b489350dd804"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'global_average_pooling2d/Mean:0' shape=(None, 2048) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oruoG7zPet_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add output layer\n",
        "prediction = tf.keras.layers.Dense(12,activation='softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TWEqvZre61D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f7a9dab6-f45c-4481-ea4d-cfc6d72f6abc"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense/Softmax:0' shape=(None, 12) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OevvanSkfCCh",
        "colab_type": "text"
      },
      "source": [
        "Building final model for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgVgj7QUe9gK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using Keras Model class\n",
        "final_model = tf.keras.models.Model(inputs=model.input, #Pre-trained model input as input layer\n",
        "                                    outputs=prediction) #Output layer added"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ysg813IIfGat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model\n",
        "final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyeyWik_fggj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5abd6d50-c2be-4856-d361-6d6f53f5d419"
      },
      "source": [
        "#How does our overall model looks\n",
        "final_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"functional_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 60, 60, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 66, 66, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 30, 30, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 30, 30, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 30, 30, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 32, 32, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 15, 15, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 15, 15, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 15, 15, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 15, 15, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 15, 15, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 15, 15, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 15, 15, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 15, 15, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 15, 15, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 15, 15, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 15, 15, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 15, 15, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 15, 15, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 15, 15, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 15, 15, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 15, 15, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 15, 15, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 12)           24588       global_average_pooling2d[0][0]   \n",
            "==================================================================================================\n",
            "Total params: 23,612,300\n",
            "Trainable params: 24,588\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCOB2DeP0H5Z",
        "colab_type": "text"
      },
      "source": [
        "As observed there are just a few paramters for training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDkp66Pafsgq",
        "colab_type": "text"
      },
      "source": [
        "Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3GIHcLpfi0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the best model using model checkpoint callback\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('seedling3.h5', \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_accuracy', \n",
        "                                                    mode='max', \n",
        "                                                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa1Fo43yf8By",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9bfe74b9-de0d-45dc-cc05-0ffcce074bce"
      },
      "source": [
        "final_model.fit(train_generator, \n",
        "                          epochs=100,\n",
        "                          steps_per_epoch= 3803//64,\n",
        "                          validation_data=test_generator,\n",
        "                          validation_steps = 947//64, \n",
        "                          callbacks=[model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 2.2549 - accuracy: 0.3057\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.44085, saving model to seedling3.h5\n",
            "59/59 [==============================] - 56s 943ms/step - loss: 2.2549 - accuracy: 0.3057 - val_loss: 1.7045 - val_accuracy: 0.4408\n",
            "Epoch 2/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.6048 - accuracy: 0.4672\n",
            "Epoch 00002: val_accuracy improved from 0.44085 to 0.46763, saving model to seedling3.h5\n",
            "59/59 [==============================] - 52s 876ms/step - loss: 1.6048 - accuracy: 0.4672 - val_loss: 1.5658 - val_accuracy: 0.4676\n",
            "Epoch 3/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.4086 - accuracy: 0.5271\n",
            "Epoch 00003: val_accuracy improved from 0.46763 to 0.56362, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 869ms/step - loss: 1.4086 - accuracy: 0.5271 - val_loss: 1.3606 - val_accuracy: 0.5636\n",
            "Epoch 4/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.3637 - accuracy: 0.5445\n",
            "Epoch 00004: val_accuracy did not improve from 0.56362\n",
            "59/59 [==============================] - 50s 853ms/step - loss: 1.3637 - accuracy: 0.5445 - val_loss: 1.3182 - val_accuracy: 0.5435\n",
            "Epoch 5/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.2413 - accuracy: 0.5689\n",
            "Epoch 00005: val_accuracy did not improve from 0.56362\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 1.2413 - accuracy: 0.5689 - val_loss: 1.3205 - val_accuracy: 0.5569\n",
            "Epoch 6/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.1788 - accuracy: 0.5961\n",
            "Epoch 00006: val_accuracy did not improve from 0.56362\n",
            "59/59 [==============================] - 51s 857ms/step - loss: 1.1788 - accuracy: 0.5961 - val_loss: 1.3554 - val_accuracy: 0.5357\n",
            "Epoch 7/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.1649 - accuracy: 0.5991\n",
            "Epoch 00007: val_accuracy improved from 0.56362 to 0.58929, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 868ms/step - loss: 1.1649 - accuracy: 0.5991 - val_loss: 1.2390 - val_accuracy: 0.5893\n",
            "Epoch 8/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.1296 - accuracy: 0.6205\n",
            "Epoch 00008: val_accuracy improved from 0.58929 to 0.59263, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 859ms/step - loss: 1.1296 - accuracy: 0.6205 - val_loss: 1.1819 - val_accuracy: 0.5926\n",
            "Epoch 9/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.0774 - accuracy: 0.6269\n",
            "Epoch 00009: val_accuracy improved from 0.59263 to 0.59487, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 866ms/step - loss: 1.0774 - accuracy: 0.6269 - val_loss: 1.2037 - val_accuracy: 0.5949\n",
            "Epoch 10/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.0978 - accuracy: 0.6224\n",
            "Epoch 00010: val_accuracy improved from 0.59487 to 0.61942, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 857ms/step - loss: 1.0978 - accuracy: 0.6224 - val_loss: 1.1886 - val_accuracy: 0.6194\n",
            "Epoch 11/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.0491 - accuracy: 0.6397\n",
            "Epoch 00011: val_accuracy did not improve from 0.61942\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 1.0491 - accuracy: 0.6397 - val_loss: 1.1762 - val_accuracy: 0.6094\n",
            "Epoch 12/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.0217 - accuracy: 0.6515\n",
            "Epoch 00012: val_accuracy did not improve from 0.61942\n",
            "59/59 [==============================] - 51s 858ms/step - loss: 1.0217 - accuracy: 0.6515 - val_loss: 1.2160 - val_accuracy: 0.6183\n",
            "Epoch 13/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.0056 - accuracy: 0.6568\n",
            "Epoch 00013: val_accuracy improved from 0.61942 to 0.62054, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 868ms/step - loss: 1.0056 - accuracy: 0.6568 - val_loss: 1.1296 - val_accuracy: 0.6205\n",
            "Epoch 14/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9713 - accuracy: 0.6684\n",
            "Epoch 00014: val_accuracy did not improve from 0.62054\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.9713 - accuracy: 0.6684 - val_loss: 1.1607 - val_accuracy: 0.6127\n",
            "Epoch 15/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9602 - accuracy: 0.6678\n",
            "Epoch 00015: val_accuracy did not improve from 0.62054\n",
            "59/59 [==============================] - 50s 855ms/step - loss: 0.9602 - accuracy: 0.6678 - val_loss: 1.1461 - val_accuracy: 0.6150\n",
            "Epoch 16/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9642 - accuracy: 0.6734\n",
            "Epoch 00016: val_accuracy did not improve from 0.62054\n",
            "59/59 [==============================] - 50s 852ms/step - loss: 0.9642 - accuracy: 0.6734 - val_loss: 1.1074 - val_accuracy: 0.6172\n",
            "Epoch 17/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9781 - accuracy: 0.6504\n",
            "Epoch 00017: val_accuracy did not improve from 0.62054\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.9781 - accuracy: 0.6504 - val_loss: 1.2248 - val_accuracy: 0.5882\n",
            "Epoch 18/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9593 - accuracy: 0.6673\n",
            "Epoch 00018: val_accuracy did not improve from 0.62054\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.9593 - accuracy: 0.6673 - val_loss: 1.1385 - val_accuracy: 0.6127\n",
            "Epoch 19/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9273 - accuracy: 0.6804\n",
            "Epoch 00019: val_accuracy improved from 0.62054 to 0.64509, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 868ms/step - loss: 0.9273 - accuracy: 0.6804 - val_loss: 1.0472 - val_accuracy: 0.6451\n",
            "Epoch 20/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9414 - accuracy: 0.6775\n",
            "Epoch 00020: val_accuracy did not improve from 0.64509\n",
            "59/59 [==============================] - 50s 853ms/step - loss: 0.9414 - accuracy: 0.6775 - val_loss: 1.1587 - val_accuracy: 0.6384\n",
            "Epoch 21/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9323 - accuracy: 0.6689\n",
            "Epoch 00021: val_accuracy did not improve from 0.64509\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.9323 - accuracy: 0.6689 - val_loss: 1.0761 - val_accuracy: 0.6451\n",
            "Epoch 22/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9208 - accuracy: 0.6769\n",
            "Epoch 00022: val_accuracy did not improve from 0.64509\n",
            "59/59 [==============================] - 51s 857ms/step - loss: 0.9208 - accuracy: 0.6769 - val_loss: 1.1270 - val_accuracy: 0.6295\n",
            "Epoch 23/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9203 - accuracy: 0.6863\n",
            "Epoch 00023: val_accuracy did not improve from 0.64509\n",
            "59/59 [==============================] - 50s 853ms/step - loss: 0.9203 - accuracy: 0.6863 - val_loss: 1.0780 - val_accuracy: 0.6328\n",
            "Epoch 24/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9069 - accuracy: 0.6852\n",
            "Epoch 00024: val_accuracy did not improve from 0.64509\n",
            "59/59 [==============================] - 50s 855ms/step - loss: 0.9069 - accuracy: 0.6852 - val_loss: 1.1744 - val_accuracy: 0.6116\n",
            "Epoch 25/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8972 - accuracy: 0.6764\n",
            "Epoch 00025: val_accuracy improved from 0.64509 to 0.66071, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 869ms/step - loss: 0.8972 - accuracy: 0.6764 - val_loss: 1.1291 - val_accuracy: 0.6607\n",
            "Epoch 26/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8948 - accuracy: 0.6855\n",
            "Epoch 00026: val_accuracy did not improve from 0.66071\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.8948 - accuracy: 0.6855 - val_loss: 1.0969 - val_accuracy: 0.6239\n",
            "Epoch 27/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8960 - accuracy: 0.6932\n",
            "Epoch 00027: val_accuracy did not improve from 0.66071\n",
            "59/59 [==============================] - 51s 858ms/step - loss: 0.8960 - accuracy: 0.6932 - val_loss: 1.0731 - val_accuracy: 0.6585\n",
            "Epoch 28/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9049 - accuracy: 0.6833\n",
            "Epoch 00028: val_accuracy did not improve from 0.66071\n",
            "59/59 [==============================] - 50s 856ms/step - loss: 0.9049 - accuracy: 0.6833 - val_loss: 1.1582 - val_accuracy: 0.6172\n",
            "Epoch 29/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8817 - accuracy: 0.6849\n",
            "Epoch 00029: val_accuracy did not improve from 0.66071\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.8817 - accuracy: 0.6849 - val_loss: 1.0474 - val_accuracy: 0.6585\n",
            "Epoch 30/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8790 - accuracy: 0.6882\n",
            "Epoch 00030: val_accuracy did not improve from 0.66071\n",
            "59/59 [==============================] - 50s 853ms/step - loss: 0.8790 - accuracy: 0.6882 - val_loss: 1.1564 - val_accuracy: 0.6306\n",
            "Epoch 31/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8804 - accuracy: 0.6873\n",
            "Epoch 00031: val_accuracy did not improve from 0.66071\n",
            "59/59 [==============================] - 50s 854ms/step - loss: 0.8804 - accuracy: 0.6873 - val_loss: 1.1075 - val_accuracy: 0.6350\n",
            "Epoch 32/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8623 - accuracy: 0.7007\n",
            "Epoch 00032: val_accuracy improved from 0.66071 to 0.67634, saving model to seedling3.h5\n",
            "59/59 [==============================] - 51s 857ms/step - loss: 0.8623 - accuracy: 0.7007 - val_loss: 1.0337 - val_accuracy: 0.6763\n",
            "Epoch 33/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8798 - accuracy: 0.6898\n",
            "Epoch 00033: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 854ms/step - loss: 0.8798 - accuracy: 0.6898 - val_loss: 1.0624 - val_accuracy: 0.6551\n",
            "Epoch 34/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8184 - accuracy: 0.7114\n",
            "Epoch 00034: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.8184 - accuracy: 0.7114 - val_loss: 1.1501 - val_accuracy: 0.6272\n",
            "Epoch 35/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8774 - accuracy: 0.6919\n",
            "Epoch 00035: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.8774 - accuracy: 0.6919 - val_loss: 1.1302 - val_accuracy: 0.6272\n",
            "Epoch 36/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8590 - accuracy: 0.7002\n",
            "Epoch 00036: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.8590 - accuracy: 0.7002 - val_loss: 1.0566 - val_accuracy: 0.6507\n",
            "Epoch 37/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8743 - accuracy: 0.6980\n",
            "Epoch 00037: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.8743 - accuracy: 0.6980 - val_loss: 1.0653 - val_accuracy: 0.6529\n",
            "Epoch 38/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8359 - accuracy: 0.7141\n",
            "Epoch 00038: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 846ms/step - loss: 0.8359 - accuracy: 0.7141 - val_loss: 1.1084 - val_accuracy: 0.6395\n",
            "Epoch 39/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8246 - accuracy: 0.7122\n",
            "Epoch 00039: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.8246 - accuracy: 0.7122 - val_loss: 1.0848 - val_accuracy: 0.6496\n",
            "Epoch 40/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8489 - accuracy: 0.6991\n",
            "Epoch 00040: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.8489 - accuracy: 0.6991 - val_loss: 1.0572 - val_accuracy: 0.6328\n",
            "Epoch 41/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8483 - accuracy: 0.7045\n",
            "Epoch 00041: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 853ms/step - loss: 0.8483 - accuracy: 0.7045 - val_loss: 1.0760 - val_accuracy: 0.6451\n",
            "Epoch 42/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8424 - accuracy: 0.7013\n",
            "Epoch 00042: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.8424 - accuracy: 0.7013 - val_loss: 1.1136 - val_accuracy: 0.6328\n",
            "Epoch 43/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8460 - accuracy: 0.7029\n",
            "Epoch 00043: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 51s 856ms/step - loss: 0.8460 - accuracy: 0.7029 - val_loss: 1.0957 - val_accuracy: 0.6473\n",
            "Epoch 44/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8172 - accuracy: 0.7194\n",
            "Epoch 00044: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.8172 - accuracy: 0.7194 - val_loss: 1.0674 - val_accuracy: 0.6562\n",
            "Epoch 45/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8204 - accuracy: 0.7200\n",
            "Epoch 00045: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.8204 - accuracy: 0.7200 - val_loss: 1.0882 - val_accuracy: 0.6451\n",
            "Epoch 46/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8233 - accuracy: 0.7085\n",
            "Epoch 00046: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 842ms/step - loss: 0.8233 - accuracy: 0.7085 - val_loss: 1.0775 - val_accuracy: 0.6685\n",
            "Epoch 47/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8111 - accuracy: 0.7144\n",
            "Epoch 00047: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 852ms/step - loss: 0.8111 - accuracy: 0.7144 - val_loss: 1.0691 - val_accuracy: 0.6451\n",
            "Epoch 48/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8194 - accuracy: 0.7149\n",
            "Epoch 00048: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.8194 - accuracy: 0.7149 - val_loss: 1.0696 - val_accuracy: 0.6551\n",
            "Epoch 49/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8031 - accuracy: 0.7224\n",
            "Epoch 00049: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 854ms/step - loss: 0.8031 - accuracy: 0.7224 - val_loss: 1.0715 - val_accuracy: 0.6373\n",
            "Epoch 50/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8012 - accuracy: 0.7176\n",
            "Epoch 00050: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.8012 - accuracy: 0.7176 - val_loss: 1.0452 - val_accuracy: 0.6741\n",
            "Epoch 51/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8105 - accuracy: 0.7154\n",
            "Epoch 00051: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.8105 - accuracy: 0.7154 - val_loss: 1.1340 - val_accuracy: 0.6362\n",
            "Epoch 52/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8296 - accuracy: 0.7136\n",
            "Epoch 00052: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 842ms/step - loss: 0.8296 - accuracy: 0.7136 - val_loss: 1.0530 - val_accuracy: 0.6462\n",
            "Epoch 53/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8200 - accuracy: 0.7082\n",
            "Epoch 00053: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 843ms/step - loss: 0.8200 - accuracy: 0.7082 - val_loss: 1.0802 - val_accuracy: 0.6473\n",
            "Epoch 54/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8137 - accuracy: 0.7178\n",
            "Epoch 00054: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 846ms/step - loss: 0.8137 - accuracy: 0.7178 - val_loss: 1.0936 - val_accuracy: 0.6652\n",
            "Epoch 55/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7817 - accuracy: 0.7243\n",
            "Epoch 00055: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 51s 856ms/step - loss: 0.7817 - accuracy: 0.7243 - val_loss: 1.0261 - val_accuracy: 0.6585\n",
            "Epoch 56/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8088 - accuracy: 0.7087\n",
            "Epoch 00056: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.8088 - accuracy: 0.7087 - val_loss: 1.1291 - val_accuracy: 0.6283\n",
            "Epoch 57/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8017 - accuracy: 0.7128\n",
            "Epoch 00057: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.8017 - accuracy: 0.7128 - val_loss: 1.0631 - val_accuracy: 0.6618\n",
            "Epoch 58/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8064 - accuracy: 0.7079\n",
            "Epoch 00058: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 850ms/step - loss: 0.8064 - accuracy: 0.7079 - val_loss: 1.0819 - val_accuracy: 0.6562\n",
            "Epoch 59/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8037 - accuracy: 0.7146\n",
            "Epoch 00059: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.8037 - accuracy: 0.7146 - val_loss: 1.1241 - val_accuracy: 0.6384\n",
            "Epoch 60/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7860 - accuracy: 0.7189\n",
            "Epoch 00060: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 850ms/step - loss: 0.7860 - accuracy: 0.7189 - val_loss: 1.0858 - val_accuracy: 0.6473\n",
            "Epoch 61/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8083 - accuracy: 0.7176\n",
            "Epoch 00061: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.8083 - accuracy: 0.7176 - val_loss: 1.0729 - val_accuracy: 0.6607\n",
            "Epoch 62/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8057 - accuracy: 0.7170\n",
            "Epoch 00062: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.8057 - accuracy: 0.7170 - val_loss: 1.0533 - val_accuracy: 0.6629\n",
            "Epoch 63/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8208 - accuracy: 0.7141\n",
            "Epoch 00063: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 852ms/step - loss: 0.8208 - accuracy: 0.7141 - val_loss: 1.0518 - val_accuracy: 0.6730\n",
            "Epoch 64/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7904 - accuracy: 0.7216\n",
            "Epoch 00064: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 852ms/step - loss: 0.7904 - accuracy: 0.7216 - val_loss: 1.1303 - val_accuracy: 0.6551\n",
            "Epoch 65/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8105 - accuracy: 0.7149\n",
            "Epoch 00065: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.8105 - accuracy: 0.7149 - val_loss: 1.0949 - val_accuracy: 0.6551\n",
            "Epoch 66/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8164 - accuracy: 0.7128\n",
            "Epoch 00066: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 853ms/step - loss: 0.8164 - accuracy: 0.7128 - val_loss: 1.1067 - val_accuracy: 0.6395\n",
            "Epoch 67/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8065 - accuracy: 0.7125\n",
            "Epoch 00067: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 849ms/step - loss: 0.8065 - accuracy: 0.7125 - val_loss: 1.1010 - val_accuracy: 0.6518\n",
            "Epoch 68/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8035 - accuracy: 0.7202\n",
            "Epoch 00068: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 850ms/step - loss: 0.8035 - accuracy: 0.7202 - val_loss: 1.1039 - val_accuracy: 0.6429\n",
            "Epoch 69/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7930 - accuracy: 0.7229\n",
            "Epoch 00069: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 850ms/step - loss: 0.7930 - accuracy: 0.7229 - val_loss: 1.0635 - val_accuracy: 0.6730\n",
            "Epoch 70/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8064 - accuracy: 0.7240\n",
            "Epoch 00070: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.8064 - accuracy: 0.7240 - val_loss: 1.1814 - val_accuracy: 0.6217\n",
            "Epoch 71/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8040 - accuracy: 0.7186\n",
            "Epoch 00071: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 846ms/step - loss: 0.8040 - accuracy: 0.7186 - val_loss: 1.0872 - val_accuracy: 0.6763\n",
            "Epoch 72/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8196 - accuracy: 0.7168\n",
            "Epoch 00072: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.8196 - accuracy: 0.7168 - val_loss: 1.0735 - val_accuracy: 0.6496\n",
            "Epoch 73/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7798 - accuracy: 0.7264\n",
            "Epoch 00073: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 846ms/step - loss: 0.7798 - accuracy: 0.7264 - val_loss: 1.1219 - val_accuracy: 0.6451\n",
            "Epoch 74/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7724 - accuracy: 0.7285\n",
            "Epoch 00074: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 854ms/step - loss: 0.7724 - accuracy: 0.7285 - val_loss: 1.1003 - val_accuracy: 0.6629\n",
            "Epoch 75/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7969 - accuracy: 0.7184\n",
            "Epoch 00075: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 850ms/step - loss: 0.7969 - accuracy: 0.7184 - val_loss: 0.9763 - val_accuracy: 0.6752\n",
            "Epoch 76/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8220 - accuracy: 0.7165\n",
            "Epoch 00076: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.8220 - accuracy: 0.7165 - val_loss: 1.1386 - val_accuracy: 0.6384\n",
            "Epoch 77/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7947 - accuracy: 0.7245\n",
            "Epoch 00077: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.7947 - accuracy: 0.7245 - val_loss: 1.1706 - val_accuracy: 0.6362\n",
            "Epoch 78/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7796 - accuracy: 0.7253\n",
            "Epoch 00078: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 850ms/step - loss: 0.7796 - accuracy: 0.7253 - val_loss: 1.1304 - val_accuracy: 0.6172\n",
            "Epoch 79/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8068 - accuracy: 0.7251\n",
            "Epoch 00079: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 842ms/step - loss: 0.8068 - accuracy: 0.7251 - val_loss: 1.1584 - val_accuracy: 0.6295\n",
            "Epoch 80/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7677 - accuracy: 0.7336\n",
            "Epoch 00080: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 853ms/step - loss: 0.7677 - accuracy: 0.7336 - val_loss: 1.1469 - val_accuracy: 0.6440\n",
            "Epoch 81/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7863 - accuracy: 0.7232\n",
            "Epoch 00081: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.7863 - accuracy: 0.7232 - val_loss: 1.0874 - val_accuracy: 0.6652\n",
            "Epoch 82/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7723 - accuracy: 0.7307\n",
            "Epoch 00082: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 841ms/step - loss: 0.7723 - accuracy: 0.7307 - val_loss: 1.1290 - val_accuracy: 0.6540\n",
            "Epoch 83/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8092 - accuracy: 0.7176\n",
            "Epoch 00083: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.8092 - accuracy: 0.7176 - val_loss: 1.1442 - val_accuracy: 0.6451\n",
            "Epoch 84/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7783 - accuracy: 0.7309\n",
            "Epoch 00084: val_accuracy did not improve from 0.67634\n",
            "59/59 [==============================] - 49s 836ms/step - loss: 0.7783 - accuracy: 0.7309 - val_loss: 1.0508 - val_accuracy: 0.6607\n",
            "Epoch 85/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7877 - accuracy: 0.7261\n",
            "Epoch 00085: val_accuracy improved from 0.67634 to 0.68192, saving model to seedling3.h5\n",
            "59/59 [==============================] - 50s 852ms/step - loss: 0.7877 - accuracy: 0.7261 - val_loss: 1.0501 - val_accuracy: 0.6819\n",
            "Epoch 86/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7983 - accuracy: 0.7259\n",
            "Epoch 00086: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 843ms/step - loss: 0.7983 - accuracy: 0.7259 - val_loss: 1.1426 - val_accuracy: 0.6350\n",
            "Epoch 87/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8070 - accuracy: 0.7186\n",
            "Epoch 00087: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.8070 - accuracy: 0.7186 - val_loss: 1.1348 - val_accuracy: 0.6529\n",
            "Epoch 88/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7788 - accuracy: 0.7264\n",
            "Epoch 00088: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.7788 - accuracy: 0.7264 - val_loss: 1.0503 - val_accuracy: 0.6775\n",
            "Epoch 89/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8009 - accuracy: 0.7221\n",
            "Epoch 00089: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.8009 - accuracy: 0.7221 - val_loss: 1.0879 - val_accuracy: 0.6518\n",
            "Epoch 90/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7777 - accuracy: 0.7237\n",
            "Epoch 00090: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 845ms/step - loss: 0.7777 - accuracy: 0.7237 - val_loss: 1.1266 - val_accuracy: 0.6362\n",
            "Epoch 91/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7813 - accuracy: 0.7283\n",
            "Epoch 00091: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 851ms/step - loss: 0.7813 - accuracy: 0.7283 - val_loss: 1.1046 - val_accuracy: 0.6574\n",
            "Epoch 92/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7428 - accuracy: 0.7430\n",
            "Epoch 00092: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.7428 - accuracy: 0.7430 - val_loss: 1.0012 - val_accuracy: 0.6741\n",
            "Epoch 93/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7559 - accuracy: 0.7243\n",
            "Epoch 00093: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 847ms/step - loss: 0.7559 - accuracy: 0.7243 - val_loss: 1.0836 - val_accuracy: 0.6596\n",
            "Epoch 94/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7727 - accuracy: 0.7219\n",
            "Epoch 00094: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.7727 - accuracy: 0.7219 - val_loss: 1.1619 - val_accuracy: 0.6328\n",
            "Epoch 95/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7578 - accuracy: 0.7366\n",
            "Epoch 00095: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 846ms/step - loss: 0.7578 - accuracy: 0.7366 - val_loss: 1.0588 - val_accuracy: 0.6752\n",
            "Epoch 96/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7725 - accuracy: 0.7293\n",
            "Epoch 00096: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 841ms/step - loss: 0.7725 - accuracy: 0.7293 - val_loss: 1.1312 - val_accuracy: 0.6607\n",
            "Epoch 97/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.8023 - accuracy: 0.7200\n",
            "Epoch 00097: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 844ms/step - loss: 0.8023 - accuracy: 0.7200 - val_loss: 1.1970 - val_accuracy: 0.6629\n",
            "Epoch 98/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7693 - accuracy: 0.7312\n",
            "Epoch 00098: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 51s 856ms/step - loss: 0.7693 - accuracy: 0.7312 - val_loss: 1.1571 - val_accuracy: 0.6574\n",
            "Epoch 99/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7883 - accuracy: 0.7269\n",
            "Epoch 00099: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 846ms/step - loss: 0.7883 - accuracy: 0.7269 - val_loss: 1.0947 - val_accuracy: 0.6518\n",
            "Epoch 100/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.7743 - accuracy: 0.7259\n",
            "Epoch 00100: val_accuracy did not improve from 0.68192\n",
            "59/59 [==============================] - 50s 848ms/step - loss: 0.7743 - accuracy: 0.7259 - val_loss: 1.0291 - val_accuracy: 0.6652\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f98b3f07f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gO717cMhbeD",
        "colab_type": "text"
      },
      "source": [
        "If we see above the model starts off well and has reached very good accuracy very soon, but stops performing beyond certain level. This might be because we have frozen all learning parameter of Resnet. \n",
        "\n",
        "Enabling a few parameters so that the model can learn more"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHejt0Q0voi7",
        "colab_type": "text"
      },
      "source": [
        "#### 3.2 Resnet50 all layers enabled for learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_3RleuW-Mzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "38ac7e30-4765-4b36-ba10-cb200a08fc43"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = tf.keras.applications.ResNet50(include_top=False, #Do not include classification layer for imagenet\n",
        "                                       input_shape=(img_size,img_size,3),\n",
        "                                       weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dMZqKl-wriS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b4a92f15-d53d-44a6-b093-347456d2b40a"
      },
      "source": [
        "#Check if layers frozen\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet50\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 60, 60, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 66, 66, 3)    0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 30, 30, 64)   9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 30, 30, 64)   256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 30, 30, 64)   0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 32, 32, 64)   0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 15, 15, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 15, 15, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 15, 15, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 15, 15, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 15, 15, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 15, 15, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 15, 15, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 15, 15, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 15, 15, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 15, 15, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 15, 15, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 15, 15, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 15, 15, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 15, 15, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 15, 15, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 15, 15, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 15, 15, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 15, 15, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 15, 15, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 15, 15, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 15, 15, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 8, 8, 128)    32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 8, 8, 128)    0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 8, 8, 128)    0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 8, 8, 512)    131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 8, 8, 512)    0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 8, 8, 512)    0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 8, 8, 128)    0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 8, 8, 128)    0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 8, 8, 512)    0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 8, 8, 512)    0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 8, 8, 128)    0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 8, 8, 128)    0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 8, 8, 512)    0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 8, 8, 512)    0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 8, 8, 128)    65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 8, 8, 128)    0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 8, 8, 128)    147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 8, 8, 128)    512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 8, 8, 128)    0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 8, 8, 512)    66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 8, 8, 512)    2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 8, 8, 512)    0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 8, 8, 512)    0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 4, 4, 256)    131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 4, 4, 256)    0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 4, 4, 256)    0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 4, 4, 1024)   525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 4, 4, 1024)   0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 4, 4, 256)    0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 4, 4, 256)    0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 4, 4, 1024)   0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 4, 4, 1024)   0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 4, 4, 256)    0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 4, 4, 256)    0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 4, 4, 1024)   0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 4, 4, 1024)   0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 4, 4, 256)    0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 4, 4, 256)    0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 4, 4, 1024)   0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 4, 4, 1024)   0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 4, 4, 256)    0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 4, 4, 256)    0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 4, 4, 1024)   0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 4, 4, 1024)   0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 4, 4, 256)    262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 4, 4, 256)    0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 4, 4, 256)    590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 4, 4, 256)    1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 4, 4, 256)    0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 4, 4, 1024)   263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 4, 4, 1024)   4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 4, 4, 1024)   0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 4, 4, 1024)   0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 2, 2, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 2, 2, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 2, 2, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 2, 2, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 2, 2, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 2, 2, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 2, 2, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 2, 2, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 2, 2, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 2, 2, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 2, 2, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 2, 2, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 2, 2, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 2, 2, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 2, 2, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 2, 2, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 2, 2, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 2, 2, 2048)   0           conv5_block3_add[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 23,587,712\n",
            "Trainable params: 23,534,592\n",
            "Non-trainable params: 53,120\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOZdsEgO_l3X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a8516521-d5b5-40af-8447-a45f03d5fb24"
      },
      "source": [
        "model.output"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'conv5_block3_out/Relu:0' shape=(None, 2, 2, 2048) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqX2HgfQ_tZL",
        "colab_type": "text"
      },
      "source": [
        "As seen above all the layers are frozen.\n",
        "\n",
        "Add FC layer for new classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUdon7Tq_pAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get Output layer of Pre0trained model\n",
        "x = model.output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XOg9NIH_wbD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Global average pool to reduce number of features and Flatten the output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmGXzhAw_4WE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3de0e226-c617-44e9-e3c5-76a7ece891df"
      },
      "source": [
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'global_average_pooling2d/Mean:0' shape=(None, 2048) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz0tdVnK_8YG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add output layer\n",
        "prediction = tf.keras.layers.Dense(12,activation='softmax')(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBpCkMBZAFto",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d42947cc-093c-41cb-b368-23b86133a8a0"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'dense/Softmax:0' shape=(None, 12) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62hzLaRXAL3k",
        "colab_type": "text"
      },
      "source": [
        "Building final model for Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGF9FDnOAIYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using Keras Model class\n",
        "final_model = tf.keras.models.Model(inputs=model.input, #Pre-trained model input as input layer\n",
        "                                    outputs=prediction) #Output layer added"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_sPs42sAPeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compile the model\n",
        "final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtcPg51S1xCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the best model using model checkpoint callback\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('seedling4.h5', \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_accuracy', \n",
        "                                                    mode='max', \n",
        "                                                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdooqMcg19De",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ca40b28-a9b6-461e-9cd3-33677778ff91"
      },
      "source": [
        "final_model.fit(train_generator, \n",
        "                          epochs=100,\n",
        "                          steps_per_epoch= 3803//64,\n",
        "                          validation_data=test_generator,\n",
        "                          validation_steps = 947//64, \n",
        "                          callbacks=[model_checkpoint])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 1.8290 - accuracy: 0.4539\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.08147, saving model to seedling4.h5\n",
            "59/59 [==============================] - 62s 1s/step - loss: 1.8290 - accuracy: 0.4539 - val_loss: 266.2794 - val_accuracy: 0.0815\n",
            "Epoch 2/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.9519 - accuracy: 0.6927\n",
            "Epoch 00002: val_accuracy improved from 0.08147 to 0.08929, saving model to seedling4.h5\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.9519 - accuracy: 0.6927 - val_loss: 53.3476 - val_accuracy: 0.0893\n",
            "Epoch 3/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6676 - accuracy: 0.7713\n",
            "Epoch 00003: val_accuracy improved from 0.08929 to 0.22321, saving model to seedling4.h5\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.6676 - accuracy: 0.7713 - val_loss: 7.7830 - val_accuracy: 0.2232\n",
            "Epoch 4/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.6059 - accuracy: 0.8010\n",
            "Epoch 00004: val_accuracy did not improve from 0.22321\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.6059 - accuracy: 0.8010 - val_loss: 201.1830 - val_accuracy: 0.1964\n",
            "Epoch 5/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.5354 - accuracy: 0.8149\n",
            "Epoch 00005: val_accuracy improved from 0.22321 to 0.45647, saving model to seedling4.h5\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.5354 - accuracy: 0.8149 - val_loss: 8.2494 - val_accuracy: 0.4565\n",
            "Epoch 6/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.4297 - accuracy: 0.8473\n",
            "Epoch 00006: val_accuracy did not improve from 0.45647\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.4297 - accuracy: 0.8473 - val_loss: 3.0334 - val_accuracy: 0.4096\n",
            "Epoch 7/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.8601\n",
            "Epoch 00007: val_accuracy improved from 0.45647 to 0.75670, saving model to seedling4.h5\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.3929 - accuracy: 0.8601 - val_loss: 0.8669 - val_accuracy: 0.7567\n",
            "Epoch 8/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3618 - accuracy: 0.8719\n",
            "Epoch 00008: val_accuracy improved from 0.75670 to 0.80804, saving model to seedling4.h5\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.3618 - accuracy: 0.8719 - val_loss: 0.6475 - val_accuracy: 0.8080\n",
            "Epoch 9/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.8847\n",
            "Epoch 00009: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.3312 - accuracy: 0.8847 - val_loss: 2.0628 - val_accuracy: 0.5201\n",
            "Epoch 10/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3116 - accuracy: 0.8842\n",
            "Epoch 00010: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.3116 - accuracy: 0.8842 - val_loss: 1.0503 - val_accuracy: 0.7467\n",
            "Epoch 11/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3071 - accuracy: 0.8871\n",
            "Epoch 00011: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.3071 - accuracy: 0.8871 - val_loss: 1.7051 - val_accuracy: 0.6629\n",
            "Epoch 12/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9048\n",
            "Epoch 00012: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.2647 - accuracy: 0.9048 - val_loss: 0.9883 - val_accuracy: 0.7377\n",
            "Epoch 13/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.8941\n",
            "Epoch 00013: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.2878 - accuracy: 0.8941 - val_loss: 1.1294 - val_accuracy: 0.7065\n",
            "Epoch 14/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2175 - accuracy: 0.9198\n",
            "Epoch 00014: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.2175 - accuracy: 0.9198 - val_loss: 1.4188 - val_accuracy: 0.6786\n",
            "Epoch 15/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2768 - accuracy: 0.9040\n",
            "Epoch 00015: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.2768 - accuracy: 0.9040 - val_loss: 2.9657 - val_accuracy: 0.4554\n",
            "Epoch 16/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.8957\n",
            "Epoch 00016: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.3108 - accuracy: 0.8957 - val_loss: 117.2192 - val_accuracy: 0.3471\n",
            "Epoch 17/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.3678 - accuracy: 0.8748\n",
            "Epoch 00017: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.3678 - accuracy: 0.8748 - val_loss: 1.3367 - val_accuracy: 0.6373\n",
            "Epoch 18/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2730 - accuracy: 0.9026\n",
            "Epoch 00018: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 59s 1000ms/step - loss: 0.2730 - accuracy: 0.9026 - val_loss: 1.5192 - val_accuracy: 0.6975\n",
            "Epoch 19/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2236 - accuracy: 0.9144\n",
            "Epoch 00019: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 59s 993ms/step - loss: 0.2236 - accuracy: 0.9144 - val_loss: 1.0476 - val_accuracy: 0.7020\n",
            "Epoch 20/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1987 - accuracy: 0.9232\n",
            "Epoch 00020: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 58s 990ms/step - loss: 0.1987 - accuracy: 0.9232 - val_loss: 1.1981 - val_accuracy: 0.7455\n",
            "Epoch 21/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2189 - accuracy: 0.9163\n",
            "Epoch 00021: val_accuracy did not improve from 0.80804\n",
            "59/59 [==============================] - 59s 998ms/step - loss: 0.2189 - accuracy: 0.9163 - val_loss: 1.2524 - val_accuracy: 0.7556\n",
            "Epoch 22/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2030 - accuracy: 0.9238\n",
            "Epoch 00022: val_accuracy improved from 0.80804 to 0.81250, saving model to seedling4.h5\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.2030 - accuracy: 0.9238 - val_loss: 0.8966 - val_accuracy: 0.8125\n",
            "Epoch 23/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1934 - accuracy: 0.9307\n",
            "Epoch 00023: val_accuracy improved from 0.81250 to 0.87723, saving model to seedling4.h5\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1934 - accuracy: 0.9307 - val_loss: 0.4470 - val_accuracy: 0.8772\n",
            "Epoch 24/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2029 - accuracy: 0.9334\n",
            "Epoch 00024: val_accuracy did not improve from 0.87723\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.2029 - accuracy: 0.9334 - val_loss: 3.1799 - val_accuracy: 0.4721\n",
            "Epoch 25/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2095 - accuracy: 0.9289\n",
            "Epoch 00025: val_accuracy did not improve from 0.87723\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.2095 - accuracy: 0.9289 - val_loss: 1.0705 - val_accuracy: 0.7455\n",
            "Epoch 26/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1873 - accuracy: 0.9331\n",
            "Epoch 00026: val_accuracy did not improve from 0.87723\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1873 - accuracy: 0.9331 - val_loss: 0.9292 - val_accuracy: 0.7455\n",
            "Epoch 27/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2690 - accuracy: 0.9093\n",
            "Epoch 00027: val_accuracy did not improve from 0.87723\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.2690 - accuracy: 0.9093 - val_loss: 3.3165 - val_accuracy: 0.5491\n",
            "Epoch 28/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2227 - accuracy: 0.9227\n",
            "Epoch 00028: val_accuracy did not improve from 0.87723\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.2227 - accuracy: 0.9227 - val_loss: 1.3831 - val_accuracy: 0.7054\n",
            "Epoch 29/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9371\n",
            "Epoch 00029: val_accuracy did not improve from 0.87723\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1773 - accuracy: 0.9371 - val_loss: 0.5863 - val_accuracy: 0.8359\n",
            "Epoch 30/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9511\n",
            "Epoch 00030: val_accuracy improved from 0.87723 to 0.89397, saving model to seedling4.h5\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.1406 - accuracy: 0.9511 - val_loss: 0.3534 - val_accuracy: 0.8940\n",
            "Epoch 31/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1560 - accuracy: 0.9452\n",
            "Epoch 00031: val_accuracy did not improve from 0.89397\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.1560 - accuracy: 0.9452 - val_loss: 0.5827 - val_accuracy: 0.8125\n",
            "Epoch 32/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9454\n",
            "Epoch 00032: val_accuracy did not improve from 0.89397\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1477 - accuracy: 0.9454 - val_loss: 0.5953 - val_accuracy: 0.8415\n",
            "Epoch 33/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1504 - accuracy: 0.9481\n",
            "Epoch 00033: val_accuracy did not improve from 0.89397\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1504 - accuracy: 0.9481 - val_loss: 0.7969 - val_accuracy: 0.7969\n",
            "Epoch 34/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9476\n",
            "Epoch 00034: val_accuracy did not improve from 0.89397\n",
            "59/59 [==============================] - 64s 1s/step - loss: 0.1366 - accuracy: 0.9476 - val_loss: 0.6051 - val_accuracy: 0.8248\n",
            "Epoch 35/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9561\n",
            "Epoch 00035: val_accuracy improved from 0.89397 to 0.89621, saving model to seedling4.h5\n",
            "59/59 [==============================] - 65s 1s/step - loss: 0.1276 - accuracy: 0.9561 - val_loss: 0.4133 - val_accuracy: 0.8962\n",
            "Epoch 36/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1233 - accuracy: 0.9553\n",
            "Epoch 00036: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 64s 1s/step - loss: 0.1233 - accuracy: 0.9553 - val_loss: 0.5179 - val_accuracy: 0.8549\n",
            "Epoch 37/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9596\n",
            "Epoch 00037: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.1161 - accuracy: 0.9596 - val_loss: 0.6927 - val_accuracy: 0.8315\n",
            "Epoch 38/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1443 - accuracy: 0.9438\n",
            "Epoch 00038: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1443 - accuracy: 0.9438 - val_loss: 0.7890 - val_accuracy: 0.8058\n",
            "Epoch 39/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1175 - accuracy: 0.9569\n",
            "Epoch 00039: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.1175 - accuracy: 0.9569 - val_loss: 0.5918 - val_accuracy: 0.8705\n",
            "Epoch 40/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1344 - accuracy: 0.9529\n",
            "Epoch 00040: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1344 - accuracy: 0.9529 - val_loss: 0.5677 - val_accuracy: 0.8605\n",
            "Epoch 41/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1314 - accuracy: 0.9511\n",
            "Epoch 00041: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1314 - accuracy: 0.9511 - val_loss: 0.6328 - val_accuracy: 0.8426\n",
            "Epoch 42/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1323 - accuracy: 0.9508\n",
            "Epoch 00042: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.1323 - accuracy: 0.9508 - val_loss: 0.6944 - val_accuracy: 0.8259\n",
            "Epoch 43/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1543 - accuracy: 0.9465\n",
            "Epoch 00043: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 58s 990ms/step - loss: 0.1543 - accuracy: 0.9465 - val_loss: 1.3513 - val_accuracy: 0.7801\n",
            "Epoch 44/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1730 - accuracy: 0.9444\n",
            "Epoch 00044: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 58s 981ms/step - loss: 0.1730 - accuracy: 0.9444 - val_loss: 8.0685 - val_accuracy: 0.2891\n",
            "Epoch 45/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2053 - accuracy: 0.9307\n",
            "Epoch 00045: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 59s 993ms/step - loss: 0.2053 - accuracy: 0.9307 - val_loss: 1.1661 - val_accuracy: 0.7388\n",
            "Epoch 46/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1549 - accuracy: 0.9420\n",
            "Epoch 00046: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1549 - accuracy: 0.9420 - val_loss: 0.9107 - val_accuracy: 0.7656\n",
            "Epoch 47/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1721 - accuracy: 0.9366\n",
            "Epoch 00047: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.1721 - accuracy: 0.9366 - val_loss: 2.3818 - val_accuracy: 0.6071\n",
            "Epoch 48/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9521\n",
            "Epoch 00048: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.1317 - accuracy: 0.9521 - val_loss: 2.4082 - val_accuracy: 0.5480\n",
            "Epoch 49/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1970 - accuracy: 0.9353\n",
            "Epoch 00049: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1970 - accuracy: 0.9353 - val_loss: 1.4236 - val_accuracy: 0.6484\n",
            "Epoch 50/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9527\n",
            "Epoch 00050: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1340 - accuracy: 0.9527 - val_loss: 2.2700 - val_accuracy: 0.7031\n",
            "Epoch 51/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1077 - accuracy: 0.9599\n",
            "Epoch 00051: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1077 - accuracy: 0.9599 - val_loss: 1.0099 - val_accuracy: 0.8058\n",
            "Epoch 52/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9585\n",
            "Epoch 00052: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1216 - accuracy: 0.9585 - val_loss: 0.8569 - val_accuracy: 0.7734\n",
            "Epoch 53/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1100 - accuracy: 0.9660\n",
            "Epoch 00053: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1100 - accuracy: 0.9660 - val_loss: 0.6343 - val_accuracy: 0.8382\n",
            "Epoch 54/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1303 - accuracy: 0.9551\n",
            "Epoch 00054: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1303 - accuracy: 0.9551 - val_loss: 0.8502 - val_accuracy: 0.7790\n",
            "Epoch 55/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1065 - accuracy: 0.9591\n",
            "Epoch 00055: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 59s 998ms/step - loss: 0.1065 - accuracy: 0.9591 - val_loss: 0.6108 - val_accuracy: 0.8415\n",
            "Epoch 56/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1059 - accuracy: 0.9615\n",
            "Epoch 00056: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.1059 - accuracy: 0.9615 - val_loss: 0.6136 - val_accuracy: 0.8605\n",
            "Epoch 57/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1857 - accuracy: 0.9468\n",
            "Epoch 00057: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.1857 - accuracy: 0.9468 - val_loss: 4.7258 - val_accuracy: 0.3136\n",
            "Epoch 58/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.4095 - accuracy: 0.8831\n",
            "Epoch 00058: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.4095 - accuracy: 0.8831 - val_loss: 2.0761 - val_accuracy: 0.6786\n",
            "Epoch 59/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.2160 - accuracy: 0.9265\n",
            "Epoch 00059: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.2160 - accuracy: 0.9265 - val_loss: 1.0540 - val_accuracy: 0.7355\n",
            "Epoch 60/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1446 - accuracy: 0.9473\n",
            "Epoch 00060: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.1446 - accuracy: 0.9473 - val_loss: 0.9102 - val_accuracy: 0.7946\n",
            "Epoch 61/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1460 - accuracy: 0.9495\n",
            "Epoch 00061: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.1460 - accuracy: 0.9495 - val_loss: 0.9294 - val_accuracy: 0.7924\n",
            "Epoch 62/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1136 - accuracy: 0.9575\n",
            "Epoch 00062: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.1136 - accuracy: 0.9575 - val_loss: 0.7305 - val_accuracy: 0.8337\n",
            "Epoch 63/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9663\n",
            "Epoch 00063: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.0904 - accuracy: 0.9663 - val_loss: 0.4839 - val_accuracy: 0.8728\n",
            "Epoch 64/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9628\n",
            "Epoch 00064: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.1029 - accuracy: 0.9628 - val_loss: 0.5452 - val_accuracy: 0.8605\n",
            "Epoch 65/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9679\n",
            "Epoch 00065: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0876 - accuracy: 0.9679 - val_loss: 1.3822 - val_accuracy: 0.7333\n",
            "Epoch 66/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9730\n",
            "Epoch 00066: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.0749 - accuracy: 0.9730 - val_loss: 0.7442 - val_accuracy: 0.8359\n",
            "Epoch 67/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0988 - accuracy: 0.9663\n",
            "Epoch 00067: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.0988 - accuracy: 0.9663 - val_loss: 0.4781 - val_accuracy: 0.8962\n",
            "Epoch 68/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9612\n",
            "Epoch 00068: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 59s 998ms/step - loss: 0.1081 - accuracy: 0.9612 - val_loss: 0.7139 - val_accuracy: 0.8315\n",
            "Epoch 69/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0940 - accuracy: 0.9700\n",
            "Epoch 00069: val_accuracy did not improve from 0.89621\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0940 - accuracy: 0.9700 - val_loss: 0.5442 - val_accuracy: 0.8828\n",
            "Epoch 70/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9695\n",
            "Epoch 00070: val_accuracy improved from 0.89621 to 0.89955, saving model to seedling4.h5\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.0810 - accuracy: 0.9695 - val_loss: 0.4446 - val_accuracy: 0.8996\n",
            "Epoch 71/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0665 - accuracy: 0.9783\n",
            "Epoch 00071: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.0665 - accuracy: 0.9783 - val_loss: 0.6465 - val_accuracy: 0.8627\n",
            "Epoch 72/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9668\n",
            "Epoch 00072: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.0963 - accuracy: 0.9668 - val_loss: 0.6860 - val_accuracy: 0.8504\n",
            "Epoch 73/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0842 - accuracy: 0.9706\n",
            "Epoch 00073: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0842 - accuracy: 0.9706 - val_loss: 0.9758 - val_accuracy: 0.7612\n",
            "Epoch 74/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0669 - accuracy: 0.9746\n",
            "Epoch 00074: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0669 - accuracy: 0.9746 - val_loss: 0.8439 - val_accuracy: 0.7980\n",
            "Epoch 75/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9708\n",
            "Epoch 00075: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.0857 - accuracy: 0.9708 - val_loss: 0.7075 - val_accuracy: 0.8359\n",
            "Epoch 76/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0871 - accuracy: 0.9700\n",
            "Epoch 00076: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.0871 - accuracy: 0.9700 - val_loss: 0.5726 - val_accuracy: 0.8795\n",
            "Epoch 77/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9781\n",
            "Epoch 00077: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0604 - accuracy: 0.9781 - val_loss: 0.6872 - val_accuracy: 0.8248\n",
            "Epoch 78/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9730\n",
            "Epoch 00078: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0779 - accuracy: 0.9730 - val_loss: 0.5014 - val_accuracy: 0.8940\n",
            "Epoch 79/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0839 - accuracy: 0.9722\n",
            "Epoch 00079: val_accuracy did not improve from 0.89955\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0839 - accuracy: 0.9722 - val_loss: 0.5277 - val_accuracy: 0.8661\n",
            "Epoch 80/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9733\n",
            "Epoch 00080: val_accuracy improved from 0.89955 to 0.90848, saving model to seedling4.h5\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.0703 - accuracy: 0.9733 - val_loss: 0.3791 - val_accuracy: 0.9085\n",
            "Epoch 81/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9725\n",
            "Epoch 00081: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0832 - accuracy: 0.9725 - val_loss: 0.3815 - val_accuracy: 0.9051\n",
            "Epoch 82/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1216 - accuracy: 0.9585\n",
            "Epoch 00082: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.1216 - accuracy: 0.9585 - val_loss: 0.4597 - val_accuracy: 0.8571\n",
            "Epoch 83/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9714\n",
            "Epoch 00083: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 64s 1s/step - loss: 0.0814 - accuracy: 0.9714 - val_loss: 0.4762 - val_accuracy: 0.8839\n",
            "Epoch 84/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0681 - accuracy: 0.9778\n",
            "Epoch 00084: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 63s 1s/step - loss: 0.0681 - accuracy: 0.9778 - val_loss: 0.8935 - val_accuracy: 0.8125\n",
            "Epoch 85/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0825 - accuracy: 0.9698\n",
            "Epoch 00085: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 64s 1s/step - loss: 0.0825 - accuracy: 0.9698 - val_loss: 0.9094 - val_accuracy: 0.8092\n",
            "Epoch 86/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9711\n",
            "Epoch 00086: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.0776 - accuracy: 0.9711 - val_loss: 1.0826 - val_accuracy: 0.7757\n",
            "Epoch 87/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0658 - accuracy: 0.9762\n",
            "Epoch 00087: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0658 - accuracy: 0.9762 - val_loss: 0.6070 - val_accuracy: 0.8482\n",
            "Epoch 88/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9749\n",
            "Epoch 00088: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.0736 - accuracy: 0.9749 - val_loss: 0.9190 - val_accuracy: 0.7935\n",
            "Epoch 89/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0919 - accuracy: 0.9692\n",
            "Epoch 00089: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 58s 991ms/step - loss: 0.0919 - accuracy: 0.9692 - val_loss: 0.5509 - val_accuracy: 0.8650\n",
            "Epoch 90/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0671 - accuracy: 0.9786\n",
            "Epoch 00090: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 58s 989ms/step - loss: 0.0671 - accuracy: 0.9786 - val_loss: 0.6606 - val_accuracy: 0.8828\n",
            "Epoch 91/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9778\n",
            "Epoch 00091: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 58s 991ms/step - loss: 0.0728 - accuracy: 0.9778 - val_loss: 0.5856 - val_accuracy: 0.8371\n",
            "Epoch 92/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0814 - accuracy: 0.9708\n",
            "Epoch 00092: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 57s 969ms/step - loss: 0.0814 - accuracy: 0.9708 - val_loss: 0.9576 - val_accuracy: 0.8136\n",
            "Epoch 93/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0934 - accuracy: 0.9668\n",
            "Epoch 00093: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 58s 980ms/step - loss: 0.0934 - accuracy: 0.9668 - val_loss: 0.5958 - val_accuracy: 0.8415\n",
            "Epoch 94/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9783\n",
            "Epoch 00094: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0696 - accuracy: 0.9783 - val_loss: 0.4570 - val_accuracy: 0.8917\n",
            "Epoch 95/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0716 - accuracy: 0.9783\n",
            "Epoch 00095: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 62s 1s/step - loss: 0.0716 - accuracy: 0.9783 - val_loss: 1.1351 - val_accuracy: 0.7701\n",
            "Epoch 96/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9762\n",
            "Epoch 00096: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 59s 1s/step - loss: 0.0670 - accuracy: 0.9762 - val_loss: 0.8401 - val_accuracy: 0.8158\n",
            "Epoch 97/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0597 - accuracy: 0.9813\n",
            "Epoch 00097: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0597 - accuracy: 0.9813 - val_loss: 0.6705 - val_accuracy: 0.8315\n",
            "Epoch 98/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9759\n",
            "Epoch 00098: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 61s 1s/step - loss: 0.0761 - accuracy: 0.9759 - val_loss: 0.8274 - val_accuracy: 0.8292\n",
            "Epoch 99/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9741\n",
            "Epoch 00099: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0799 - accuracy: 0.9741 - val_loss: 1.1375 - val_accuracy: 0.7723\n",
            "Epoch 100/100\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9733\n",
            "Epoch 00100: val_accuracy did not improve from 0.90848\n",
            "59/59 [==============================] - 60s 1s/step - loss: 0.0751 - accuracy: 0.9733 - val_loss: 0.6503 - val_accuracy: 0.8493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5e50173630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TnBw0xrYqo7",
        "colab_type": "text"
      },
      "source": [
        "As can be seen the validation has not been improved over 20 epochs and it seems the model is also overfitting hence discarding this model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYvptMNT2FW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "8995b5ef-5cbc-483b-e4ae-a764c6dc4528"
      },
      "source": [
        "files.download('seedling4.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_cb366644-f098-4d80-92ab-0355c899a6df\", \"seedling4.h5\", 283911328)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIvRUzYgSWNR",
        "colab_type": "text"
      },
      "source": [
        "### 4. Final Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAakIR0VS29i",
        "colab_type": "text"
      },
      "source": [
        "We could conclude that the Resnet50 or the pre-trained model did not perform as good as the model from start. Taking the 2nd model from start as the base model for analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEqm7a_PTOx1",
        "colab_type": "text"
      },
      "source": [
        "Load the \"seedling2.h5\" model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqPjQaNZSbB9",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f36edb22-815a-4d59-e412-c291f4cff60f"
      },
      "source": [
        "files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b5a5f82d-997f-453f-b30d-33c4fcce867b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b5a5f82d-997f-453f-b30d-33c4fcce867b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving seedling2.h5 to seedling2.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0IKgF4zA_Et",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "60f40f26-fac7-48b4-cf72-e09184cf0c93"
      },
      "source": [
        "ls -l"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 75652\n",
            "drwx------  4 root root     4096 Aug 23 16:20 \u001b[0m\u001b[01;34mdrive\u001b[0m/\n",
            "drwxr-xr-x  1 root root     4096 Jul 30 16:30 \u001b[01;34msample_data\u001b[0m/\n",
            "-rw-r--r--  1 root root    19863 Dec 11  2019 sample_submission.csv\n",
            "-rw-r--r--  1 root root 77396288 Aug 23 16:44 seedling2.h5\n",
            "drwxr-xr-x  2 root root    36864 Aug 23 16:20 \u001b[01;34mtest\u001b[0m/\n",
            "drwxr-xr-x 14 root root     4096 Aug 23 16:21 \u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6K_kQMwTuqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Clear any previous model from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "final_model = load_model('seedling2.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og72xo7z6moj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 503
        },
        "outputId": "bc91f0d6-b56f-43f1-90de-4f78d1019bad"
      },
      "source": [
        "final_model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "batch_normalization (BatchNo (None, 60, 60, 3)         12        \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 58, 58, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 58, 58, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 56, 56, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 56, 56, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 50176)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               6422656   \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 12)                1548      \n",
            "=================================================================\n",
            "Total params: 6,443,992\n",
            "Trainable params: 6,443,794\n",
            "Non-trainable params: 198\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLHzq-rK6s_P",
        "colab_type": "text"
      },
      "source": [
        "Trying to run for a few (50) more epochs with EarlyStopping = 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOJtNKZr6trf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a46f6407-88ca-476c-9be7-a4558e0e4ab5"
      },
      "source": [
        "earlyStop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
        "#Saving the best model using model checkpoint callback\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('seedling2.h5', \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_accuracy', \n",
        "                                                    mode='max', \n",
        "                                                    verbose=1)\n",
        "final_model.fit(train_generator,\n",
        "          epochs=400,\n",
        "          initial_epoch=300,\n",
        "          steps_per_epoch= 3803//64,  #Number of training images//batch_size\n",
        "          validation_data=test_generator,\n",
        "          validation_steps = 947//64, #Number of test images//batch_size\n",
        "          callbacks = [model_checkpoint, earlyStop])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 301/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9457\n",
            "Epoch 00301: val_accuracy improved from -inf to 0.90625, saving model to seedling2.h5\n",
            "59/59 [==============================] - 67s 1s/step - loss: 0.1658 - accuracy: 0.9457 - val_loss: 0.6738 - val_accuracy: 0.9062\n",
            "Epoch 302/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9537\n",
            "Epoch 00302: val_accuracy improved from 0.90625 to 0.90848, saving model to seedling2.h5\n",
            "59/59 [==============================] - 56s 942ms/step - loss: 0.1406 - accuracy: 0.9537 - val_loss: 0.6507 - val_accuracy: 0.9085\n",
            "Epoch 303/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1249 - accuracy: 0.9591\n",
            "Epoch 00303: val_accuracy improved from 0.90848 to 0.91518, saving model to seedling2.h5\n",
            "59/59 [==============================] - 55s 935ms/step - loss: 0.1249 - accuracy: 0.9591 - val_loss: 0.6712 - val_accuracy: 0.9152\n",
            "Epoch 304/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9492\n",
            "Epoch 00304: val_accuracy did not improve from 0.91518\n",
            "59/59 [==============================] - 56s 945ms/step - loss: 0.1553 - accuracy: 0.9492 - val_loss: 0.5948 - val_accuracy: 0.9074\n",
            "Epoch 305/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1643 - accuracy: 0.9473\n",
            "Epoch 00305: val_accuracy improved from 0.91518 to 0.91964, saving model to seedling2.h5\n",
            "59/59 [==============================] - 55s 935ms/step - loss: 0.1643 - accuracy: 0.9473 - val_loss: 0.6077 - val_accuracy: 0.9196\n",
            "Epoch 306/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1395 - accuracy: 0.9516\n",
            "Epoch 00306: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 55s 940ms/step - loss: 0.1395 - accuracy: 0.9516 - val_loss: 0.6756 - val_accuracy: 0.9040\n",
            "Epoch 307/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1319 - accuracy: 0.9545\n",
            "Epoch 00307: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 942ms/step - loss: 0.1319 - accuracy: 0.9545 - val_loss: 0.8869 - val_accuracy: 0.9051\n",
            "Epoch 308/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9537\n",
            "Epoch 00308: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 943ms/step - loss: 0.1404 - accuracy: 0.9537 - val_loss: 0.5516 - val_accuracy: 0.9018\n",
            "Epoch 309/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9484\n",
            "Epoch 00309: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 948ms/step - loss: 0.1535 - accuracy: 0.9484 - val_loss: 0.6351 - val_accuracy: 0.9196\n",
            "Epoch 310/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1658 - accuracy: 0.9489\n",
            "Epoch 00310: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 943ms/step - loss: 0.1658 - accuracy: 0.9489 - val_loss: 0.5160 - val_accuracy: 0.9118\n",
            "Epoch 311/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1561 - accuracy: 0.9508\n",
            "Epoch 00311: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 947ms/step - loss: 0.1561 - accuracy: 0.9508 - val_loss: 0.5608 - val_accuracy: 0.9152\n",
            "Epoch 312/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1364 - accuracy: 0.9532\n",
            "Epoch 00312: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 55s 932ms/step - loss: 0.1364 - accuracy: 0.9532 - val_loss: 0.6423 - val_accuracy: 0.8962\n",
            "Epoch 313/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9551\n",
            "Epoch 00313: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 55s 936ms/step - loss: 0.1477 - accuracy: 0.9551 - val_loss: 0.6418 - val_accuracy: 0.9152\n",
            "Epoch 314/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9462\n",
            "Epoch 00314: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 55s 939ms/step - loss: 0.1575 - accuracy: 0.9462 - val_loss: 0.7418 - val_accuracy: 0.9018\n",
            "Epoch 315/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1424 - accuracy: 0.9505\n",
            "Epoch 00315: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 947ms/step - loss: 0.1424 - accuracy: 0.9505 - val_loss: 0.6163 - val_accuracy: 0.9007\n",
            "Epoch 316/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1538 - accuracy: 0.9470\n",
            "Epoch 00316: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 55s 937ms/step - loss: 0.1538 - accuracy: 0.9470 - val_loss: 0.5166 - val_accuracy: 0.9096\n",
            "Epoch 317/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1407 - accuracy: 0.9519\n",
            "Epoch 00317: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 55s 936ms/step - loss: 0.1407 - accuracy: 0.9519 - val_loss: 0.8073 - val_accuracy: 0.9018\n",
            "Epoch 318/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1631 - accuracy: 0.9417\n",
            "Epoch 00318: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 942ms/step - loss: 0.1631 - accuracy: 0.9417 - val_loss: 0.6638 - val_accuracy: 0.9029\n",
            "Epoch 319/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9527\n",
            "Epoch 00319: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 55s 939ms/step - loss: 0.1317 - accuracy: 0.9527 - val_loss: 0.6390 - val_accuracy: 0.9196\n",
            "Epoch 320/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9519\n",
            "Epoch 00320: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 55s 939ms/step - loss: 0.1390 - accuracy: 0.9519 - val_loss: 0.6571 - val_accuracy: 0.9174\n",
            "Epoch 321/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1404 - accuracy: 0.9516\n",
            "Epoch 00321: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 950ms/step - loss: 0.1404 - accuracy: 0.9516 - val_loss: 0.7055 - val_accuracy: 0.9040\n",
            "Epoch 322/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9577\n",
            "Epoch 00322: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 57s 961ms/step - loss: 0.1431 - accuracy: 0.9577 - val_loss: 0.7925 - val_accuracy: 0.9185\n",
            "Epoch 323/400\n",
            "59/59 [==============================] - ETA: 0s - loss: 0.1671 - accuracy: 0.9452\n",
            "Epoch 00323: val_accuracy did not improve from 0.91964\n",
            "59/59 [==============================] - 56s 941ms/step - loss: 0.1671 - accuracy: 0.9452 - val_loss: 0.8806 - val_accuracy: 0.8984\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffb21949908>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cpBRNfWTx0W",
        "colab_type": "text"
      },
      "source": [
        "From above we can see that that training accuracy is about 95.5% and testing accuracy is 92%\n",
        "\n",
        "This seems to be and otimal model and not over fitting with a good accuracy level"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIryPXat60a3",
        "colab_type": "text"
      },
      "source": [
        "## Make the prediction for the test images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DJPocdG6xIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = []\n",
        "# Read the data from the test folder which has all the test images to make the predictions.\n",
        "import os\n",
        "directory=''\n",
        "for entry in os.scandir('test'):\n",
        "    image = tf.keras.preprocessing.image.load_img(path='test/'+entry.name, target_size=(60,60))\n",
        "    input_arr = tf.keras.preprocessing.image.img_to_array(image)\n",
        "    input_arr = np.array([input_arr])  # Convert single image to a batch.\n",
        "    predictions.append([entry.name, all_categories[np.argmax((final_model.predict(input_arr))[0])]])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iyQJPfj67Pd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6a81973a-5d1e-400d-a511-fc96ee353173"
      },
      "source": [
        "predictions"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['8301b0547.png', 'Charlock'],\n",
              " ['862b8e7a0.png', 'Sugar beet'],\n",
              " ['a800caead.png', 'Sugar beet'],\n",
              " ['a74d475c2.png', 'Common Chickweed'],\n",
              " ['490c4f9c8.png', 'Fat Hen'],\n",
              " ['4bbfd1e05.png', 'Cleavers'],\n",
              " ['6908fb540.png', 'Sugar beet'],\n",
              " ['2df78338c.png', 'Common Chickweed'],\n",
              " ['3fbd0fc6a.png', 'Common Chickweed'],\n",
              " ['b39c71707.png', 'Maize'],\n",
              " ['6b9d6f8c9.png', 'Sugar beet'],\n",
              " ['e4d5ec761.png', 'Loose Silky-bent'],\n",
              " ['e19673dc9.png', 'Charlock'],\n",
              " ['a276c65f7.png', 'Loose Silky-bent'],\n",
              " ['659412b1a.png', 'Loose Silky-bent'],\n",
              " ['59358cd44.png', 'Small-flowered Cranesbill'],\n",
              " ['d09d24c58.png', 'Loose Silky-bent'],\n",
              " ['8c98a6e9b.png', 'Loose Silky-bent'],\n",
              " ['6be169e41.png', 'Shepherds Purse'],\n",
              " ['8b043093d.png', 'Small-flowered Cranesbill'],\n",
              " ['b130a0632.png', 'Cleavers'],\n",
              " ['abc331628.png', 'Common wheat'],\n",
              " ['59b2c6f2b.png', 'Common Chickweed'],\n",
              " ['a2d25b4f3.png', 'Small-flowered Cranesbill'],\n",
              " ['fea355851.png', 'Loose Silky-bent'],\n",
              " ['88ac6df54.png', 'Loose Silky-bent'],\n",
              " ['1d48b7564.png', 'Loose Silky-bent'],\n",
              " ['bebcaab66.png', 'Scentless Mayweed'],\n",
              " ['c63da993b.png', 'Sugar beet'],\n",
              " ['39d569be4.png', 'Loose Silky-bent'],\n",
              " ['6bce55e05.png', 'Charlock'],\n",
              " ['f8318faf1.png', 'Scentless Mayweed'],\n",
              " ['cf3a8b2fd.png', 'Loose Silky-bent'],\n",
              " ['e1809cef2.png', 'Scentless Mayweed'],\n",
              " ['2bd74d2da.png', 'Loose Silky-bent'],\n",
              " ['406ecb5c5.png', 'Charlock'],\n",
              " ['fadc6adbc.png', 'Charlock'],\n",
              " ['00c47e980.png', 'Sugar beet'],\n",
              " ['486e59179.png', 'Common Chickweed'],\n",
              " ['42e7ed442.png', 'Shepherds Purse'],\n",
              " ['456d507c0.png', 'Cleavers'],\n",
              " ['73260a4ee.png', 'Fat Hen'],\n",
              " ['b7192c70f.png', 'Common wheat'],\n",
              " ['6da892be6.png', 'Small-flowered Cranesbill'],\n",
              " ['1d56351b2.png', 'Common Chickweed'],\n",
              " ['87608f7aa.png', 'Charlock'],\n",
              " ['87f627bf9.png', 'Common Chickweed'],\n",
              " ['5315c2dca.png', 'Loose Silky-bent'],\n",
              " ['391dcd7fd.png', 'Loose Silky-bent'],\n",
              " ['0c51bf229.png', 'Maize'],\n",
              " ['590f5aea6.png', 'Charlock'],\n",
              " ['c06e7c748.png', 'Scentless Mayweed'],\n",
              " ['827279bad.png', 'Scentless Mayweed'],\n",
              " ['cf90fc52d.png', 'Maize'],\n",
              " ['9326bda1b.png', 'Common Chickweed'],\n",
              " ['eef131644.png', 'Loose Silky-bent'],\n",
              " ['f3fcfff1b.png', 'Cleavers'],\n",
              " ['22e79540f.png', 'Cleavers'],\n",
              " ['d689256be.png', 'Cleavers'],\n",
              " ['8faadb6a8.png', 'Sugar beet'],\n",
              " ['43ede9de9.png', 'Charlock'],\n",
              " ['de0b79659.png', 'Fat Hen'],\n",
              " ['060f1dc84.png', 'Shepherds Purse'],\n",
              " ['258b1183c.png', 'Cleavers'],\n",
              " ['8e3ed0a25.png', 'Small-flowered Cranesbill'],\n",
              " ['521b27a17.png', 'Loose Silky-bent'],\n",
              " ['3f64c2c1b.png', 'Common wheat'],\n",
              " ['7506c0c02.png', 'Scentless Mayweed'],\n",
              " ['2a667e099.png', 'Fat Hen'],\n",
              " ['165681fd9.png', 'Shepherds Purse'],\n",
              " ['4392d93cf.png', 'Fat Hen'],\n",
              " ['6ba4ef411.png', 'Cleavers'],\n",
              " ['37c3108d6.png', 'Shepherds Purse'],\n",
              " ['71f5323c5.png', 'Loose Silky-bent'],\n",
              " ['56112b92c.png', 'Small-flowered Cranesbill'],\n",
              " ['5c3cd7ea2.png', 'Scentless Mayweed'],\n",
              " ['721b2c47a.png', 'Fat Hen'],\n",
              " ['6edb96d45.png', 'Cleavers'],\n",
              " ['dcd7ff249.png', 'Scentless Mayweed'],\n",
              " ['20817c846.png', 'Scentless Mayweed'],\n",
              " ['e1a0e3202.png', 'Loose Silky-bent'],\n",
              " ['8a30b2de3.png', 'Sugar beet'],\n",
              " ['e80a259c5.png', 'Small-flowered Cranesbill'],\n",
              " ['dc55449b2.png', 'Sugar beet'],\n",
              " ['4049a6ea6.png', 'Loose Silky-bent'],\n",
              " ['2e86f1085.png', 'Fat Hen'],\n",
              " ['92292055d.png', 'Common Chickweed'],\n",
              " ['b7ad92859.png', 'Shepherds Purse'],\n",
              " ['78b1bf91a.png', 'Sugar beet'],\n",
              " ['bf3924a57.png', 'Common Chickweed'],\n",
              " ['e5064f6be.png', 'Sugar beet'],\n",
              " ['2a5064f19.png', 'Common wheat'],\n",
              " ['0caeda5df.png', 'Common wheat'],\n",
              " ['4f44ca525.png', 'Common Chickweed'],\n",
              " ['36ed4f215.png', 'Common Chickweed'],\n",
              " ['9cce7328c.png', 'Loose Silky-bent'],\n",
              " ['e73e308be.png', 'Fat Hen'],\n",
              " ['dc4cd56a3.png', 'Loose Silky-bent'],\n",
              " ['3bbef3ecb.png', 'Scentless Mayweed'],\n",
              " ['86676d627.png', 'Fat Hen'],\n",
              " ['3d67c434b.png', 'Fat Hen'],\n",
              " ['eaf0815e2.png', 'Small-flowered Cranesbill'],\n",
              " ['e19ad6ac9.png', 'Charlock'],\n",
              " ['c60e91e07.png', 'Loose Silky-bent'],\n",
              " ['19e58cc5e.png', 'Common Chickweed'],\n",
              " ['16357b436.png', 'Loose Silky-bent'],\n",
              " ['e5e3dccff.png', 'Common wheat'],\n",
              " ['1c52ea820.png', 'Fat Hen'],\n",
              " ['35cf9fa01.png', 'Shepherds Purse'],\n",
              " ['85431c075.png', 'Loose Silky-bent'],\n",
              " ['7d22abf91.png', 'Scentless Mayweed'],\n",
              " ['df7cb5f87.png', 'Common Chickweed'],\n",
              " ['1dc7c45df.png', 'Loose Silky-bent'],\n",
              " ['8170d33c1.png', 'Maize'],\n",
              " ['686dc7ec8.png', 'Loose Silky-bent'],\n",
              " ['0ebf8f2f4.png', 'Maize'],\n",
              " ['7f46a71db.png', 'Sugar beet'],\n",
              " ['b341d0aab.png', 'Common Chickweed'],\n",
              " ['5e6a237f2.png', 'Sugar beet'],\n",
              " ['d84d37a61.png', 'Common Chickweed'],\n",
              " ['7f9e9565d.png', 'Scentless Mayweed'],\n",
              " ['377283a21.png', 'Maize'],\n",
              " ['b29339405.png', 'Loose Silky-bent'],\n",
              " ['043449b0b.png', 'Sugar beet'],\n",
              " ['d6c8c3c48.png', 'Small-flowered Cranesbill'],\n",
              " ['ef02b4ee7.png', 'Sugar beet'],\n",
              " ['03a2ee656.png', 'Small-flowered Cranesbill'],\n",
              " ['a19c3faca.png', 'Shepherds Purse'],\n",
              " ['39c8fde99.png', 'Loose Silky-bent'],\n",
              " ['6c874918c.png', 'Black-grass'],\n",
              " ['589e643b8.png', 'Shepherds Purse'],\n",
              " ['995c7ab1e.png', 'Loose Silky-bent'],\n",
              " ['bffc08672.png', 'Scentless Mayweed'],\n",
              " ['ce3d280eb.png', 'Common Chickweed'],\n",
              " ['8dbb8e1b9.png', 'Charlock'],\n",
              " ['03566743d.png', 'Fat Hen'],\n",
              " ['752101fdf.png', 'Small-flowered Cranesbill'],\n",
              " ['5296835a0.png', 'Small-flowered Cranesbill'],\n",
              " ['efe19dc32.png', 'Charlock'],\n",
              " ['897e5a8de.png', 'Cleavers'],\n",
              " ['c50335991.png', 'Fat Hen'],\n",
              " ['48d97c645.png', 'Small-flowered Cranesbill'],\n",
              " ['1376f3b63.png', 'Loose Silky-bent'],\n",
              " ['32b42c120.png', 'Small-flowered Cranesbill'],\n",
              " ['b30ab4659.png', 'Cleavers'],\n",
              " ['a83820a2c.png', 'Sugar beet'],\n",
              " ['fbf88b6be.png', 'Charlock'],\n",
              " ['9fab816f2.png', 'Fat Hen'],\n",
              " ['8a4d3a1b1.png', 'Loose Silky-bent'],\n",
              " ['1364b297a.png', 'Cleavers'],\n",
              " ['2b55a2da2.png', 'Charlock'],\n",
              " ['c88ebfb47.png', 'Small-flowered Cranesbill'],\n",
              " ['0885e7690.png', 'Scentless Mayweed'],\n",
              " ['780bd2a2c.png', 'Sugar beet'],\n",
              " ['11d3f68ff.png', 'Loose Silky-bent'],\n",
              " ['a5db42f7d.png', 'Maize'],\n",
              " ['8d6991365.png', 'Black-grass'],\n",
              " ['da5255450.png', 'Common Chickweed'],\n",
              " ['699d3c707.png', 'Sugar beet'],\n",
              " ['279df95f2.png', 'Sugar beet'],\n",
              " ['2f0004a7f.png', 'Common wheat'],\n",
              " ['172f9b10b.png', 'Common Chickweed'],\n",
              " ['e921021a8.png', 'Fat Hen'],\n",
              " ['3da774107.png', 'Fat Hen'],\n",
              " ['b145ba9d4.png', 'Scentless Mayweed'],\n",
              " ['17529c555.png', 'Scentless Mayweed'],\n",
              " ['0bf7bfb05.png', 'Loose Silky-bent'],\n",
              " ['19618ad6a.png', 'Sugar beet'],\n",
              " ['721be0a4a.png', 'Loose Silky-bent'],\n",
              " ['0021e90e4.png', 'Small-flowered Cranesbill'],\n",
              " ['71334c634.png', 'Maize'],\n",
              " ['e84464f5a.png', 'Cleavers'],\n",
              " ['9aa5587fe.png', 'Loose Silky-bent'],\n",
              " ['958bb9e96.png', 'Charlock'],\n",
              " ['df521c0c0.png', 'Loose Silky-bent'],\n",
              " ['e88bf0db9.png', 'Small-flowered Cranesbill'],\n",
              " ['ac3193f78.png', 'Common Chickweed'],\n",
              " ['9a3f20121.png', 'Loose Silky-bent'],\n",
              " ['74068643d.png', 'Loose Silky-bent'],\n",
              " ['6b721f68e.png', 'Black-grass'],\n",
              " ['7c85b0265.png', 'Loose Silky-bent'],\n",
              " ['ff65bc002.png', 'Charlock'],\n",
              " ['026716f9b.png', 'Loose Silky-bent'],\n",
              " ['20ea96bcc.png', 'Fat Hen'],\n",
              " ['59f62ad1d.png', 'Loose Silky-bent'],\n",
              " ['df11d56a7.png', 'Sugar beet'],\n",
              " ['23e480e64.png', 'Shepherds Purse'],\n",
              " ['444473900.png', 'Fat Hen'],\n",
              " ['a3d0031fd.png', 'Loose Silky-bent'],\n",
              " ['948cdb277.png', 'Cleavers'],\n",
              " ['8bc0261c9.png', 'Loose Silky-bent'],\n",
              " ['a1e0a6c02.png', 'Cleavers'],\n",
              " ['7e9cf1c46.png', 'Common Chickweed'],\n",
              " ['f6d250856.png', 'Shepherds Purse'],\n",
              " ['1312065a5.png', 'Small-flowered Cranesbill'],\n",
              " ['24d78df74.png', 'Common Chickweed'],\n",
              " ['96f14d90c.png', 'Sugar beet'],\n",
              " ['03ef36742.png', 'Scentless Mayweed'],\n",
              " ['b944a49ca.png', 'Loose Silky-bent'],\n",
              " ['7d4cd07ad.png', 'Common Chickweed'],\n",
              " ['b62dca166.png', 'Fat Hen'],\n",
              " ['4e69d100a.png', 'Small-flowered Cranesbill'],\n",
              " ['61b044411.png', 'Scentless Mayweed'],\n",
              " ['14bb43eee.png', 'Common wheat'],\n",
              " ['71b232519.png', 'Loose Silky-bent'],\n",
              " ['e14afa235.png', 'Black-grass'],\n",
              " ['b3e08b037.png', 'Fat Hen'],\n",
              " ['d17f48d3b.png', 'Scentless Mayweed'],\n",
              " ['8874bba69.png', 'Loose Silky-bent'],\n",
              " ['96ecad7a1.png', 'Small-flowered Cranesbill'],\n",
              " ['4823c3ffa.png', 'Loose Silky-bent'],\n",
              " ['b687160f5.png', 'Small-flowered Cranesbill'],\n",
              " ['afcf6abd5.png', 'Small-flowered Cranesbill'],\n",
              " ['7a38416be.png', 'Charlock'],\n",
              " ['d41d87796.png', 'Scentless Mayweed'],\n",
              " ['cb76a7766.png', 'Fat Hen'],\n",
              " ['a890ac088.png', 'Charlock'],\n",
              " ['b6f3d8b5d.png', 'Scentless Mayweed'],\n",
              " ['b31292706.png', 'Common Chickweed'],\n",
              " ['a2c89c367.png', 'Charlock'],\n",
              " ['8104422bb.png', 'Common Chickweed'],\n",
              " ['64fe8beb9.png', 'Scentless Mayweed'],\n",
              " ['4ac29bbf0.png', 'Common Chickweed'],\n",
              " ['7cabd68cc.png', 'Loose Silky-bent'],\n",
              " ['0ad9e7dfb.png', 'Scentless Mayweed'],\n",
              " ['3f826b318.png', 'Common Chickweed'],\n",
              " ['26852751a.png', 'Charlock'],\n",
              " ['c832e4302.png', 'Fat Hen'],\n",
              " ['2ff5cb348.png', 'Sugar beet'],\n",
              " ['1926e82fd.png', 'Loose Silky-bent'],\n",
              " ['8ab8a958b.png', 'Fat Hen'],\n",
              " ['5a6bf96f6.png', 'Maize'],\n",
              " ['7691014a1.png', 'Small-flowered Cranesbill'],\n",
              " ['8303b27ed.png', 'Loose Silky-bent'],\n",
              " ['d350a25fa.png', 'Common Chickweed'],\n",
              " ['d01873fdd.png', 'Loose Silky-bent'],\n",
              " ['32c86784b.png', 'Common Chickweed'],\n",
              " ['c4ed8ed38.png', 'Charlock'],\n",
              " ['ac75d3326.png', 'Loose Silky-bent'],\n",
              " ['f2dc546ca.png', 'Cleavers'],\n",
              " ['241e6935a.png', 'Sugar beet'],\n",
              " ['55a852f40.png', 'Common Chickweed'],\n",
              " ['19b51843a.png', 'Common Chickweed'],\n",
              " ['1758a1baf.png', 'Common Chickweed'],\n",
              " ['0d31e6602.png', 'Small-flowered Cranesbill'],\n",
              " ['ace8761dd.png', 'Common Chickweed'],\n",
              " ['3eda9cbb6.png', 'Scentless Mayweed'],\n",
              " ['b3d6fdb80.png', 'Shepherds Purse'],\n",
              " ['0751c0bbc.png', 'Sugar beet'],\n",
              " ['46c14fde2.png', 'Loose Silky-bent'],\n",
              " ['7696badea.png', 'Black-grass'],\n",
              " ['7f31c7f42.png', 'Loose Silky-bent'],\n",
              " ['38c054379.png', 'Small-flowered Cranesbill'],\n",
              " ['8b9144917.png', 'Small-flowered Cranesbill'],\n",
              " ['ae90f2827.png', 'Common Chickweed'],\n",
              " ['f85ed9b6d.png', 'Shepherds Purse'],\n",
              " ['29f49cd0b.png', 'Fat Hen'],\n",
              " ['2f246d688.png', 'Small-flowered Cranesbill'],\n",
              " ['98da6ef4e.png', 'Scentless Mayweed'],\n",
              " ['63c07d340.png', 'Sugar beet'],\n",
              " ['02cfeb38d.png', 'Loose Silky-bent'],\n",
              " ['fea1d13d6.png', 'Shepherds Purse'],\n",
              " ['e9cd91682.png', 'Common Chickweed'],\n",
              " ['d8f4923f8.png', 'Common Chickweed'],\n",
              " ['1459e96a0.png', 'Cleavers'],\n",
              " ['976e4e079.png', 'Common Chickweed'],\n",
              " ['606647f64.png', 'Common Chickweed'],\n",
              " ['2dd5cfba9.png', 'Shepherds Purse'],\n",
              " ['1f290e016.png', 'Scentless Mayweed'],\n",
              " ['3185294c8.png', 'Scentless Mayweed'],\n",
              " ['550a8b7e6.png', 'Small-flowered Cranesbill'],\n",
              " ['20e562fd5.png', 'Sugar beet'],\n",
              " ['5f04aed97.png', 'Small-flowered Cranesbill'],\n",
              " ['808cf55c6.png', 'Shepherds Purse'],\n",
              " ['97b2f0a10.png', 'Small-flowered Cranesbill'],\n",
              " ['e98e5d1d5.png', 'Charlock'],\n",
              " ['9b4800b42.png', 'Charlock'],\n",
              " ['9c8b08a24.png', 'Loose Silky-bent'],\n",
              " ['969a851be.png', 'Scentless Mayweed'],\n",
              " ['3d38a87bc.png', 'Loose Silky-bent'],\n",
              " ['e15fce4f2.png', 'Scentless Mayweed'],\n",
              " ['35ebe165c.png', 'Black-grass'],\n",
              " ['ffc6f8527.png', 'Common wheat'],\n",
              " ['446f7da01.png', 'Loose Silky-bent'],\n",
              " ['b573b7a56.png', 'Loose Silky-bent'],\n",
              " ['32a8c8a1d.png', 'Loose Silky-bent'],\n",
              " ['c0f5d9ac8.png', 'Small-flowered Cranesbill'],\n",
              " ['a9d2eab61.png', 'Black-grass'],\n",
              " ['6dd095129.png', 'Common Chickweed'],\n",
              " ['24d36c52c.png', 'Common Chickweed'],\n",
              " ['39858776a.png', 'Cleavers'],\n",
              " ['003d61042.png', 'Fat Hen'],\n",
              " ['3f92d8039.png', 'Loose Silky-bent'],\n",
              " ['1f5e5554e.png', 'Small-flowered Cranesbill'],\n",
              " ['65e97117e.png', 'Loose Silky-bent'],\n",
              " ['e6f1211a2.png', 'Sugar beet'],\n",
              " ['8e2e5604e.png', 'Sugar beet'],\n",
              " ['855955aaf.png', 'Small-flowered Cranesbill'],\n",
              " ['824f5d4e5.png', 'Sugar beet'],\n",
              " ['a8b431a3e.png', 'Sugar beet'],\n",
              " ['523e5505c.png', 'Fat Hen'],\n",
              " ['9baf94467.png', 'Sugar beet'],\n",
              " ['f48916a8c.png', 'Scentless Mayweed'],\n",
              " ['9643fc5f4.png', 'Charlock'],\n",
              " ['52dc7a4d6.png', 'Common Chickweed'],\n",
              " ['a4b61a4ea.png', 'Loose Silky-bent'],\n",
              " ['34dd57ca9.png', 'Loose Silky-bent'],\n",
              " ['4e1190d78.png', 'Loose Silky-bent'],\n",
              " ['5ca0205f9.png', 'Charlock'],\n",
              " ['a38b8a581.png', 'Small-flowered Cranesbill'],\n",
              " ['0c5f6c493.png', 'Common Chickweed'],\n",
              " ['79dafec17.png', 'Loose Silky-bent'],\n",
              " ['78c5fba1d.png', 'Sugar beet'],\n",
              " ['248436078.png', 'Cleavers'],\n",
              " ['d102e1a15.png', 'Cleavers'],\n",
              " ['19fdf19fb.png', 'Common wheat'],\n",
              " ['219fd68d5.png', 'Scentless Mayweed'],\n",
              " ['0ae6668fa.png', 'Loose Silky-bent'],\n",
              " ['01291174f.png', 'Fat Hen'],\n",
              " ['8311740de.png', 'Fat Hen'],\n",
              " ['54b3afd58.png', 'Cleavers'],\n",
              " ['65d08b894.png', 'Sugar beet'],\n",
              " ['fc3e58836.png', 'Scentless Mayweed'],\n",
              " ['c6c8d4ba0.png', 'Loose Silky-bent'],\n",
              " ['80e299ae9.png', 'Shepherds Purse'],\n",
              " ['82b5f4d33.png', 'Shepherds Purse'],\n",
              " ['a0b393945.png', 'Small-flowered Cranesbill'],\n",
              " ['187668bde.png', 'Scentless Mayweed'],\n",
              " ['f9f35cbd4.png', 'Common Chickweed'],\n",
              " ['4bbf1f6ea.png', 'Shepherds Purse'],\n",
              " ['8cf2e3e6c.png', 'Common Chickweed'],\n",
              " ['a8c8a1db0.png', 'Shepherds Purse'],\n",
              " ['98062cd87.png', 'Common Chickweed'],\n",
              " ['a35fd6fbb.png', 'Common Chickweed'],\n",
              " ['cbba27d89.png', 'Maize'],\n",
              " ['edfdb4aeb.png', 'Scentless Mayweed'],\n",
              " ['24c94a6ca.png', 'Sugar beet'],\n",
              " ['1c680883c.png', 'Loose Silky-bent'],\n",
              " ['085974290.png', 'Scentless Mayweed'],\n",
              " ['25cf6eb73.png', 'Maize'],\n",
              " ['d2f422ccb.png', 'Common Chickweed'],\n",
              " ['c7ae30f3a.png', 'Scentless Mayweed'],\n",
              " ['afa446484.png', 'Small-flowered Cranesbill'],\n",
              " ['126a71ce0.png', 'Charlock'],\n",
              " ['5779fe8b4.png', 'Fat Hen'],\n",
              " ['c64370a72.png', 'Sugar beet'],\n",
              " ['089ad62a7.png', 'Common Chickweed'],\n",
              " ['a8de7c1b7.png', 'Loose Silky-bent'],\n",
              " ['71e73a8a0.png', 'Sugar beet'],\n",
              " ['a74bf916d.png', 'Shepherds Purse'],\n",
              " ['6d6eb3830.png', 'Shepherds Purse'],\n",
              " ['7b52585da.png', 'Loose Silky-bent'],\n",
              " ['74d810f87.png', 'Loose Silky-bent'],\n",
              " ['50de8a115.png', 'Shepherds Purse'],\n",
              " ['3ebbe9ca4.png', 'Charlock'],\n",
              " ['ef3e232ad.png', 'Common Chickweed'],\n",
              " ['5b63dcc21.png', 'Common Chickweed'],\n",
              " ['e5297b675.png', 'Loose Silky-bent'],\n",
              " ['ba3ce6b3e.png', 'Scentless Mayweed'],\n",
              " ['9c777333d.png', 'Common Chickweed'],\n",
              " ['099b961ec.png', 'Shepherds Purse'],\n",
              " ['1f3f44563.png', 'Maize'],\n",
              " ['6049234e6.png', 'Fat Hen'],\n",
              " ['8585f9718.png', 'Loose Silky-bent'],\n",
              " ['60ee66ddd.png', 'Cleavers'],\n",
              " ['a55d26a4c.png', 'Common Chickweed'],\n",
              " ['c7b07431e.png', 'Scentless Mayweed'],\n",
              " ['b9062c1c8.png', 'Sugar beet'],\n",
              " ['da713c465.png', 'Loose Silky-bent'],\n",
              " ['08d591441.png', 'Sugar beet'],\n",
              " ['770a265f5.png', 'Small-flowered Cranesbill'],\n",
              " ['adb7a032c.png', 'Loose Silky-bent'],\n",
              " ['99569b224.png', 'Common wheat'],\n",
              " ['39b740f7e.png', 'Loose Silky-bent'],\n",
              " ['3dd52bd2a.png', 'Small-flowered Cranesbill'],\n",
              " ['fc6f686fb.png', 'Sugar beet'],\n",
              " ['abf8b0772.png', 'Scentless Mayweed'],\n",
              " ['966ae5ad9.png', 'Loose Silky-bent'],\n",
              " ['c7051c902.png', 'Common Chickweed'],\n",
              " ['fa5fd1384.png', 'Shepherds Purse'],\n",
              " ['07e62f903.png', 'Maize'],\n",
              " ['f593c9cf0.png', 'Sugar beet'],\n",
              " ['be341dbdc.png', 'Loose Silky-bent'],\n",
              " ['c0461776c.png', 'Common Chickweed'],\n",
              " ['77ccb8b2a.png', 'Small-flowered Cranesbill'],\n",
              " ['aee6fa3df.png', 'Loose Silky-bent'],\n",
              " ['29ce426a1.png', 'Common Chickweed'],\n",
              " ['17d5e5ac4.png', 'Loose Silky-bent'],\n",
              " ['a006a475c.png', 'Common Chickweed'],\n",
              " ['177d7e2a4.png', 'Common Chickweed'],\n",
              " ['44e8b8833.png', 'Small-flowered Cranesbill'],\n",
              " ['e471f1d3a.png', 'Charlock'],\n",
              " ['856f2910a.png', 'Small-flowered Cranesbill'],\n",
              " ['ef9676433.png', 'Charlock'],\n",
              " ['f23faf9c1.png', 'Loose Silky-bent'],\n",
              " ['d6d80a321.png', 'Scentless Mayweed'],\n",
              " ['bfab3e3d0.png', 'Sugar beet'],\n",
              " ['61dd2cdc5.png', 'Common wheat'],\n",
              " ['ab0f67743.png', 'Black-grass'],\n",
              " ['d668409ff.png', 'Black-grass'],\n",
              " ['b6a3f7876.png', 'Black-grass'],\n",
              " ['f1e87cba7.png', 'Loose Silky-bent'],\n",
              " ['835dc5447.png', 'Small-flowered Cranesbill'],\n",
              " ['c85ef220d.png', 'Common Chickweed'],\n",
              " ['47b7d8e17.png', 'Small-flowered Cranesbill'],\n",
              " ['c0d9e170b.png', 'Small-flowered Cranesbill'],\n",
              " ['7b21ba6ba.png', 'Sugar beet'],\n",
              " ['17a78fb44.png', 'Common wheat'],\n",
              " ['d3331e071.png', 'Fat Hen'],\n",
              " ['30ad31220.png', 'Charlock'],\n",
              " ['f4021df6c.png', 'Fat Hen'],\n",
              " ['3efa1f66c.png', 'Loose Silky-bent'],\n",
              " ['bd789d151.png', 'Common Chickweed'],\n",
              " ['c5e88cd42.png', 'Common Chickweed'],\n",
              " ['d515398fd.png', 'Loose Silky-bent'],\n",
              " ['cd5f0db1c.png', 'Small-flowered Cranesbill'],\n",
              " ['270b939cf.png', 'Common Chickweed'],\n",
              " ['24dbc3b21.png', 'Sugar beet'],\n",
              " ['785a73ab8.png', 'Common Chickweed'],\n",
              " ['98d819587.png', 'Shepherds Purse'],\n",
              " ['223e4af09.png', 'Cleavers'],\n",
              " ['bb1c84bbc.png', 'Loose Silky-bent'],\n",
              " ['593896f83.png', 'Common Chickweed'],\n",
              " ['9d3cb4745.png', 'Common wheat'],\n",
              " ['29bab7cad.png', 'Common Chickweed'],\n",
              " ['fe9e87b78.png', 'Scentless Mayweed'],\n",
              " ['f9b6bfb00.png', 'Cleavers'],\n",
              " ['a544fc46d.png', 'Scentless Mayweed'],\n",
              " ['148bbda66.png', 'Charlock'],\n",
              " ['8b27bfd2b.png', 'Small-flowered Cranesbill'],\n",
              " ['dd9f36df7.png', 'Small-flowered Cranesbill'],\n",
              " ['1fefb54b7.png', 'Fat Hen'],\n",
              " ['c5e419015.png', 'Scentless Mayweed'],\n",
              " ['6a41bf95b.png', 'Scentless Mayweed'],\n",
              " ['675ec1b0b.png', 'Small-flowered Cranesbill'],\n",
              " ['b98327bf4.png', 'Scentless Mayweed'],\n",
              " ['d09275360.png', 'Sugar beet'],\n",
              " ['f1f7c833f.png', 'Shepherds Purse'],\n",
              " ['d0cdc768f.png', 'Common wheat'],\n",
              " ['8e29abce1.png', 'Shepherds Purse'],\n",
              " ['31f3dd81f.png', 'Loose Silky-bent'],\n",
              " ['5817b766d.png', 'Shepherds Purse'],\n",
              " ['8c9953903.png', 'Fat Hen'],\n",
              " ['b1cd2a91e.png', 'Cleavers'],\n",
              " ['54c8bb900.png', 'Charlock'],\n",
              " ['3d65168c2.png', 'Common Chickweed'],\n",
              " ['974959ec1.png', 'Sugar beet'],\n",
              " ['5a38ac566.png', 'Loose Silky-bent'],\n",
              " ['f4caf74f9.png', 'Charlock'],\n",
              " ['00d090cde.png', 'Loose Silky-bent'],\n",
              " ['9516e56c4.png', 'Black-grass'],\n",
              " ['d6d31dcbe.png', 'Scentless Mayweed'],\n",
              " ['f4234cf4f.png', 'Fat Hen'],\n",
              " ['fea3da57c.png', 'Sugar beet'],\n",
              " ['be2499cf4.png', 'Loose Silky-bent'],\n",
              " ['3abb502fb.png', 'Loose Silky-bent'],\n",
              " ['466bb6d3b.png', 'Maize'],\n",
              " ['115f93ecc.png', 'Loose Silky-bent'],\n",
              " ['cc74feadc.png', 'Loose Silky-bent'],\n",
              " ['c7eb96871.png', 'Common wheat'],\n",
              " ['0dba99002.png', 'Sugar beet'],\n",
              " ['cec5bf198.png', 'Cleavers'],\n",
              " ['6982a9d30.png', 'Small-flowered Cranesbill'],\n",
              " ['c74c5b7fc.png', 'Shepherds Purse'],\n",
              " ['604dd663f.png', 'Shepherds Purse'],\n",
              " ['599691cd9.png', 'Small-flowered Cranesbill'],\n",
              " ['8916793ce.png', 'Small-flowered Cranesbill'],\n",
              " ['a1da8be3c.png', 'Maize'],\n",
              " ['25a4c427e.png', 'Loose Silky-bent'],\n",
              " ['4c8005bbc.png', 'Loose Silky-bent'],\n",
              " ['e478c452c.png', 'Sugar beet'],\n",
              " ['d2f0f326e.png', 'Fat Hen'],\n",
              " ['36d62bf36.png', 'Common wheat'],\n",
              " ['b2706e2b3.png', 'Black-grass'],\n",
              " ['beebe5f4e.png', 'Scentless Mayweed'],\n",
              " ['406162ef9.png', 'Charlock'],\n",
              " ['892e9d6c6.png', 'Scentless Mayweed'],\n",
              " ['122913909.png', 'Maize'],\n",
              " ['60f0bc617.png', 'Small-flowered Cranesbill'],\n",
              " ['5eb9c26a6.png', 'Black-grass'],\n",
              " ['a3b375b34.png', 'Loose Silky-bent'],\n",
              " ['e96e57a90.png', 'Shepherds Purse'],\n",
              " ['6edc76e7c.png', 'Fat Hen'],\n",
              " ['b026bf8ca.png', 'Loose Silky-bent'],\n",
              " ['bb7621cb3.png', 'Fat Hen'],\n",
              " ['6db684fff.png', 'Scentless Mayweed'],\n",
              " ['93079d970.png', 'Sugar beet'],\n",
              " ['0c27cf05f.png', 'Common Chickweed'],\n",
              " ['3fbf1a417.png', 'Cleavers'],\n",
              " ['5b3beec58.png', 'Loose Silky-bent'],\n",
              " ['24a058589.png', 'Charlock'],\n",
              " ['76555b064.png', 'Loose Silky-bent'],\n",
              " ['79e5ea8fa.png', 'Common wheat'],\n",
              " ['aa7d098d1.png', 'Common Chickweed'],\n",
              " ['502dff972.png', 'Small-flowered Cranesbill'],\n",
              " ['cc3d2a59a.png', 'Sugar beet'],\n",
              " ['4a337a4a9.png', 'Fat Hen'],\n",
              " ['74fd477eb.png', 'Common Chickweed'],\n",
              " ['0c45ace27.png', 'Common Chickweed'],\n",
              " ['a85b48a95.png', 'Scentless Mayweed'],\n",
              " ['67ce3eaa6.png', 'Small-flowered Cranesbill'],\n",
              " ['dabe3e5be.png', 'Small-flowered Cranesbill'],\n",
              " ['00ef713a8.png', 'Common Chickweed'],\n",
              " ['fb022edf9.png', 'Sugar beet'],\n",
              " ['9df3275da.png', 'Small-flowered Cranesbill'],\n",
              " ['c6b76307d.png', 'Cleavers'],\n",
              " ['ec08a5d56.png', 'Small-flowered Cranesbill'],\n",
              " ['a169b71e7.png', 'Charlock'],\n",
              " ['52a87abe5.png', 'Fat Hen'],\n",
              " ['65489944f.png', 'Shepherds Purse'],\n",
              " ['33317fc2a.png', 'Fat Hen'],\n",
              " ['d14aa43f3.png', 'Small-flowered Cranesbill'],\n",
              " ['60fea2ef6.png', 'Sugar beet'],\n",
              " ['1d321253f.png', 'Fat Hen'],\n",
              " ['da231c97f.png', 'Scentless Mayweed'],\n",
              " ['dfb1d9012.png', 'Loose Silky-bent'],\n",
              " ['2126dc71b.png', 'Common Chickweed'],\n",
              " ['e3f50adfc.png', 'Cleavers'],\n",
              " ['b47691c08.png', 'Scentless Mayweed'],\n",
              " ['592cc5b89.png', 'Loose Silky-bent'],\n",
              " ['bdde957ec.png', 'Scentless Mayweed'],\n",
              " ['205df1df3.png', 'Fat Hen'],\n",
              " ['aad8375e0.png', 'Shepherds Purse'],\n",
              " ['3edf5e9ef.png', 'Loose Silky-bent'],\n",
              " ['36839d5e9.png', 'Scentless Mayweed'],\n",
              " ['668c1007c.png', 'Charlock'],\n",
              " ['e5881dd33.png', 'Fat Hen'],\n",
              " ['b5c7fd009.png', 'Black-grass'],\n",
              " ['f25996db8.png', 'Small-flowered Cranesbill'],\n",
              " ['69d1669f8.png', 'Charlock'],\n",
              " ['fd87b36ae.png', 'Loose Silky-bent'],\n",
              " ['06d12f6fa.png', 'Shepherds Purse'],\n",
              " ['a93f940d6.png', 'Scentless Mayweed'],\n",
              " ['dd5ec63d9.png', 'Sugar beet'],\n",
              " ['a935ca110.png', 'Scentless Mayweed'],\n",
              " ['007b3da8b.png', 'Sugar beet'],\n",
              " ['a7bd7cadb.png', 'Fat Hen'],\n",
              " ['0437393b1.png', 'Fat Hen'],\n",
              " ['3526b05cc.png', 'Scentless Mayweed'],\n",
              " ['534e74d83.png', 'Maize'],\n",
              " ['25fa8d109.png', 'Common wheat'],\n",
              " ['bb1d1bfd3.png', 'Charlock'],\n",
              " ['bea23d9f8.png', 'Scentless Mayweed'],\n",
              " ['41f1c3cdb.png', 'Fat Hen'],\n",
              " ['d93c7ab6d.png', 'Loose Silky-bent'],\n",
              " ['f4e7733d4.png', 'Small-flowered Cranesbill'],\n",
              " ['59e1cea8d.png', 'Cleavers'],\n",
              " ['fe29629fb.png', 'Scentless Mayweed'],\n",
              " ['1694a70e4.png', 'Scentless Mayweed'],\n",
              " ['0ee4ad224.png', 'Scentless Mayweed'],\n",
              " ['5bc6595f6.png', 'Maize'],\n",
              " ['b7a7f6390.png', 'Fat Hen'],\n",
              " ['9c32a797e.png', 'Common wheat'],\n",
              " ['20f983a71.png', 'Fat Hen'],\n",
              " ['d2fd9df40.png', 'Loose Silky-bent'],\n",
              " ['2d9c798f9.png', 'Small-flowered Cranesbill'],\n",
              " ['b215531dd.png', 'Cleavers'],\n",
              " ['78750e0ff.png', 'Charlock'],\n",
              " ['86c309150.png', 'Sugar beet'],\n",
              " ['9c0c5b731.png', 'Loose Silky-bent'],\n",
              " ['754b1adf8.png', 'Small-flowered Cranesbill'],\n",
              " ['7615e52d3.png', 'Cleavers'],\n",
              " ['37714071b.png', 'Scentless Mayweed'],\n",
              " ['ce15eee52.png', 'Charlock'],\n",
              " ['9b9911f20.png', 'Scentless Mayweed'],\n",
              " ['f351ce097.png', 'Loose Silky-bent'],\n",
              " ['22fbf13d6.png', 'Loose Silky-bent'],\n",
              " ['35a90f8d0.png', 'Scentless Mayweed'],\n",
              " ['0625f063b.png', 'Common Chickweed'],\n",
              " ['3827436f3.png', 'Common Chickweed'],\n",
              " ['3a909ead8.png', 'Small-flowered Cranesbill'],\n",
              " ['16467a950.png', 'Loose Silky-bent'],\n",
              " ['6680836dd.png', 'Sugar beet'],\n",
              " ['837ac0270.png', 'Scentless Mayweed'],\n",
              " ['26e7ae885.png', 'Fat Hen'],\n",
              " ['410e6f702.png', 'Small-flowered Cranesbill'],\n",
              " ['5bbc0a255.png', 'Sugar beet'],\n",
              " ['0e8492cb1.png', 'Scentless Mayweed'],\n",
              " ['c5078bac5.png', 'Loose Silky-bent'],\n",
              " ['caa2fbd79.png', 'Common Chickweed'],\n",
              " ['1d0cbd819.png', 'Loose Silky-bent'],\n",
              " ['c8f50f0c3.png', 'Common wheat'],\n",
              " ['23bc8ec4f.png', 'Common Chickweed'],\n",
              " ['8455169fe.png', 'Common wheat'],\n",
              " ['e1abb4ff9.png', 'Sugar beet'],\n",
              " ['8ca6140ca.png', 'Loose Silky-bent'],\n",
              " ['060450d79.png', 'Common Chickweed'],\n",
              " ['8a8d6c712.png', 'Cleavers'],\n",
              " ['55fed435f.png', 'Maize'],\n",
              " ['5ca2687a4.png', 'Maize'],\n",
              " ['0f6cbe5e8.png', 'Maize'],\n",
              " ['e52493d0b.png', 'Cleavers'],\n",
              " ['2d992d1fb.png', 'Loose Silky-bent'],\n",
              " ['338c7e907.png', 'Charlock'],\n",
              " ['55251925f.png', 'Common Chickweed'],\n",
              " ['8db450ce3.png', 'Common Chickweed'],\n",
              " ['41e07778c.png', 'Cleavers'],\n",
              " ['8e6ec1ca6.png', 'Small-flowered Cranesbill'],\n",
              " ['33448fe39.png', 'Loose Silky-bent'],\n",
              " ['1623fb9e1.png', 'Fat Hen'],\n",
              " ['cae684f8f.png', 'Charlock'],\n",
              " ['c75a82234.png', 'Scentless Mayweed'],\n",
              " ['b4c3df835.png', 'Maize'],\n",
              " ['e0ec5b6a1.png', 'Sugar beet'],\n",
              " ['866be78b0.png', 'Common Chickweed'],\n",
              " ['8cfd98117.png', 'Scentless Mayweed'],\n",
              " ['12625488b.png', 'Fat Hen'],\n",
              " ['c35efa095.png', 'Scentless Mayweed'],\n",
              " ['116b136de.png', 'Sugar beet'],\n",
              " ['800a8c17e.png', 'Fat Hen'],\n",
              " ['d7017f701.png', 'Fat Hen'],\n",
              " ['3eebd36c6.png', 'Charlock'],\n",
              " ['558aa7deb.png', 'Loose Silky-bent'],\n",
              " ['c10db7ae2.png', 'Maize'],\n",
              " ['6df8e31ea.png', 'Charlock'],\n",
              " ['539961189.png', 'Common Chickweed'],\n",
              " ['fd925f542.png', 'Common Chickweed'],\n",
              " ['cf46d09c5.png', 'Common Chickweed'],\n",
              " ['90d119d25.png', 'Scentless Mayweed'],\n",
              " ['3e9f41817.png', 'Loose Silky-bent'],\n",
              " ['75cb95e91.png', 'Charlock'],\n",
              " ['ef74dbcad.png', 'Scentless Mayweed'],\n",
              " ['66ab0e8d0.png', 'Charlock'],\n",
              " ['3b73c3b61.png', 'Black-grass'],\n",
              " ['780defa2e.png', 'Common Chickweed'],\n",
              " ['8a32d0bfa.png', 'Small-flowered Cranesbill'],\n",
              " ['bf66b9cd2.png', 'Maize'],\n",
              " ['acdb75e00.png', 'Loose Silky-bent'],\n",
              " ['cb496f36e.png', 'Loose Silky-bent'],\n",
              " ['f445fe6fb.png', 'Charlock'],\n",
              " ['37e545a60.png', 'Common Chickweed'],\n",
              " ['a060c1cf8.png', 'Maize'],\n",
              " ['728eabae1.png', 'Charlock'],\n",
              " ['90b595f12.png', 'Common Chickweed'],\n",
              " ['4b032563b.png', 'Common Chickweed'],\n",
              " ['e4a76885b.png', 'Maize'],\n",
              " ['13b9fa92d.png', 'Sugar beet'],\n",
              " ['1be0713da.png', 'Cleavers'],\n",
              " ['8e4eaeec0.png', 'Common Chickweed'],\n",
              " ['f9ea23fb5.png', 'Fat Hen'],\n",
              " ['5ee9d0a5b.png', 'Loose Silky-bent'],\n",
              " ['0086a6340.png', 'Common Chickweed'],\n",
              " ['65e262a6d.png', 'Charlock'],\n",
              " ['bb64660b7.png', 'Sugar beet'],\n",
              " ['79d93bc96.png', 'Loose Silky-bent'],\n",
              " ['dabea05f4.png', 'Black-grass'],\n",
              " ['1b6a6494d.png', 'Sugar beet'],\n",
              " ['711b46fba.png', 'Shepherds Purse'],\n",
              " ['76dbd1054.png', 'Scentless Mayweed'],\n",
              " ['a0f37c726.png', 'Small-flowered Cranesbill'],\n",
              " ['f33e9d918.png', 'Scentless Mayweed'],\n",
              " ['8ece6efec.png', 'Scentless Mayweed'],\n",
              " ['ce42adffb.png', 'Loose Silky-bent'],\n",
              " ['48ef6a2ff.png', 'Common wheat'],\n",
              " ['5dcad9a53.png', 'Small-flowered Cranesbill'],\n",
              " ['da4ed3a28.png', 'Charlock'],\n",
              " ['88d8a4508.png', 'Fat Hen'],\n",
              " ['a85fc8c9a.png', 'Small-flowered Cranesbill'],\n",
              " ['8cf909eb3.png', 'Charlock'],\n",
              " ['59c6a9f95.png', 'Fat Hen'],\n",
              " ['fd253a74e.png', 'Cleavers'],\n",
              " ['4e9d3765f.png', 'Charlock'],\n",
              " ['99036c51d.png', 'Loose Silky-bent'],\n",
              " ['0c4199daa.png', 'Loose Silky-bent'],\n",
              " ['5bdcfa329.png', 'Loose Silky-bent'],\n",
              " ['288564c76.png', 'Sugar beet'],\n",
              " ['0a64e3e6c.png', 'Black-grass'],\n",
              " ['4e18ab737.png', 'Cleavers'],\n",
              " ['239bdf640.png', 'Charlock'],\n",
              " ['e901b0f28.png', 'Fat Hen'],\n",
              " ['fef2ade8c.png', 'Sugar beet'],\n",
              " ['d89db156f.png', 'Small-flowered Cranesbill'],\n",
              " ['48231e475.png', 'Scentless Mayweed'],\n",
              " ['f66ae4070.png', 'Maize'],\n",
              " ['429211ee6.png', 'Fat Hen'],\n",
              " ['c1ecff98b.png', 'Loose Silky-bent'],\n",
              " ['851c90831.png', 'Loose Silky-bent'],\n",
              " ['7beb2766f.png', 'Scentless Mayweed'],\n",
              " ['dce2f6612.png', 'Common Chickweed'],\n",
              " ['e783f5a4f.png', 'Sugar beet'],\n",
              " ['809eb0b82.png', 'Charlock'],\n",
              " ['4c7838de4.png', 'Common Chickweed'],\n",
              " ['d563be369.png', 'Fat Hen'],\n",
              " ['e15472085.png', 'Common Chickweed'],\n",
              " ['56a01b835.png', 'Cleavers'],\n",
              " ['1bf9b94a6.png', 'Scentless Mayweed'],\n",
              " ['9d79a1f0c.png', 'Small-flowered Cranesbill'],\n",
              " ['bb20fce02.png', 'Scentless Mayweed'],\n",
              " ['2d5058a59.png', 'Common Chickweed'],\n",
              " ['ef65533d5.png', 'Loose Silky-bent'],\n",
              " ['4c5ab9b68.png', 'Cleavers'],\n",
              " ['e82017baa.png', 'Loose Silky-bent'],\n",
              " ['e5368474f.png', 'Scentless Mayweed'],\n",
              " ['aecfaed64.png', 'Shepherds Purse'],\n",
              " ['31fcdc161.png', 'Maize'],\n",
              " ['0d117d910.png', 'Common Chickweed'],\n",
              " ['e9d48d664.png', 'Shepherds Purse'],\n",
              " ['d0152bd7c.png', 'Fat Hen'],\n",
              " ['592473c83.png', 'Loose Silky-bent'],\n",
              " ['953496deb.png', 'Fat Hen'],\n",
              " ['1541bdb2e.png', 'Maize'],\n",
              " ['2693e5c65.png', 'Maize'],\n",
              " ['ab35453cb.png', 'Cleavers'],\n",
              " ['a8da9c08d.png', 'Charlock'],\n",
              " ['cbe761896.png', 'Charlock'],\n",
              " ['f0ffa00bd.png', 'Sugar beet'],\n",
              " ['5af1d74ee.png', 'Sugar beet'],\n",
              " ['fda0b5c38.png', 'Common wheat'],\n",
              " ['c0bc3997b.png', 'Scentless Mayweed'],\n",
              " ['37297a64c.png', 'Common Chickweed'],\n",
              " ['5883b423d.png', 'Scentless Mayweed'],\n",
              " ['647689543.png', 'Loose Silky-bent'],\n",
              " ['fda39e16f.png', 'Loose Silky-bent'],\n",
              " ['7fdb7202d.png', 'Scentless Mayweed'],\n",
              " ['c069fc3fa.png', 'Fat Hen'],\n",
              " ['eec1079a1.png', 'Charlock'],\n",
              " ['bd72d4d8a.png', 'Scentless Mayweed'],\n",
              " ['5b3000b9a.png', 'Maize'],\n",
              " ['8d6acbe9b.png', 'Common Chickweed'],\n",
              " ['1191ba346.png', 'Cleavers'],\n",
              " ['2406d6c99.png', 'Shepherds Purse'],\n",
              " ['c10ccbd82.png', 'Scentless Mayweed'],\n",
              " ['e7077322d.png', 'Scentless Mayweed'],\n",
              " ['063363305.png', 'Small-flowered Cranesbill'],\n",
              " ['cfb18d262.png', 'Scentless Mayweed'],\n",
              " ['47f9e5d91.png', 'Common Chickweed'],\n",
              " ['b03397525.png', 'Loose Silky-bent'],\n",
              " ['4f83143e1.png', 'Maize'],\n",
              " ['04814f36d.png', 'Scentless Mayweed'],\n",
              " ['1821eb11a.png', 'Scentless Mayweed'],\n",
              " ['599c82eea.png', 'Scentless Mayweed'],\n",
              " ['fdea6b119.png', 'Common wheat'],\n",
              " ['4287d810c.png', 'Common Chickweed'],\n",
              " ['53e6e9000.png', 'Common wheat'],\n",
              " ['cd6adba97.png', 'Small-flowered Cranesbill'],\n",
              " ['aaf4da98f.png', 'Sugar beet'],\n",
              " ['b0acaff4a.png', 'Loose Silky-bent'],\n",
              " ['2ea664465.png', 'Sugar beet'],\n",
              " ['b828443ff.png', 'Maize'],\n",
              " ['c2de6020a.png', 'Small-flowered Cranesbill'],\n",
              " ['e721c6ac8.png', 'Loose Silky-bent'],\n",
              " ['ef7a5651d.png', 'Scentless Mayweed'],\n",
              " ['963544aa0.png', 'Loose Silky-bent'],\n",
              " ['4b155fb07.png', 'Maize'],\n",
              " ['af45e222a.png', 'Scentless Mayweed'],\n",
              " ['3281183f9.png', 'Scentless Mayweed'],\n",
              " ['cfd8165e9.png', 'Charlock'],\n",
              " ['ede6b84b4.png', 'Cleavers'],\n",
              " ['fa9f3a8f9.png', 'Cleavers'],\n",
              " ['03e322a29.png', 'Scentless Mayweed'],\n",
              " ['f4ad9d950.png', 'Sugar beet'],\n",
              " ['8f523520c.png', 'Shepherds Purse'],\n",
              " ['a2b703e21.png', 'Small-flowered Cranesbill'],\n",
              " ['55920f07f.png', 'Maize'],\n",
              " ['632156793.png', 'Cleavers'],\n",
              " ['91e469b4a.png', 'Charlock'],\n",
              " ['4ea7493d5.png', 'Sugar beet'],\n",
              " ['cadab6616.png', 'Shepherds Purse'],\n",
              " ['79fba50db.png', 'Common Chickweed'],\n",
              " ['7d3045fc3.png', 'Sugar beet'],\n",
              " ['da9ef7858.png', 'Charlock'],\n",
              " ['97844bfd5.png', 'Shepherds Purse'],\n",
              " ['808578ed5.png', 'Charlock'],\n",
              " ['618de3d7a.png', 'Charlock'],\n",
              " ['0911d3dee.png', 'Common Chickweed'],\n",
              " ['653193c1a.png', 'Common wheat'],\n",
              " ['fba8fc78a.png', 'Loose Silky-bent'],\n",
              " ['615d2b0a9.png', 'Sugar beet'],\n",
              " ['a254d71f6.png', 'Loose Silky-bent'],\n",
              " ['1e095a7e1.png', 'Small-flowered Cranesbill'],\n",
              " ['664194d19.png', 'Small-flowered Cranesbill'],\n",
              " ['506347cfe.png', 'Small-flowered Cranesbill'],\n",
              " ['d488a4fe1.png', 'Common wheat'],\n",
              " ['33748968f.png', 'Cleavers'],\n",
              " ['16fd2e01a.png', 'Small-flowered Cranesbill'],\n",
              " ['53ceb4657.png', 'Common Chickweed'],\n",
              " ['c26ccf73c.png', 'Common Chickweed'],\n",
              " ['d9c50616e.png', 'Sugar beet'],\n",
              " ['5bd71f445.png', 'Common Chickweed'],\n",
              " ['1b490196c.png', 'Shepherds Purse'],\n",
              " ['05341a8a6.png', 'Scentless Mayweed'],\n",
              " ['86f08e6d1.png', 'Fat Hen'],\n",
              " ['a8388a37f.png', 'Black-grass'],\n",
              " ['756dd5070.png', 'Common Chickweed'],\n",
              " ['1cfd91582.png', 'Charlock'],\n",
              " ['4bcdaa5e2.png', 'Shepherds Purse'],\n",
              " ['6a47821f9.png', 'Loose Silky-bent'],\n",
              " ['93d76fd5d.png', 'Small-flowered Cranesbill'],\n",
              " ['67e185673.png', 'Small-flowered Cranesbill'],\n",
              " ['b4f7c9214.png', 'Loose Silky-bent'],\n",
              " ['0fb233ad6.png', 'Small-flowered Cranesbill'],\n",
              " ['d5f7dd60a.png', 'Scentless Mayweed'],\n",
              " ['ad12382d4.png', 'Loose Silky-bent'],\n",
              " ['071cb3ece.png', 'Sugar beet']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR8Et19oU2Y5",
        "colab_type": "text"
      },
      "source": [
        "Take the predictions in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yKfNOIsU0G5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictedDataFrame = pd.DataFrame(predictions, columns=['Image Name', 'Predicted Class'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3ASLKqqU-w0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "0df567fe-a9ef-4e1f-e76b-4dbc9b847930"
      },
      "source": [
        "predictedDataFrame"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Image Name</th>\n",
              "      <th>Predicted Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8301b0547.png</td>\n",
              "      <td>Charlock</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>862b8e7a0.png</td>\n",
              "      <td>Sugar beet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a800caead.png</td>\n",
              "      <td>Sugar beet</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a74d475c2.png</td>\n",
              "      <td>Common Chickweed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>490c4f9c8.png</td>\n",
              "      <td>Fat Hen</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>b4f7c9214.png</td>\n",
              "      <td>Loose Silky-bent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>0fb233ad6.png</td>\n",
              "      <td>Small-flowered Cranesbill</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>d5f7dd60a.png</td>\n",
              "      <td>Scentless Mayweed</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>ad12382d4.png</td>\n",
              "      <td>Loose Silky-bent</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>071cb3ece.png</td>\n",
              "      <td>Sugar beet</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>794 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Image Name            Predicted Class\n",
              "0    8301b0547.png                   Charlock\n",
              "1    862b8e7a0.png                 Sugar beet\n",
              "2    a800caead.png                 Sugar beet\n",
              "3    a74d475c2.png           Common Chickweed\n",
              "4    490c4f9c8.png                    Fat Hen\n",
              "..             ...                        ...\n",
              "789  b4f7c9214.png           Loose Silky-bent\n",
              "790  0fb233ad6.png  Small-flowered Cranesbill\n",
              "791  d5f7dd60a.png          Scentless Mayweed\n",
              "792  ad12382d4.png           Loose Silky-bent\n",
              "793  071cb3ece.png                 Sugar beet\n",
              "\n",
              "[794 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWnHpSiFVATU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}